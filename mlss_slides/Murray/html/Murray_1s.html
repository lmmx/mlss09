<!DOCTYPE html><html>
<head>
<title></title>
<style type="text/css">
<!--
.xflip {
    -moz-transform: scaleX(-1);
    -webkit-transform: scaleX(-1);
    -o-transform: scaleX(-1);
    transform: scaleX(-1);
    filter: fliph;
}
.yflip {
    -moz-transform: scaleY(-1);
    -webkit-transform: scaleY(-1);
    -o-transform: scaleY(-1);
    transform: scaleY(-1);
    filter: flipv;
}
.xyflip {
    -moz-transform: scaleX(-1) scaleY(-1);
    -webkit-transform: scaleX(-1) scaleY(-1);
    -o-transform: scaleX(-1) scaleY(-1);
    transform: scaleX(-1) scaleY(-1);
    filter: fliph + flipv;
}
-->
</style>
</head>
<body>
<a name=1></a><img src="./Murray_1-1_1.png"/><br/>
Markov&#160;chain&#160;Monte&#160;Carlo<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;2009<br/>
http://mlg.eng.cam.ac.uk/mlss09/<br/>
Iain&#160;Murray<br/>
http://www.cs.toronto.edu/~murray/<br/>
<hr/>
<a name=2></a>A&#160;statistical&#160;problem<br/>
What&#160;is&#160;the&#160;average&#160;height&#160;of&#160;the&#160;MLSS&#160;lecturers?<br/>Method:&#160;measure&#160;their&#160;heights,&#160;add&#160;them&#160;up&#160;and&#160;divide&#160;by&#160;N&#160;=&#160;20.<br/>
What&#160;is&#160;the&#160;average&#160;height&#160;f&#160;of&#160;people&#160;p&#160;in&#160;Cambridge&#160;C?<br/>
1<br/>
E<br/>
X<br/>
p‚ààC[f&#160;(p)]&#160;‚â°<br/>
f(p),<br/>
|C|<br/>
‚Äúintractable‚Äù?<br/>
p‚ààC<br/>
1&#160;S<br/>
‚âà&#160;X&#160;f&#160;p(s),<br/>
S<br/>
for&#160;random&#160;survey&#160;of&#160;S&#160;people&#160;{p(s)}&#160;‚àà&#160;C<br/>
s=1<br/>
Surveying&#160;works&#160;for&#160;large&#160;and&#160;notionally&#160;infinite&#160;populations.<br/>
<hr/>
<a name=3></a>Simple&#160;Monte&#160;Carlo<br/>
Statistical&#160;sampling&#160;can&#160;be&#160;applied&#160;to&#160;any&#160;expectation:<br/>
In&#160;general:<br/>
S<br/>
Z<br/>
1<br/>
f(x)P&#160;(x)&#160;dx&#160;‚âà<br/>
Xf(x(s)),&#160;x(s)&#160;‚àº&#160;P&#160;(x)<br/>
S&#160;s=1<br/>
Example:&#160;making&#160;predictions<br/>
Z<br/>
p(x|D)&#160;=<br/>
P&#160;(x|Œ∏,&#160;D)P&#160;(Œ∏|D)&#160;dŒ∏<br/>
1&#160;S<br/>
‚âà<br/>
XP&#160;(x|Œ∏(s),&#160;D),&#160;Œ∏(s)&#160;‚àº&#160;P&#160;(Œ∏|D)<br/>
S&#160;s=1<br/>
More&#160;examples:&#160;E-step&#160;statistics&#160;in&#160;EM,&#160;Boltzmann&#160;machine&#160;learning<br/>
<hr/>
<a name=4></a>Properties&#160;of&#160;Monte&#160;Carlo<br/>
S<br/>
Z<br/>
1&#160;X<br/>
Estimator:<br/>
f(x)P&#160;(x)&#160;dx&#160;‚âà&#160;ÀÜ<br/>
f&#160;‚â°<br/>
f(x(s)),&#160;x(s)&#160;‚àº&#160;P&#160;(x)<br/>
S&#160;s=1<br/>
Estimator&#160;is&#160;unbiased:<br/>
S<br/>
h&#160;ÀÜ<br/>
1&#160;X<br/>
E<br/>
fi&#160;=<br/>
P&#160;({x(s)})<br/>
S<br/>
EP&#160;(x)&#160;[f&#160;(x)]&#160;=&#160;EP&#160;(x)&#160;[f&#160;(x)]<br/>
s=1<br/>
Variance&#160;shrinks&#160;‚àù&#160;1/S:<br/>
1&#160;S<br/>
var<br/>
h&#160;ÀÜ<br/>
fi&#160;=<br/>
X&#160;var<br/>
P&#160;({x(s)})<br/>
S2<br/>
P&#160;(x)&#160;[f&#160;(x)]<br/>
=&#160;varP(x)[f(x)]&#160;/S<br/>
s=1<br/>
‚àö<br/>
‚ÄúError&#160;bars‚Äù&#160;shrink&#160;like<br/>
S<br/>
<hr/>
<a name=5></a>A&#160;dumb&#160;approximation&#160;of&#160;œÄ<br/>
(1&#160;0&lt;x&lt;1&#160;and&#160;0&lt;y&#160;&lt;1<br/>
P&#160;(x,&#160;y)&#160;=&#160;0&#160;otherwise<br/>
Z&#160;Z<br/>
œÄ&#160;=&#160;4<br/>
I&#160;(x2&#160;+&#160;y2)&#160;&lt;&#160;1P&#160;(x,&#160;y)&#160;dx&#160;dy<br/>
octave:1&gt;&#160;S=12;&#160;a=rand(S,2);&#160;4*mean(sum(a.*a,2)&lt;1)<br/>
ans&#160;=&#160;3.3333<br/>
octave:2&gt;&#160;S=1e7;&#160;a=rand(S,2);&#160;4*mean(sum(a.*a,2)&lt;1)<br/>
ans&#160;=&#160;3.1418<br/>
<hr/>
<a name=6></a>Aside:&#160;don‚Äôt&#160;always&#160;sample!<br/>
‚ÄúMonte&#160;Carlo&#160;is&#160;an&#160;extremely&#160;bad&#160;method;&#160;it&#160;should&#160;be&#160;used&#160;only<br/>when&#160;all&#160;alternative&#160;methods&#160;are&#160;worse.‚Äù<br/>
‚Äî&#160;Alan&#160;Sokal,&#160;1996<br/>
Example:&#160;numerical&#160;solutions&#160;to&#160;(nice)&#160;1D&#160;integrals&#160;are&#160;fast<br/>
octave:1&gt;&#160;4&#160;*&#160;quadl(@(x)&#160;sqrt(1-x.^2),&#160;0,&#160;1,&#160;tolerance)<br/>
Gives&#160;œÄ&#160;to&#160;6&#160;dp‚Äôs&#160;in&#160;108&#160;evaluations,&#160;machine&#160;precision&#160;in&#160;2598.<br/>(NB&#160;Matlab‚Äôs&#160;quadl&#160;fails&#160;at&#160;zero&#160;tolerance)<br/>
Other&#160;lecturers&#160;are&#160;covering&#160;alternatives&#160;for&#160;higher&#160;dimensions.<br/>
No&#160;approx.&#160;integration&#160;method&#160;always&#160;works.&#160;Sometimes&#160;Monte&#160;Carlo&#160;is&#160;the&#160;best.<br/>
<hr/>
<a name=7></a><img src="./Murray_1-7_1.png"/><br/>
<img src="./Murray_1-7_2.png"/><br/>
<img src="./Murray_1-7_3.png"/><br/>
Eye-balling&#160;samples<br/>
Sometimes&#160;samples&#160;are&#160;pleasing&#160;to&#160;look&#160;at:<br/>
(if&#160;you‚Äôre&#160;into&#160;geometrical&#160;combinatorics)<br/>
Figure&#160;by&#160;Propp&#160;and&#160;Wilson.&#160;Source:&#160;MacKay&#160;textbook.<br/>
Sanity&#160;check&#160;probabilistic&#160;modelling&#160;assumptions:<br/>
Data&#160;samples<br/>
MoB&#160;samples<br/>
RBM&#160;samples<br/>
<hr/>
<a name=8></a><img src="./Murray_1-8_1.jpg"/><br/>
Monte&#160;Carlo&#160;and&#160;Insomnia<br/>
Enrico&#160;Fermi&#160;(1901‚Äì1954)&#160;took&#160;great<br/>delight&#160;in&#160;astonishing&#160;his&#160;colleagues<br/>with&#160;his&#160;remakably&#160;accurate&#160;predictions<br/>of&#160;experimental&#160;results.&#160;.&#160;.&#160;he&#160;revealed<br/>that&#160;his&#160;‚Äúguesses‚Äù&#160;were&#160;really&#160;derived<br/>from&#160;the&#160;statistical&#160;sampling&#160;techniques<br/>that&#160;he&#160;used&#160;to&#160;calculate&#160;with&#160;whenever<br/>insomnia&#160;struck&#160;in&#160;the&#160;wee&#160;morning<br/>hours!<br/>
‚ÄîThe&#160;beginning&#160;of&#160;the&#160;Monte&#160;Carlo&#160;method,<br/>
N.&#160;Metropolis<br/>
<hr/>
<a name=9></a>Sampling&#160;from&#160;a&#160;Bayes&#160;net<br/>
Ancestral&#160;pass&#160;for&#160;directed&#160;graphical&#160;models:<br/>
‚Äî&#160;sample&#160;each&#160;top&#160;level&#160;variable&#160;from&#160;its&#160;marginal<br/>
‚Äî&#160;sample&#160;each&#160;other&#160;node&#160;from&#160;its&#160;conditional<br/>
once&#160;its&#160;parents&#160;have&#160;been&#160;sampled<br/>
A<br/>
B<br/>
Sample:<br/>A&#160;‚àº&#160;P&#160;(A)<br/>
B&#160;‚àº&#160;P&#160;(B)<br/>
C<br/>
C&#160;‚àº&#160;P&#160;(C&#160;|A,&#160;B)<br/>
D<br/>
D&#160;‚àº&#160;P&#160;(D&#160;|B,&#160;C)<br/>
E<br/>
E&#160;‚àº&#160;P&#160;(D&#160;|C,&#160;D)<br/>
P&#160;(A,&#160;B,&#160;C,&#160;D,&#160;E)&#160;=&#160;P&#160;(A)&#160;P&#160;(B)&#160;P&#160;(C&#160;|A,&#160;B)&#160;P&#160;(D&#160;|B,&#160;C)&#160;P&#160;(E&#160;|C,&#160;D)<br/>
<hr/>
<a name=10></a><img src="./Murray_1-10_1.jpg"/><br/>
Sampling&#160;the&#160;conditionals<br/>
Use&#160;library&#160;routines&#160;for<br/>univariate&#160;distributions<br/>(and&#160;some&#160;other&#160;special&#160;cases)<br/>
This&#160;book&#160;(free&#160;online)&#160;explains&#160;how<br/>some&#160;of&#160;them&#160;work<br/>
http://cg.scs.carleton.ca/~luc/rnbookindex.html<br/>
<hr/>
<a name=11></a>Sampling&#160;from&#160;distributions<br/>
Draw&#160;points&#160;uniformly&#160;under&#160;the&#160;curve:<br/>
P&#160;(x)<br/>
x<br/>
x(2)<br/>
x(3)<br/>
x(1)&#160;x(4)<br/>
Probability&#160;mass&#160;to&#160;left&#160;of&#160;point&#160;‚àº&#160;Uniform[0,1]<br/>
<hr/>
<a name=12></a>Sampling&#160;from&#160;distributions<br/>
How&#160;to&#160;convert&#160;samples&#160;from&#160;a&#160;Uniform[0,1]&#160;generator:<br/>
1<br/>
h(y)<br/>
h(y)&#160;=&#160;R&#160;y&#160;p(y0)&#160;dy0<br/>
‚àí‚àû<br/>
p(y)<br/>
Draw&#160;mass&#160;to&#160;left&#160;of&#160;point:<br/>
u&#160;‚àº&#160;Uniform[0,1]<br/>
Sample,&#160;y(u)&#160;=&#160;h‚àí1(u)<br/>
0<br/>
y<br/>
Figure&#160;from&#160;PRML,&#160;Bishop&#160;(2006)<br/>
Although&#160;we&#160;can‚Äôt&#160;always&#160;compute&#160;and&#160;invert&#160;h(y)<br/>
<hr/>
<a name=13></a>Rejection&#160;sampling<br/>
Sampling&#160;underneath&#160;a&#160;Àú<br/>
P&#160;(x)‚àùP&#160;(x)&#160;curve&#160;is&#160;also&#160;valid<br/>
Draw&#160;underneath&#160;a&#160;simple<br/>curve&#160;k&#160;Àú<br/>
Q(x)&#160;‚â•&#160;Àú<br/>
P&#160;(x):<br/>
k&#160;Àú<br/>
Q(x)<br/>
k&#160;Àú<br/>
optQ(x)<br/>
‚Äì&#160;Draw&#160;x&#160;‚àº&#160;Q(x)<br/>‚Äì&#160;height&#160;u&#160;‚àº&#160;Uniform[0,&#160;k&#160;Àú<br/>
Q(x)]<br/>
Àú<br/>
P&#160;(x)<br/>
(xi,&#160;ui)<br/>
Discard&#160;the&#160;point&#160;if&#160;above&#160;Àú<br/>
P&#160;,<br/>
i.e.&#160;if&#160;u&#160;&gt;&#160;Àú<br/>
P&#160;(x)<br/>
(xj,&#160;uj)<br/>
x<br/>
x(1)<br/>
<hr/>
<a name=14></a>Importance&#160;sampling<br/>
Computing&#160;Àú<br/>
P&#160;(x)&#160;and&#160;Àú<br/>
Q(x),&#160;then&#160;throwing&#160;x&#160;away&#160;seems&#160;wasteful<br/>
Instead&#160;rewrite&#160;the&#160;integral&#160;as&#160;an&#160;expectation&#160;under&#160;Q:<br/>
Z<br/>
Z<br/>
P&#160;(x)<br/>
f(x)P&#160;(x)&#160;dx&#160;=<br/>
f(x)<br/>
Q(x)&#160;dx,<br/>
Q(x)<br/>
(Q(x)&#160;&gt;&#160;0&#160;if&#160;P&#160;(x)&#160;&gt;&#160;0)<br/>
1&#160;S<br/>
P&#160;(x(s))<br/>
‚âà<br/>
Xf(x(s))<br/>
,&#160;x(s)&#160;‚àº&#160;Q(x)<br/>
S<br/>
Q(x(s))<br/>
s=1<br/>
This&#160;is&#160;just&#160;simple&#160;Monte&#160;Carlo&#160;again,&#160;so&#160;it&#160;is&#160;unbiased.<br/>
Importance&#160;sampling&#160;applies&#160;when&#160;the&#160;integral&#160;is&#160;not&#160;an&#160;expectation.<br/>Divide&#160;and&#160;multiply&#160;any&#160;integrand&#160;by&#160;a&#160;convenient&#160;distribution.<br/>
<hr/>
<a name=15></a>Importance&#160;sampling&#160;(2)<br/>
Previous&#160;slide&#160;assumed&#160;we&#160;could&#160;evaluate&#160;P&#160;(x)&#160;=&#160;Àú<br/>
P&#160;(x)/ZP<br/>
S<br/>
Z<br/>
Z&#160;1<br/>
Àú<br/>
P&#160;(x(s))<br/>
f(x)P&#160;(x)&#160;dx&#160;‚âà<br/>
Q<br/>
X&#160;f(x(s))<br/>
,&#160;x(s)&#160;‚àº&#160;Q(x)<br/>
Z<br/>
Àú<br/>
P&#160;S<br/>
Q(x(s))<br/>
s=1<br/>
|<br/>
{z<br/>
}<br/>
Àúr(s)<br/>
S<br/>
Àúr(s)<br/>
S<br/>
‚âà&#160;1&#160;X<br/>
X<br/>
<br/>
f(x(s))<br/>
‚â°<br/>
f(x(s))w(s)<br/>
<br/>
1&#160;P<br/>
<br/>
S<br/>
<br/>
<br/>
<br/>
s=1<br/>
<br/>
S<br/>
s0&#160;Àú<br/>
r(s0)<br/>
s=1<br/>
<br/>
This&#160;estimator&#160;is&#160;consistent&#160;but&#160;biased<br/>
Exercise:&#160;Prove&#160;that&#160;Z<br/>
P<br/>
P&#160;/ZQ&#160;‚âà&#160;1<br/>
Àúr(s)<br/>
S<br/>
s<br/>
<hr/>
<a name=16></a>Summary&#160;so&#160;far<br/>
‚Ä¢&#160;Sums&#160;and&#160;integrals,&#160;often&#160;expectations,&#160;occur&#160;frequently&#160;in&#160;statistics<br/>
‚Ä¢&#160;Monte&#160;Carlo&#160;approximates&#160;expectations&#160;with&#160;a&#160;sample&#160;average<br/>
‚Ä¢&#160;Rejection&#160;sampling&#160;draws&#160;samples&#160;from&#160;complex&#160;distributions<br/>
‚Ä¢&#160;Importance&#160;sampling&#160;applies&#160;Monte&#160;Carlo&#160;to&#160;‚Äòany‚Äô&#160;sum/integral<br/>
<hr/>
<a name=17></a>Application&#160;to&#160;large&#160;problems<br/>
We&#160;often&#160;can‚Äôt&#160;decompose&#160;P&#160;(X)&#160;into&#160;low-dimensional&#160;conditionals<br/>
Undirected&#160;graphical&#160;models:&#160;P&#160;(x)&#160;=&#160;1&#160;Q&#160;f<br/>
Z<br/>
i<br/>
i(x)<br/>
A<br/>
B<br/>
Posterior&#160;of&#160;a&#160;directed&#160;graphical&#160;model<br/>
C<br/>
P&#160;(A,&#160;B,&#160;C,&#160;D,&#160;E)<br/>
P&#160;(A,&#160;B,&#160;C,&#160;D&#160;|E)&#160;=<br/>
D<br/>
P&#160;(E)<br/>
E<br/>
We&#160;often&#160;don‚Äôt&#160;know&#160;Z&#160;or&#160;P&#160;(E)<br/>
<hr/>
<a name=18></a>Application&#160;to&#160;large&#160;problems<br/>
Rejection&#160;&amp;&#160;importance&#160;sampling&#160;scale&#160;badly&#160;with&#160;dimensionality<br/>
Example:<br/>
P&#160;(x)&#160;=&#160;N&#160;(0,&#160;I),&#160;Q(x)&#160;=&#160;N&#160;(0,&#160;œÉ2I)<br/>
Rejection&#160;sampling:<br/>
Requires&#160;œÉ&#160;‚â•&#160;1.&#160;Fraction&#160;of&#160;proposals&#160;accepted&#160;=&#160;œÉ‚àíD<br/>
Importance&#160;sampling:<br/>
D/2<br/>
Variance&#160;of&#160;importance&#160;weights&#160;=&#160;&#160;œÉ2<br/>
‚àí&#160;1<br/>
2‚àí1/œÉ2<br/>
‚àö<br/>
Infinite&#160;/&#160;undefined&#160;variance&#160;if&#160;œÉ&#160;‚â§&#160;1/&#160;2<br/>
<hr/>
<a name=19></a>Importance&#160;sampling&#160;weights<br/>
w&#160;=0.00548&#160;w&#160;=1.59e-08&#160;w&#160;=9.65e-06<br/>
w&#160;=0.371<br/>
w&#160;=0.103<br/>
w&#160;=1.01e-08<br/>
w&#160;=0.111<br/>
w&#160;=1.92e-09<br/>
w&#160;=0.0126<br/>
w&#160;=1.1e-51<br/>
<hr/>
<a name=20></a>Metropolis&#160;algorithm<br/>
3<br/>
‚Ä¢<br/>
2.5<br/>
Perturb&#160;parameters:&#160;Q(Œ∏0;&#160;Œ∏),&#160;e.g.&#160;N&#160;(Œ∏,&#160;œÉ2)<br/>
2<br/>
&#160;<br/>
Àú<br/>
P&#160;(Œ∏0|D)!<br/>
‚Ä¢<br/>
1.5<br/>
Accept&#160;with&#160;probability&#160;min&#160;1,&#160;ÀúP(Œ∏|D)<br/>
1<br/>
‚Ä¢<br/>
0.5<br/>
Otherwise&#160;keep&#160;old&#160;parameters<br/>
00<br/>
0.5<br/>
1<br/>
1.5<br/>
2<br/>
2.5<br/>
3<br/>
0<br/>
0<br/>
Detail:&#160;Metropolis,&#160;as&#160;stated,&#160;requires&#160;Q<br/>
This&#160;subfigure&#160;from&#160;PRML,&#160;Bishop&#160;(2006)<br/>
(Œ∏&#160;;&#160;Œ∏)&#160;=&#160;Q(Œ∏;&#160;Œ∏&#160;)<br/>
<hr/>
<a name=21></a><img src="./Murray_1-21_1.png"/><br/>
Markov&#160;chain&#160;Monte&#160;Carlo<br/>
Construct&#160;a&#160;biased&#160;random&#160;walk&#160;that&#160;explores&#160;target&#160;dist&#160;P&#160;?(x)<br/>
Markov&#160;steps,&#160;xt&#160;‚àº&#160;T&#160;(xt&#160;‚Üê&#160;xt‚àí1)<br/>
MCMC&#160;gives&#160;approximate,&#160;correlated&#160;samples&#160;from&#160;P&#160;?(x)<br/>
<hr/>
<a name=22></a>Transition&#160;operators<br/>
Discrete&#160;example<br/>
Ô£´3/5Ô£∂<br/>
Ô£´2/3&#160;1/2&#160;1/2Ô£∂<br/>
P&#160;?&#160;=&#160;1/5<br/>
T&#160;=&#160;1/6<br/>
0<br/>
1/2<br/>
T<br/>
Ô£≠<br/>
Ô£∏<br/>
Ô£≠<br/>
Ô£∏<br/>
ij&#160;=&#160;T&#160;(xi&#160;‚Üê&#160;xj)<br/>
1/5<br/>
1/6&#160;1/2<br/>
0<br/>
P&#160;?&#160;is&#160;an&#160;invariant&#160;distribution&#160;of&#160;T&#160;because&#160;T&#160;P&#160;?&#160;=P&#160;?,&#160;i.e.<br/>
X&#160;T&#160;(x0‚Üêx)P&#160;?(x)&#160;=&#160;P&#160;?(x0)<br/>
x<br/>
Also&#160;P&#160;?&#160;is&#160;the&#160;equilibrium&#160;distribution&#160;of&#160;T&#160;:<br/>
11<br/>
3/51<br/>
To&#160;machine&#160;precision:&#160;T&#160;10000&#160;=&#160;01/5&#160;=&#160;P&#160;?<br/>
@<br/>
A<br/>
@<br/>
A<br/>
0<br/>
1/5<br/>
Ergodicity&#160;requires:&#160;T&#160;K(x0&#160;‚Üê&#160;x)&#160;&gt;&#160;0&#160;for&#160;all&#160;x0&#160;:&#160;P&#160;?(x0)&#160;&gt;&#160;0,&#160;for&#160;some&#160;K<br/>
<hr/>
<a name=23></a><img src="./Murray_1-23_1.png"/><br/>
Detailed&#160;Balance<br/>
Detailed&#160;balance&#160;means&#160;‚Üí&#160;x&#160;‚Üí&#160;x0&#160;and&#160;‚Üí&#160;x0&#160;‚Üí&#160;x&#160;are&#160;equally&#160;probable:<br/>
T&#160;(x0&#160;‚Üê&#160;x)P&#160;?(x)&#160;=&#160;T&#160;(x&#160;‚Üê&#160;x0)P&#160;?(x0)<br/>
Detailed&#160;balance&#160;implies&#160;the&#160;invariant&#160;condition:<br/>
1<br/>
<br/>
*<br/>
<br/>
X&#160;T&#160;(x0‚Üêx)P&#160;?(x)&#160;=&#160;P&#160;?(x0)<br/>
<br/>
X<br/>
<br/>
<br/>
<br/>
T&#160;(x‚Üêx0)<br/>
<br/>
<br/>
<br/>
<br/>
x<br/>
<br/>
<br/>
x<br/>
<br/>
<br/>
Enforcing&#160;detailed&#160;balance&#160;is&#160;easy:&#160;it&#160;only&#160;involves&#160;isolated&#160;pairs<br/>
<hr/>
<a name=24></a>Reverse&#160;operators<br/>
If&#160;T&#160;satisfies&#160;stationarity,&#160;we&#160;can&#160;define&#160;a&#160;reverse&#160;operator<br/>
T&#160;(x0&#160;‚Üêx)&#160;P&#160;?(x)<br/>
T&#160;(x0&#160;‚Üêx)&#160;P&#160;?(x)<br/>
T&#160;(x‚Üêx0)&#160;‚àù&#160;T&#160;(x0&#160;‚Üêx)&#160;P&#160;?(x)&#160;=<br/>
=<br/>
.<br/>
e<br/>
P<br/>
T&#160;(x0&#160;‚Üêx)&#160;P&#160;?(x)<br/>
P&#160;?(x0)<br/>
x<br/>
Generalized&#160;balance&#160;condition:<br/>
T&#160;(x0&#160;‚Üêx)P&#160;?(x)&#160;=&#160;T&#160;(x‚Üêx0)P&#160;?(x0)<br/>
e<br/>
also&#160;implies&#160;the&#160;invariant&#160;condition&#160;and&#160;is&#160;necessary.<br/>
Operators&#160;satisfying&#160;detailed&#160;balance&#160;are&#160;their&#160;own&#160;reverse&#160;operator.<br/>
<hr/>
<a name=25></a>Metropolis‚ÄìHastings<br/>
Transition&#160;operator<br/>
‚Ä¢&#160;Propose&#160;a&#160;move&#160;from&#160;the&#160;current&#160;state&#160;Q(x0;&#160;x),&#160;e.g.&#160;N&#160;(x,&#160;œÉ2)<br/>‚Ä¢<br/>
<br/>
Accept&#160;with&#160;probability&#160;min1,&#160;P&#160;(x0)Q(x;x0)<br/>
P&#160;(x)Q(x0;x)<br/>
‚Ä¢&#160;Otherwise&#160;next&#160;state&#160;in&#160;chain&#160;is&#160;a&#160;copy&#160;of&#160;current&#160;state<br/>
Notes<br/>
‚Ä¢&#160;Can&#160;use&#160;Àú<br/>
P&#160;‚àù&#160;P&#160;(x);&#160;normalizer&#160;cancels&#160;in&#160;acceptance&#160;ratio<br/>
‚Ä¢&#160;Satisfies&#160;detailed&#160;balance&#160;(shown&#160;below)<br/>‚Ä¢&#160;Q&#160;must&#160;be&#160;chosen&#160;to&#160;fulfill&#160;the&#160;other&#160;technical&#160;requirements<br/>
&#160;<br/>
0<br/>
0&#160;!<br/>
0<br/>
0<br/>
P&#160;(x&#160;)Q(x;&#160;x&#160;)<br/>
0<br/>
0<br/>
0<br/>
P&#160;(x)&#160;¬∑&#160;T&#160;(x&#160;‚Üê&#160;x)&#160;=&#160;P&#160;(x)&#160;¬∑&#160;Q(x&#160;;&#160;x)&#160;min&#160;1,<br/>
=&#160;min‚ÄúP&#160;(x)Q(x&#160;;&#160;x),&#160;P&#160;(x&#160;)Q(x;&#160;x&#160;)‚Äù<br/>
P&#160;(x)Q(x0;&#160;x)<br/>
&#160;<br/>
0<br/>
!<br/>
P&#160;(x)Q(x&#160;;&#160;x)<br/>
=<br/>
0<br/>
0<br/>
0<br/>
0<br/>
P&#160;(x&#160;)¬∑Q(x;&#160;x&#160;)&#160;min&#160;1,<br/>
=&#160;P&#160;(x&#160;)¬∑T&#160;(x&#160;‚Üê&#160;x&#160;)<br/>
P&#160;(x0)Q(x;&#160;x0)<br/>
<hr/>
<a name=26></a>Matlab/Octave&#160;code&#160;for&#160;demo<br/>
function&#160;samples&#160;=&#160;dumb_metropolis(init,&#160;log_ptilde,&#160;iters,&#160;sigma)<br/>
D&#160;=&#160;numel(init);<br/>samples&#160;=&#160;zeros(D,&#160;iters);<br/>
state&#160;=&#160;init;<br/>Lp_state&#160;=&#160;log_ptilde(state);<br/>for&#160;ss&#160;=&#160;1:iters<br/>
%&#160;Propose<br/>prop&#160;=&#160;state&#160;+&#160;sigma*randn(size(state));<br/>Lp_prop&#160;=&#160;log_ptilde(prop);<br/>if&#160;log(rand)&#160;&lt;&#160;(Lp_prop&#160;-&#160;Lp_state)<br/>
%&#160;Accept<br/>state&#160;=&#160;prop;<br/>Lp_state&#160;=&#160;Lp_prop;<br/>
end<br/>samples(:,&#160;ss)&#160;=&#160;state(:);<br/>
end<br/>
<hr/>
<a name=27></a>Step-size&#160;demo<br/>
Explore&#160;N&#160;(0,&#160;1)&#160;with&#160;different&#160;step&#160;sizes&#160;œÉ<br/>
sigma&#160;=&#160;@(s)&#160;plot(dumb_metropolis(0,&#160;@(x)&#160;-0.5*x*x,&#160;1e3,&#160;s));<br/>
sigma(0.1)<br/>
4<br/>
2<br/>
0<br/>
99.8%&#160;accepts<br/>
‚àí2<br/>
‚àí40<br/>
100<br/>
200<br/>
300<br/>
400<br/>
500<br/>
600<br/>
700<br/>
800<br/>
900<br/>
1000<br/>
sigma(1)<br/>
4<br/>
2<br/>
0<br/>
68.4%&#160;accepts<br/>
‚àí2<br/>
‚àí40<br/>
100<br/>
200<br/>
300<br/>
400<br/>
500<br/>
600<br/>
700<br/>
800<br/>
900<br/>
1000<br/>
sigma(100)<br/>
4<br/>
2<br/>
0<br/>
0.5%&#160;accepts<br/>
‚àí2<br/>
‚àí40<br/>
100<br/>
200<br/>
300<br/>
400<br/>
500<br/>
600<br/>
700<br/>
800<br/>
900<br/>
1000<br/>
<hr/>
<a name=28></a>Metropolis&#160;limitations<br/>
P<br/>
Generic&#160;proposals&#160;use<br/>Q(x0;&#160;x)&#160;=&#160;N&#160;(x,&#160;œÉ2)<br/>
Q<br/>
œÉ&#160;large&#160;‚Üí&#160;many&#160;rejections<br/>
L<br/>
œÉ&#160;small&#160;‚Üí&#160;slow&#160;diffusion:<br/>
‚àº(L/œÉ)2&#160;iterations&#160;required<br/>
<hr/>
<a name=29></a>Combining&#160;operators<br/>
A&#160;sequence&#160;of&#160;operators,&#160;each&#160;with&#160;P&#160;?&#160;invariant:<br/>
x0&#160;‚àº&#160;P&#160;?(x)<br/>x1&#160;‚àº&#160;T<br/>
P&#160;(x<br/>
T<br/>
a(x1&#160;‚Üê&#160;x0)<br/>
1)&#160;=&#160;Px0&#160;a(x1&#160;‚Üêx0)P&#160;?(x0)&#160;=&#160;P&#160;?(x1)<br/>
x2&#160;‚àº&#160;T<br/>
P&#160;(x<br/>
T<br/>
b(x2&#160;‚Üê&#160;x1)<br/>
2)&#160;=&#160;Px1&#160;b(x2&#160;‚Üêx1)P&#160;?(x1)&#160;=&#160;P&#160;?(x2)<br/>
x3&#160;‚àº&#160;Tc(x3‚Üêx2)<br/>
P&#160;(x3)&#160;=&#160;P&#160;T<br/>
x1&#160;c(x3&#160;‚Üê&#160;x2)P&#160;?(x2)&#160;=&#160;P&#160;?(x3)<br/>
¬∑&#160;¬∑&#160;¬∑<br/>
¬∑&#160;¬∑&#160;¬∑<br/>
‚Äî&#160;Combination&#160;TcTbTa&#160;leaves&#160;P&#160;?&#160;invariant<br/>
‚Äî&#160;If&#160;they&#160;can&#160;reach&#160;any&#160;x,&#160;TcTbTa&#160;is&#160;a&#160;valid&#160;MCMC&#160;operator<br/>
‚Äî&#160;Individually&#160;Tc,&#160;Tb&#160;and&#160;Ta&#160;need&#160;not&#160;be&#160;ergodic<br/>
<hr/>
<a name=30></a>Gibbs&#160;sampling<br/>
z2<br/>
L<br/>
A&#160;method&#160;with&#160;no&#160;rejections:<br/>
‚Äì&#160;Initialize&#160;x&#160;to&#160;some&#160;value<br/>‚Äì&#160;Pick&#160;each&#160;variable&#160;in&#160;turn&#160;or&#160;randomly<br/>
l<br/>
and&#160;resample&#160;P&#160;(xi|xj6=i)<br/>
z1<br/>
Figure&#160;from&#160;PRML,&#160;Bishop&#160;(2006)<br/>
Proof&#160;of&#160;validity:&#160;a)&#160;check&#160;detailed&#160;balance&#160;for&#160;component&#160;update.<br/>b)&#160;Metropolis‚ÄìHastings&#160;‚Äòproposals‚Äô&#160;P&#160;(xi|xj6=i)&#160;‚áí&#160;accept&#160;with&#160;prob.&#160;1<br/>Apply&#160;a&#160;series&#160;of&#160;these&#160;operators.&#160;Don‚Äôt&#160;need&#160;to&#160;check&#160;acceptance.<br/>
<hr/>
<a name=31></a>Gibbs&#160;sampling<br/>
Alternative&#160;explanation:<br/>
Chain&#160;is&#160;currently&#160;at&#160;x<br/>
At&#160;equilibrium&#160;can&#160;assume&#160;x&#160;‚àº&#160;P&#160;(x)<br/>
Consistent&#160;with&#160;xj6=i&#160;‚àº&#160;P&#160;(xj6=i),&#160;xi&#160;‚àº&#160;P&#160;(xi&#160;|&#160;xj6=i)<br/>
Pretend&#160;xi&#160;was&#160;never&#160;sampled&#160;and&#160;do&#160;it&#160;again.<br/>
This&#160;view&#160;may&#160;be&#160;useful&#160;later&#160;for&#160;non-parametric&#160;applications<br/>
<hr/>
<a name=32></a>‚ÄúRoutine‚Äù&#160;Gibbs&#160;sampling<br/>
Gibbs&#160;sampling&#160;benefits&#160;from&#160;few&#160;free&#160;choices&#160;and<br/>convenient&#160;features&#160;of&#160;conditional&#160;distributions:<br/>
‚Ä¢&#160;Conditionals&#160;with&#160;a&#160;few&#160;discrete&#160;settings&#160;can&#160;be&#160;explicitly&#160;normalized:<br/>
P&#160;(xi|xj6=i)&#160;‚àù&#160;P&#160;(xi,&#160;xj6=i)<br/>
P&#160;(x<br/>
=<br/>
i,&#160;xj6=i)<br/>
P<br/>
P&#160;(x0,&#160;x<br/>
x0<br/>
i<br/>
j6=i)&#160;‚Üê&#160;this&#160;sum&#160;is&#160;small&#160;and&#160;easy<br/>
i<br/>
‚Ä¢&#160;Continuous&#160;conditionals&#160;only&#160;univariate<br/>
‚áí&#160;amenable&#160;to&#160;standard&#160;sampling&#160;methods.<br/>
WinBUGS&#160;and&#160;OpenBUGS&#160;sample&#160;graphical&#160;models&#160;using&#160;these&#160;tricks<br/>
<hr/>
<a name=33></a>Summary&#160;so&#160;far<br/>
‚Ä¢&#160;We&#160;need&#160;approximate&#160;methods&#160;to&#160;solve&#160;sums/integrals<br/>‚Ä¢&#160;Monte&#160;Carlo&#160;does&#160;not&#160;explicitly&#160;depend&#160;on&#160;dimension,<br/>
although&#160;simple&#160;methods&#160;work&#160;only&#160;in&#160;low&#160;dimensions<br/>
‚Ä¢&#160;Markov&#160;chain&#160;Monte&#160;Carlo&#160;(MCMC)&#160;can&#160;make&#160;local&#160;moves.<br/>
By&#160;assuming&#160;less,&#160;it‚Äôs&#160;more&#160;applicable&#160;to&#160;higher&#160;dimensions<br/>
‚Ä¢&#160;simple&#160;computations&#160;‚áí&#160;‚Äúeasy‚Äù&#160;to&#160;implement<br/>
(harder&#160;to&#160;diagnose).<br/>
How&#160;do&#160;we&#160;use&#160;these&#160;MCMC&#160;samples?<br/>
<hr/>
<a name=34></a>End&#160;of&#160;Lecture&#160;1<br/>
<hr/>
<a name=35></a><img src="./Murray_1-35_1.png"/><br/>
Quick&#160;review<br/>
Construct&#160;a&#160;biased&#160;random&#160;walk&#160;that&#160;explores&#160;a&#160;target&#160;dist.<br/>
Markov&#160;steps,&#160;x(s)&#160;‚àº&#160;T&#160;x(s)&#160;‚Üê&#160;x(s‚àí1)<br/>
MCMC&#160;gives&#160;approximate,<br/>correlated&#160;samples<br/>
1&#160;S<br/>
X<br/>
EP&#160;[f&#160;]&#160;‚âà<br/>
f(x(s))<br/>
S&#160;s=1<br/>
Example&#160;transitions:<br/>
<br/>
P&#160;(x0)&#160;Q(x;&#160;x0)<br/>
Metropolis‚ÄìHastings:&#160;T&#160;(x0&#160;‚Üê&#160;x)&#160;=&#160;Q(x0;&#160;x)&#160;min&#160;1,&#160;P(x)Q(x0;x)<br/>
Gibbs&#160;sampling:&#160;Ti(x0&#160;‚Üê&#160;x)&#160;=&#160;P&#160;(x0&#160;|&#160;x<br/>
‚àí&#160;x<br/>
i<br/>
j6=i)&#160;Œ¥(x0j6=i<br/>
j6=i)<br/>
<hr/>
<a name=36></a>How&#160;should&#160;we&#160;run&#160;MCMC?<br/>
‚Ä¢&#160;The&#160;samples&#160;aren‚Äôt&#160;independent.&#160;Should&#160;we&#160;thin,<br/>
only&#160;keep&#160;every&#160;Kth&#160;sample?<br/>
‚Ä¢&#160;Arbitrary&#160;initialization&#160;means&#160;starting&#160;iterations&#160;are&#160;bad.<br/>
Should&#160;we&#160;discard&#160;a&#160;‚Äúburn-in‚Äù&#160;period?<br/>
‚Ä¢&#160;Maybe&#160;we&#160;should&#160;perform&#160;multiple&#160;runs?<br/>‚Ä¢&#160;How&#160;do&#160;we&#160;know&#160;if&#160;we&#160;have&#160;run&#160;for&#160;long&#160;enough?<br/>
<hr/>
<a name=37></a>Forming&#160;estimates<br/>
Approximately&#160;independent&#160;samples&#160;can&#160;be&#160;obtained&#160;by&#160;thinning.<br/>However,&#160;all&#160;the&#160;samples&#160;can&#160;be&#160;used.<br/>
Use&#160;the&#160;simple&#160;Monte&#160;Carlo&#160;estimator&#160;on&#160;MCMC&#160;samples.&#160;It&#160;is:<br/>
‚Äî&#160;consistent<br/>‚Äî&#160;unbiased&#160;if&#160;the&#160;chain&#160;has&#160;‚Äúburned&#160;in‚Äù<br/>
The&#160;correct&#160;motivation&#160;to&#160;thin:&#160;if&#160;computing&#160;f&#160;(x(s))&#160;is&#160;expensive<br/>
<hr/>
<a name=38></a><img src="./Murray_1-38_1.png"/><br/>
Empirical&#160;diagnostics<br/>
Rasmussen&#160;(2000)<br/>
Recommendations<br/>
For&#160;diagnostics:<br/>Standard&#160;software&#160;packages&#160;like&#160;R-CODA<br/>
For&#160;opinion&#160;on&#160;thinning,&#160;multiple&#160;runs,&#160;burn&#160;in,&#160;etc.<br/>
Practical&#160;Markov&#160;chain&#160;Monte&#160;Carlo<br/>Charles&#160;J.&#160;Geyer,&#160;Statistical&#160;Science.&#160;7(4):473‚Äì483,&#160;1992.<br/>http://www.jstor.org/stable/2246094<br/>
<hr/>
<a name=39></a>Consistency&#160;checks<br/>
Do&#160;I&#160;get&#160;the&#160;right&#160;answer&#160;on&#160;tiny&#160;versions<br/>of&#160;my&#160;problem?<br/>
Can&#160;I&#160;make&#160;good&#160;inferences&#160;about&#160;synthetic&#160;data<br/>drawn&#160;from&#160;my&#160;model?<br/>
Getting&#160;it&#160;right:&#160;joint&#160;distribution&#160;tests&#160;of&#160;posterior&#160;simulators,<br/>John&#160;Geweke,&#160;JASA,&#160;99(467):799‚Äì804,&#160;2004.<br/>
[next:&#160;using&#160;the&#160;samples]<br/>
<hr/>
<a name=40></a>Making&#160;good&#160;use&#160;of&#160;samples<br/>
Is&#160;the&#160;standard&#160;estimator&#160;too&#160;noisy?<br/>
e.g.&#160;need&#160;many&#160;samples&#160;from&#160;a<br/>distribution&#160;to&#160;estimate&#160;its&#160;tail<br/>
We&#160;can&#160;often&#160;do&#160;some&#160;analytic&#160;calculations<br/>
<hr/>
<a name=41></a>Finding&#160;P&#160;(xi&#160;=&#160;1)<br/>
Method&#160;1:&#160;fraction&#160;of&#160;time&#160;xi&#160;=&#160;1<br/>
1&#160;S<br/>
P&#160;(x<br/>
X<br/>
i&#160;=&#160;1)&#160;=&#160;X&#160;I(xi&#160;=&#160;1)P&#160;(xi)&#160;‚âà<br/>
),&#160;x(s)&#160;‚àº&#160;P&#160;(x<br/>
S<br/>
I(x(s)<br/>
i<br/>
i<br/>
i)<br/>
xi<br/>
s=1<br/>
Method&#160;2:&#160;average&#160;of&#160;P&#160;(xi&#160;=&#160;1|x\i)<br/>
P&#160;(xi&#160;=1)&#160;=&#160;X&#160;P&#160;(xi&#160;=1|x\i)P&#160;(x\i)<br/>
x\i<br/>
1&#160;S<br/>
‚âà&#160;X&#160;P&#160;(x<br/>
(s)),<br/>
(s)&#160;‚àº&#160;P(<br/>
S<br/>
i&#160;=&#160;1|x<br/>
x<br/>
x<br/>
\i<br/>
\i<br/>
\i)<br/>
s=1<br/>
Example&#160;of&#160;‚ÄúRao-Blackwellization‚Äù.&#160;See&#160;also&#160;‚Äúwaste&#160;recycling‚Äù.<br/>
<hr/>
<a name=42></a>Processing&#160;samples<br/>
This&#160;is&#160;easy<br/>
1&#160;S<br/>
I&#160;=&#160;X&#160;f(x<br/>
X<br/>
i)P&#160;(x)&#160;‚âà<br/>
f(x(s)),<br/>
S<br/>
x(s)&#160;‚àº&#160;P&#160;(x)<br/>
i<br/>
x<br/>
s=1<br/>
But&#160;this&#160;might&#160;be&#160;better<br/>
<br/>
<br/>
I&#160;=&#160;X&#160;f(x<br/>
X<br/>
i)P&#160;(xi|x\i)P&#160;(x\i)&#160;=&#160;X<br/>
f(xi)P&#160;(xi|x\i)&#160;P&#160;(x\i)<br/>
x<br/>
x\i<br/>
xi<br/>
1&#160;S&#160;<br/>
<br/>
‚âà&#160;X&#160;X&#160;f(x<br/>
(s))&#160;,<br/>
(s)&#160;‚àº&#160;P(<br/>
S<br/>
i)P&#160;(xi|x<br/>
x<br/>
x<br/>
\i<br/>
\i<br/>
\i)<br/>
s=1<br/>
xi<br/>
A&#160;more&#160;general&#160;form&#160;of&#160;‚ÄúRao-Blackwellization‚Äù.<br/>
<hr/>
<a name=43></a>Summary&#160;so&#160;far<br/>
‚Ä¢&#160;MCMC&#160;algorithms&#160;are&#160;general&#160;and&#160;often&#160;easy&#160;to&#160;implement<br/>
‚Ä¢&#160;Running&#160;them&#160;is&#160;a&#160;bit&#160;messy.&#160;.&#160;.<br/>
.&#160;.&#160;.&#160;but&#160;there&#160;are&#160;some&#160;established&#160;procedures.<br/>
‚Ä¢&#160;Given&#160;the&#160;samples&#160;there&#160;might&#160;be&#160;a&#160;choice&#160;of&#160;estimators<br/>
Next&#160;question:<br/>Is&#160;MCMC&#160;research&#160;all&#160;about&#160;finding&#160;a&#160;good&#160;Q(x)?<br/>
<hr/>
<a name=44></a>Auxiliary&#160;variables<br/>
The&#160;point&#160;of&#160;MCMC&#160;is&#160;to&#160;marginalize&#160;out&#160;variables,<br/>but&#160;one&#160;can&#160;introduce&#160;more&#160;variables:<br/>
Z<br/>
Z<br/>
f(x)P&#160;(x)&#160;dx&#160;=<br/>
f(x)P&#160;(x,&#160;v)&#160;dx&#160;dv<br/>
1&#160;S<br/>
‚âà&#160;X&#160;f(x(s)),&#160;x,&#160;v&#160;‚àº&#160;P&#160;(x,&#160;v)<br/>
S&#160;s=1<br/>
We&#160;might&#160;want&#160;to&#160;do&#160;this&#160;if<br/>
‚Ä¢&#160;P&#160;(x|v)&#160;and&#160;P&#160;(v|x)&#160;are&#160;simple<br/>
‚Ä¢&#160;P&#160;(x,&#160;v)&#160;is&#160;otherwise&#160;easier&#160;to&#160;navigate<br/>
<hr/>
<a name=45></a>Swendsen‚ÄìWang&#160;(1987)<br/>
Seminal&#160;algorithm&#160;using&#160;auxiliary&#160;variables<br/>
Edwards&#160;and&#160;Sokal&#160;(1988)&#160;identified&#160;and&#160;generalized&#160;the<br/>‚ÄúFortuin-Kasteleyn-Swendsen-Wang‚Äù&#160;auxiliary&#160;variable&#160;joint<br/>distribution&#160;that&#160;underlies&#160;the&#160;algorithm.<br/>
<hr/>
<a name=46></a>Slice&#160;sampling&#160;idea<br/>
Sample&#160;point&#160;uniformly&#160;under&#160;curve&#160;Àú<br/>
P&#160;(x)&#160;‚àù&#160;P&#160;(x)<br/>
Àú<br/>
P&#160;(x)<br/>
(x,&#160;u)<br/>
u<br/>
x<br/>
p(u|x)&#160;=&#160;Uniform[0,&#160;Àú<br/>
P&#160;(x)]<br/>
(1&#160;Àú<br/>
P&#160;(x)&#160;‚â•&#160;u<br/>
p(x|u)&#160;‚àù<br/>
=&#160;‚ÄúUniform&#160;on&#160;the&#160;slice‚Äù<br/>
0&#160;otherwise<br/>
<hr/>
<a name=47></a>Slice&#160;sampling<br/>
Unimodal&#160;conditionals<br/>
(x,&#160;u)<br/>
(x,&#160;u)<br/>
(x,&#160;u)<br/>
u<br/>
u<br/>
u<br/>
x<br/>
x<br/>
x<br/>
‚Ä¢&#160;bracket&#160;slice<br/>‚Ä¢&#160;sample&#160;uniformly&#160;within&#160;bracket<br/>‚Ä¢&#160;shrink&#160;bracket&#160;if&#160;Àú<br/>
P&#160;(x)&#160;&lt;&#160;u&#160;(off&#160;slice)<br/>
‚Ä¢&#160;accept&#160;first&#160;point&#160;on&#160;the&#160;slice<br/>
<hr/>
<a name=48></a>Slice&#160;sampling<br/>
Multimodal&#160;conditionals<br/>
Àú<br/>
P&#160;(x)<br/>
(x,&#160;u)<br/>
u<br/>
x<br/>
‚Ä¢&#160;place&#160;bracket&#160;randomly&#160;around&#160;point<br/>‚Ä¢&#160;linearly&#160;step&#160;out&#160;until&#160;bracket&#160;ends&#160;are&#160;off&#160;slice<br/>‚Ä¢&#160;sample&#160;on&#160;bracket,&#160;shrinking&#160;as&#160;before<br/>
Satisfies&#160;detailed&#160;balance,&#160;leaves&#160;p(x|u)&#160;invariant<br/>
<hr/>
<a name=49></a>Slice&#160;sampling<br/>
Advantages&#160;of&#160;slice-sampling:<br/>
‚Ä¢&#160;Easy&#160;‚Äî&#160;only&#160;require&#160;Àú<br/>
P&#160;(x)&#160;‚àù&#160;P&#160;(x)&#160;pointwise<br/>
‚Ä¢&#160;No&#160;rejections<br/>‚Ä¢&#160;Step-size&#160;parameters&#160;less&#160;important&#160;than&#160;Metropolis<br/>
More&#160;advanced&#160;versions&#160;of&#160;slice&#160;sampling&#160;have&#160;been&#160;developed.<br/>Neal&#160;(2003)&#160;contains&#160;many&#160;ideas.<br/>
<hr/>
<a name=50></a>Hamiltonian&#160;dynamics<br/>
Construct&#160;a&#160;landscape&#160;with&#160;gravitational&#160;potential&#160;energy,&#160;E(x):<br/>
P&#160;(x)&#160;‚àù&#160;e‚àíE(x),<br/>
E(x)&#160;=&#160;‚àí&#160;log&#160;P&#160;‚àó(x)<br/>
Introduce&#160;velocity&#160;v&#160;carrying&#160;kinetic&#160;energy&#160;K(v)&#160;=&#160;v&gt;v/2<br/>
Some&#160;physics:<br/>
‚Ä¢&#160;Total&#160;energy&#160;or&#160;Hamiltonian,&#160;H&#160;=&#160;E(x)&#160;+&#160;K(v)<br/>
‚Ä¢&#160;Frictionless&#160;ball&#160;rolling&#160;(x,&#160;v)‚Üí(x0,&#160;v0)&#160;satisfies&#160;H(x0,&#160;v0)&#160;=&#160;H(x,&#160;v)<br/>
‚Ä¢&#160;Ideal&#160;Hamiltonian&#160;dynamics&#160;are&#160;time&#160;reversible:<br/>
‚Äì&#160;reverse&#160;v&#160;and&#160;the&#160;ball&#160;will&#160;return&#160;to&#160;its&#160;start&#160;point<br/>
<hr/>
<a name=51></a>Hamiltonian&#160;Monte&#160;Carlo<br/>
Define&#160;a&#160;joint&#160;distribution:<br/>
‚Ä¢&#160;P&#160;(x,&#160;v)&#160;‚àù&#160;e‚àíE(x)e‚àíK(v)&#160;=&#160;e‚àíE(x)‚àíK(v)&#160;=&#160;e‚àíH(x,v)<br/>‚Ä¢&#160;Velocity&#160;is&#160;independent&#160;of&#160;position&#160;and&#160;Gaussian&#160;distributed<br/>
Markov&#160;chain&#160;operators<br/>
‚Ä¢&#160;Gibbs&#160;sample&#160;velocity<br/>‚Ä¢&#160;Simulate&#160;Hamiltonian&#160;dynamics&#160;then&#160;flip&#160;sign&#160;of&#160;velocity<br/>
‚Äì&#160;Hamiltonian&#160;‚Äòproposal‚Äô&#160;is&#160;deterministic&#160;and&#160;reversible<br/>
q(x0,&#160;v0;&#160;x,&#160;v)&#160;=&#160;q(x,&#160;v;&#160;x0,&#160;v0)&#160;=&#160;1<br/>
‚Äì&#160;Conservation&#160;of&#160;energy&#160;means&#160;P&#160;(x,&#160;v)&#160;=&#160;P&#160;(x0,&#160;v0)<br/>‚Äì&#160;Metropolis&#160;acceptance&#160;probability&#160;is&#160;1<br/>
Except&#160;we&#160;can‚Äôt&#160;simulate&#160;Hamiltonian&#160;dynamics&#160;exactly<br/>
<hr/>
<a name=52></a>Leap-frog&#160;dynamics<br/>
a&#160;discrete&#160;approximation&#160;to&#160;Hamiltonian&#160;dynamics:<br/>
&#160;‚àÇE(x(t))<br/>
vi(t&#160;+&#160;2)&#160;=&#160;vi(t)&#160;‚àí&#160;2&#160;‚àÇxi<br/>xi(t&#160;+&#160;)&#160;=&#160;xi(t)&#160;+&#160;vi(t&#160;+&#160;2)<br/>
&#160;‚àÇE(x(t&#160;+&#160;))<br/>
pi(t&#160;+&#160;)&#160;=&#160;vi(t&#160;+&#160;2)&#160;‚àí&#160;2<br/>
‚àÇxi<br/>
‚Ä¢&#160;H&#160;is&#160;not&#160;conserved<br/>
‚Ä¢&#160;dynamics&#160;are&#160;still&#160;deterministic&#160;and&#160;reversible<br/>
‚Ä¢&#160;Acceptance&#160;probability&#160;becomes&#160;min[1,&#160;exp(H(v,&#160;x)&#160;‚àí&#160;H(v0,&#160;x0))]<br/>
<hr/>
<a name=53></a>Hamiltonian&#160;Monte&#160;Carlo<br/>
The&#160;algorithm:<br/>
‚Ä¢&#160;Gibbs&#160;sample&#160;velocity&#160;‚àº&#160;N&#160;(0,&#160;I)<br/>
‚Ä¢&#160;Simulate&#160;Leapfrog&#160;dynamics&#160;for&#160;L&#160;steps<br/>
‚Ä¢&#160;Accept&#160;new&#160;position&#160;with&#160;probability<br/>
min[1,&#160;exp(H(v,&#160;x)&#160;‚àí&#160;H(v0,&#160;x0))]<br/>
The&#160;original&#160;name&#160;is&#160;Hybrid&#160;Monte&#160;Carlo,&#160;with&#160;reference&#160;to&#160;the<br/>‚Äúhybrid‚Äù&#160;dynamical&#160;simulation&#160;method&#160;on&#160;which&#160;it&#160;was&#160;based.<br/>
<hr/>
<a name=54></a>Summary&#160;of&#160;auxiliary&#160;variables<br/>
‚Äî&#160;Swendsen‚ÄìWang<br/>‚Äî&#160;Slice&#160;sampling<br/>‚Äî&#160;Hamiltonian&#160;(Hybrid)&#160;Monte&#160;Carlo<br/>
A&#160;fair&#160;amount&#160;of&#160;my&#160;research&#160;(not&#160;covered&#160;in&#160;this&#160;tutorial)&#160;has&#160;been<br/>finding&#160;the&#160;right&#160;auxiliary&#160;representation&#160;on&#160;which&#160;to&#160;run&#160;standard<br/>MCMC&#160;updates.<br/>
Example&#160;benefits:<br/>
Population&#160;methods&#160;to&#160;give&#160;better&#160;mixing&#160;and&#160;exploit&#160;parallel&#160;hardware<br/>
Being&#160;robust&#160;to&#160;bad&#160;random&#160;number&#160;generators<br/>
Removing&#160;step-size&#160;parameters&#160;when&#160;slice&#160;sample&#160;doesn‚Äôt&#160;really&#160;apply<br/>
<hr/>
<a name=55></a>Finding&#160;normalizers&#160;is&#160;hard<br/>
Prior&#160;sampling:&#160;like&#160;finding&#160;fraction&#160;of&#160;needles&#160;in&#160;a&#160;hay-stack<br/>
Z<br/>
P&#160;(D|M)&#160;=<br/>
P&#160;(D|Œ∏,&#160;M)P&#160;(Œ∏|M)&#160;dŒ∏<br/>
1&#160;S<br/>
=&#160;X&#160;P&#160;(D|Œ∏(s),&#160;M),&#160;Œ∏(s)&#160;‚àº&#160;P&#160;(Œ∏|M)<br/>
S&#160;s=1<br/>
.&#160;.&#160;.&#160;usually&#160;has&#160;huge&#160;variance<br/>
Similarly&#160;for&#160;undirected&#160;graphs:<br/>
P&#160;‚àó(<br/>
P&#160;(<br/>
x)<br/>
x)&#160;=<br/>
,<br/>
Z&#160;=&#160;X&#160;P&#160;‚àó(<br/>
Z<br/>
x)<br/>
x<br/>
I&#160;will&#160;use&#160;this&#160;as&#160;an&#160;easy-to-illustrate&#160;case-study<br/>
<hr/>
<a name=56></a><img src="./Murray_1-56_1.png"/><br/>
<img src="./Murray_1-56_2.png"/><br/>
<img src="./Murray_1-56_3.png"/><br/>
Benchmark&#160;experiment<br/>
Training&#160;set<br/>
RBM&#160;samples<br/>
MoB&#160;samples<br/>
RBM&#160;setup:<br/>
‚Äî&#160;28&#160;√ó&#160;28&#160;=&#160;784&#160;binary&#160;visible&#160;variables<br/>‚Äî&#160;500&#160;binary&#160;hidden&#160;variables<br/>
Goal:&#160;Compare&#160;P&#160;(x)&#160;on&#160;test&#160;set,&#160;(PRBM(x)&#160;=&#160;P&#160;‚àó(x)/Z)<br/>
<hr/>
<a name=57></a>Simple&#160;Importance&#160;Sampling<br/>
P&#160;‚àó(<br/>
1&#160;S&#160;P&#160;‚àó(<br/>
Z&#160;=&#160;X<br/>
x)&#160;Q(<br/>
X<br/>
x(s)),<br/>
Q(<br/>
x)&#160;‚âà<br/>
x(s)&#160;‚àº&#160;Q(x)<br/>
x)<br/>
S<br/>
Q(x)<br/>
x<br/>
s=1<br/>
x(1)&#160;=<br/>
,<br/>
x(2)&#160;=<br/>
,<br/>
x(3)&#160;=<br/>
,<br/>
x(4)&#160;=<br/>
,<br/>
x(5)&#160;=<br/>
,<br/>
x(6)&#160;=<br/>
,.&#160;.&#160;.<br/>
1<br/>
2D&#160;S<br/>
Z&#160;=&#160;2D&#160;X<br/>
P&#160;‚àó(<br/>
X&#160;P&#160;‚àó(<br/>
2<br/>
x)&#160;‚âà<br/>
x(s)),<br/>
x(s)&#160;‚àº&#160;Uniform<br/>
D<br/>
S<br/>
x<br/>
s=1<br/>
<hr/>
<a name=58></a><img src="./Murray_1-58_1.png"/><br/>
<img src="./Murray_1-58_2.png"/><br/>
<img src="./Murray_1-58_3.png"/><br/>
<img src="./Murray_1-58_4.png"/><br/>
<img src="./Murray_1-58_5.png"/><br/>
<img src="./Murray_1-58_6.png"/><br/>
‚ÄúPosterior‚Äù&#160;Sampling<br/>
P&#160;‚àó(x)<br/>
<br/>
P&#160;(D|Œ∏)P&#160;(Œ∏)<br/>
Sample&#160;from&#160;P&#160;(x)&#160;=<br/>
Z&#160;,<br/>
or&#160;P&#160;(Œ∏|D)&#160;=<br/>
P&#160;(D)<br/>
x(1)&#160;=<br/>
,<br/>
x(2)&#160;=<br/>
,<br/>
x(3)&#160;=<br/>
,<br/>
x(4)&#160;=<br/>
,<br/>
x(5)&#160;=<br/>
,<br/>
x(6)&#160;=<br/>
,.&#160;.&#160;.<br/>
1&#160;S&#160;P&#160;‚àó(<br/>
Z&#160;=&#160;X&#160;P&#160;‚àó(<br/>
X<br/>
x)<br/>
x)<br/>
Z&#160;‚Äú‚âà‚Äù<br/>
=&#160;Z<br/>
S<br/>
P&#160;(x)<br/>
x<br/>
s=1<br/>
<hr/>
<a name=59></a>Finding&#160;a&#160;Volume<br/>
‚Üí&#160;x<br/>
‚Üì<br/>P&#160;‚àó(x)<br/>
Lake&#160;analogy&#160;and&#160;figure&#160;from&#160;MacKay&#160;textbook&#160;(2003)<br/>
<hr/>
<a name=60></a><img src="./Murray_1-60_1.png"/><br/>
<img src="./Murray_1-60_2.png"/><br/>
<img src="./Murray_1-60_3.png"/><br/>
<img src="./Murray_1-60_4.png"/><br/>
Annealing&#160;/&#160;Tempering<br/>
e.g.&#160;P&#160;(x;&#160;Œ≤)&#160;‚àù&#160;P&#160;‚àó(x)Œ≤&#160;œÄ(x)(1‚àíŒ≤)<br/>
Œ≤&#160;=&#160;0<br/>
Œ≤&#160;=&#160;0.01<br/>
Œ≤&#160;=&#160;0.1<br/>
Œ≤&#160;=&#160;0.25<br/>
Œ≤&#160;=&#160;0.5<br/>
Œ≤&#160;=&#160;1<br/>
1/Œ≤&#160;=&#160;‚Äútemperature‚Äù<br/>
<hr/>
<a name=61></a>Using&#160;other&#160;distributions<br/>
Chain&#160;between&#160;posterior&#160;and&#160;prior:<br/>
1<br/>
e.g.&#160;P&#160;(Œ∏;&#160;Œ≤)&#160;=<br/>
P&#160;(D|Œ∏)Œ≤P&#160;(Œ∏)<br/>
Z(Œ≤)<br/>
Œ≤&#160;=&#160;0<br/>
Œ≤&#160;=&#160;0.01<br/>
Œ≤&#160;=&#160;0.1<br/>
Œ≤&#160;=&#160;0.25<br/>
Œ≤&#160;=&#160;0.5<br/>
Œ≤&#160;=&#160;1<br/>
Advantages:<br/>
‚Ä¢&#160;mixing&#160;easier&#160;at&#160;low&#160;Œ≤,&#160;good&#160;initialization&#160;for&#160;higher&#160;Œ≤?<br/>
Z(1)<br/>
Z(Œ≤<br/>
Z(Œ≤<br/>
Z(Œ≤<br/>
Z(Œ≤<br/>
Z(1)<br/>
‚Ä¢<br/>
=<br/>
1)&#160;¬∑<br/>
2)&#160;¬∑<br/>
3)&#160;¬∑<br/>
4)&#160;¬∑<br/>
Z(0)<br/>
Z(0)&#160;Z(Œ≤1)&#160;Z(Œ≤2)&#160;Z(Œ≤3)&#160;Z(Œ≤4)<br/>
Related&#160;to&#160;annealing&#160;or&#160;tempering,&#160;1/Œ≤&#160;=&#160;‚Äútemperature‚Äù<br/>
<hr/>
<a name=62></a>Parallel&#160;tempering<br/>
Normal&#160;MCMC&#160;transitions&#160;+&#160;swap&#160;proposals&#160;on&#160;P&#160;(X)&#160;=&#160;Y&#160;P&#160;(X;&#160;Œ≤)<br/>
Œ≤<br/>
P&#160;(x)<br/>
T1<br/>
T1<br/>
T1<br/>
T1<br/>
T1<br/>
T<br/>
T<br/>
T<br/>
T<br/>
T<br/>
P<br/>
Œ≤1<br/>
Œ≤1<br/>
Œ≤1<br/>
Œ≤1<br/>
Œ≤1<br/>
Œ≤&#160;(x)<br/>
1<br/>
T<br/>
T<br/>
T<br/>
T<br/>
T<br/>
P<br/>
Œ≤2<br/>
Œ≤2<br/>
Œ≤2<br/>
Œ≤2<br/>
Œ≤2<br/>
Œ≤&#160;(x)<br/>
2<br/>
T<br/>
T<br/>
T<br/>
T<br/>
T<br/>
P<br/>
Œ≤3<br/>
Œ≤3<br/>
Œ≤3<br/>
Œ≤3<br/>
Œ≤3<br/>
Œ≤&#160;(x)<br/>
3<br/>
Problems&#160;/&#160;trade-offs:<br/>
‚Ä¢&#160;obvious&#160;space&#160;cost<br/>‚Ä¢&#160;need&#160;to&#160;equilibriate&#160;larger&#160;system<br/>‚Ä¢&#160;information&#160;from&#160;low&#160;Œ≤&#160;diffuses&#160;up&#160;by&#160;slow&#160;random&#160;walk<br/>
<hr/>
<a name=63></a>Tempered&#160;transitions<br/>
Drive&#160;temperature&#160;up.&#160;.&#160;.<br/>
P&#160;(X)&#160;:<br/>
ÀÜ<br/>
T<br/>
Àá<br/>
Œ≤<br/>
T<br/>
K<br/>
¬ØxK<br/>
Œ≤K<br/>
ÀÜxK‚àí1<br/>
ÀáxK‚àí1<br/>
ÀÜ<br/>
T<br/>
Àá<br/>
Œ≤<br/>
T<br/>
2<br/>
ÀÜx2<br/>
Àáx2<br/>
Œ≤<br/>
ÀÜ<br/>
2<br/>
T<br/>
Àá<br/>
Œ≤<br/>
T<br/>
1<br/>
ÀÜx1<br/>
Àáx1<br/>
Œ≤1<br/>
ÀÜx0<br/>
Àáx0<br/>
ÀÜx0&#160;‚àº&#160;P&#160;(x)<br/>
.&#160;.&#160;.&#160;and&#160;back&#160;down<br/>
Proposal:&#160;swap&#160;order&#160;of&#160;points&#160;so&#160;final&#160;point&#160;Àá<br/>
x0&#160;putatively&#160;‚àº&#160;P&#160;(x)<br/>
Acceptance&#160;probability:<br/>
&#34;<br/>
P&#160;(ÀÜx<br/>
P&#160;(ÀÜx<br/>
P<br/>
(Àáx<br/>
P&#160;(Àáx&#160;#<br/>
min&#160;1,&#160;Œ≤1&#160;0)&#160;¬∑&#160;¬∑&#160;¬∑&#160;Œ≤K&#160;K‚àí1)&#160;Œ≤K‚àí1&#160;K‚àí1)&#160;¬∑&#160;¬∑&#160;¬∑<br/>
0)<br/>
P&#160;(ÀÜx0)<br/>
PŒ≤<br/>
(ÀÜx<br/>
P&#160;(Àáx<br/>
P&#160;(Àáx<br/>
K‚àí1<br/>
0)<br/>
Œ≤K<br/>
K‚àí1)<br/>
Œ≤1<br/>
0)<br/>
<hr/>
<a name=64></a>Annealed&#160;Importance&#160;Sampling<br/>
x0&#160;‚àº&#160;p0(x)<br/>
Àú<br/>
Àú<br/>
Àú<br/>
P&#160;(X)&#160;:<br/>
x<br/>
T1<br/>
T2<br/>
TK<br/>
0<br/>
x1<br/>
x2<br/>
xK‚àí1<br/>
xK<br/>
Q(X)&#160;:<br/>
x0<br/>
x1<br/>
x2<br/>
xK‚àí1<br/>
xK<br/>
T1<br/>
T2<br/>
TK<br/>
xK&#160;‚àº&#160;pK+1(x)<br/>
P&#160;‚àó(<br/>
K<br/>
K<br/>
P(X)&#160;=<br/>
xK)&#160;Y&#160;T<br/>
Y&#160;T<br/>
Z<br/>
ek(xk‚àí1;&#160;xk),<br/>
Q(X)&#160;=&#160;œÄ(x0)<br/>
k(xk;&#160;xk‚àí1)<br/>
k=1<br/>
k=1<br/>
Then&#160;standard&#160;importance&#160;sampling&#160;of&#160;P(X)&#160;=&#160;P‚àó(X)&#160;with&#160;Q(X)<br/>
Z<br/>
<hr/>
<a name=65></a><img src="./Murray_1-65_1.png"/><br/>
<img src="./Murray_1-65_2.png"/><br/>
<img src="./Murray_1-65_3.png"/><br/>
<img src="./Murray_1-65_4.png"/><br/>
Annealed&#160;Importance&#160;Sampling<br/>
1&#160;S&#160;P‚àó(X)<br/>
Z&#160;‚âà&#160;X<br/>
S<br/>
Q(X)<br/>
<b>259</b><br/>
&#160;<br/>
<b>Estimated logZ</b><br/>
s=1<br/>
<b>258</b><br/>
<b>True logZ</b><br/>
3.3 min<br/>
<b>257</b><br/>
33 min<br/>
17 min<br/>
5.5 hrs<br/>
<b>256</b><br/>
20 sec<br/>
<b>log Z&#160;255</b><br/>
<b>254</b><br/>
Q‚Üì<br/>
‚ÜëP<br/>
<b>253</b><br/>
Large Variance<br/>
<b>252</b>&#160;<br/>
<b>&#160;10 &#160;</b><br/>
<b>&#160;100&#160;</b><br/>
<b>&#160;500&#160;</b><br/>
<b>1000&#160;</b><br/>
<b>10000</b><br/>
<b>Number of AIS runs</b><br/>
<hr/>
<a name=66></a>Summary&#160;on&#160;Z<br/>
Whirlwind&#160;tour&#160;of&#160;roughly&#160;how&#160;to&#160;find&#160;Z&#160;with&#160;Monte&#160;Carlo<br/>
The&#160;algorithms&#160;really&#160;have&#160;to&#160;be&#160;good&#160;at&#160;exploring&#160;the&#160;distribution<br/>
These&#160;are&#160;also&#160;the&#160;Monte&#160;Carlo&#160;approaches&#160;to&#160;watch&#160;for&#160;general&#160;use<br/>on&#160;the&#160;hardest&#160;problems.<br/>
Can&#160;be&#160;useful&#160;for&#160;optimization&#160;too.<br/>
See&#160;the&#160;references&#160;for&#160;more.<br/>
<hr/>
<a name=67></a>References<br/>
<hr/>
<a name=68></a>Further&#160;reading&#160;(1/2)<br/>
General&#160;references:<br/>
Probabilistic&#160;inference&#160;using&#160;Markov&#160;chain&#160;Monte&#160;Carlo&#160;methods,&#160;Radford&#160;M.&#160;Neal,&#160;Technical&#160;report:&#160;CRG-TR-93-1,<br/>Department&#160;of&#160;Computer&#160;Science,&#160;University&#160;of&#160;Toronto,&#160;1993.&#160;http://www.cs.toronto.edu/~radford/review.abstract.html<br/>
Various&#160;figures&#160;and&#160;more&#160;came&#160;from&#160;(see&#160;also&#160;references&#160;therein):<br/>
Advances&#160;in&#160;Markov&#160;chain&#160;Monte&#160;Carlo&#160;methods.&#160;Iain&#160;Murray.&#160;2007.&#160;http://www.cs.toronto.edu/~murray/pub/07thesis/<br/>
Information&#160;theory,&#160;inference,&#160;and&#160;learning&#160;algorithms.&#160;David&#160;MacKay,&#160;2003.&#160;http://www.inference.phy.cam.ac.uk/mackay/itila/<br/>
Pattern&#160;recognition&#160;and&#160;machine&#160;learning.&#160;Christopher&#160;M.&#160;Bishop.&#160;2006.&#160;http://research.microsoft.com/~cmbishop/PRML/<br/>
Specific&#160;points:<br/>
If&#160;you&#160;do&#160;Gibbs&#160;sampling&#160;with&#160;continuous&#160;distributions&#160;this&#160;method,&#160;which&#160;I&#160;omitted&#160;for&#160;material-overload&#160;reasons,&#160;may&#160;help:<br/>Suppressing&#160;random&#160;walks&#160;in&#160;Markov&#160;chain&#160;Monte&#160;Carlo&#160;using&#160;ordered&#160;overrelaxation,&#160;Radford&#160;M.&#160;Neal,&#160;Learning&#160;in&#160;graphical&#160;models,<br/>M.&#160;I.&#160;Jordan&#160;(editor),&#160;205‚Äì228,&#160;Kluwer&#160;Academic&#160;Publishers,&#160;1998.&#160;http://www.cs.toronto.edu/~radford/overk.abstract.html<br/>
An&#160;example&#160;of&#160;picking&#160;estimators&#160;carefully:<br/>Speed-up&#160;of&#160;Monte&#160;Carlo&#160;simulations&#160;by&#160;sampling&#160;of&#160;rejected&#160;states,&#160;Frenkel,&#160;D,&#160;Proceedings&#160;of&#160;the&#160;National&#160;Academy&#160;of&#160;Sciences,<br/>101(51):17571‚Äì17575,&#160;The&#160;National&#160;Academy&#160;of&#160;Sciences,&#160;2004.&#160;http://www.pnas.org/cgi/content/abstract/101/51/17571<br/>
A&#160;key&#160;reference&#160;for&#160;auxiliary&#160;variable&#160;methods&#160;is:<br/>Generalizations&#160;of&#160;the&#160;Fortuin-Kasteleyn-Swendsen-Wang&#160;representation&#160;and&#160;Monte&#160;Carlo&#160;algorithm,&#160;Robert&#160;G.&#160;Edwards&#160;and&#160;A.&#160;D.&#160;Sokal,<br/>Physical&#160;Review,&#160;38:2009‚Äì2012,&#160;1988.<br/>
Slice&#160;sampling,&#160;Radford&#160;M.&#160;Neal,&#160;Annals&#160;of&#160;Statistics,&#160;31(3):705‚Äì767,&#160;2003.&#160;http://www.cs.toronto.edu/~radford/slice-aos.abstract.html<br/>
Bayesian&#160;training&#160;of&#160;backpropagation&#160;networks&#160;by&#160;the&#160;hybrid&#160;Monte&#160;Carlo&#160;method,&#160;Radford&#160;M.&#160;Neal,<br/>Technical&#160;report:&#160;CRG-TR-92-1,&#160;Connectionist&#160;Research&#160;Group,&#160;University&#160;of&#160;Toronto,&#160;1992.<br/>
http://www.cs.toronto.edu/~radford/bbp.abstract.html<br/>
An&#160;early&#160;reference&#160;for&#160;parallel&#160;tempering:<br/>Markov&#160;chain&#160;Monte&#160;Carlo&#160;maximum&#160;likelihood,&#160;Geyer,&#160;C.&#160;J,&#160;Computing&#160;Science&#160;and&#160;Statistics:&#160;Proceedings&#160;of&#160;the&#160;23rd&#160;Symposium&#160;on&#160;the<br/>Interface,&#160;156‚Äì163,&#160;1991.<br/>
Sampling&#160;from&#160;multimodal&#160;distributions&#160;using&#160;tempered&#160;transitions,&#160;Radford&#160;M.&#160;Neal,&#160;Statistics&#160;and&#160;Computing,&#160;6(4):353‚Äì366,&#160;1996.<br/>
<hr/>
<a name=69></a>Further&#160;reading&#160;(2/2)<br/>
Software:<br/>
Gibbs&#160;sampling&#160;for&#160;graphical&#160;models:&#160;http://mathstat.helsinki.fi/openbugs/<br/>Neural&#160;networks&#160;and&#160;other&#160;flexible&#160;models:&#160;http://www.cs.utoronto.ca/~radford/fbm.software.html<br/>
CODA:&#160;http://www-fis.iarc.fr/coda/<br/>
Other&#160;Monte&#160;Carlo&#160;methods:<br/>
Nested&#160;sampling&#160;is&#160;a&#160;new&#160;Monte&#160;Carlo&#160;method&#160;with&#160;some&#160;interesting&#160;properties:<br/>Nested&#160;sampling&#160;for&#160;general&#160;Bayesian&#160;computation,&#160;John&#160;Skilling,&#160;Bayesian&#160;Analysis,&#160;2006.<br/>(to&#160;appear,&#160;posted&#160;online&#160;June&#160;5).&#160;http://ba.stat.cmu.edu/journal/forthcoming/skilling.pdf<br/>
Approaches&#160;based&#160;on&#160;the&#160;‚Äúmulti-canonicle&#160;ensemble‚Äù&#160;also&#160;solve&#160;some&#160;of&#160;the&#160;problems&#160;with&#160;traditional&#160;tempterature-based&#160;methods:<br/>Multicanonical&#160;ensemble:&#160;a&#160;new&#160;approach&#160;to&#160;simulate&#160;first-order&#160;phase&#160;transitions,&#160;Bernd&#160;A.&#160;Berg&#160;and&#160;Thomas&#160;Neuhaus,&#160;Phys.&#160;Rev.&#160;Lett,<br/>68(1):9‚Äì12,&#160;1992.&#160;http://prola.aps.org/abstract/PRL/v68/i1/p9&#160;1<br/>
A&#160;good&#160;review&#160;paper:<br/>Extended&#160;Ensemble&#160;Monte&#160;Carlo.&#160;Y&#160;Iba.&#160;Int&#160;J&#160;Mod&#160;Phys&#160;C&#160;[Computational&#160;Physics&#160;and&#160;Physical&#160;Computation]&#160;12(5):623-656.&#160;2001.<br/>
Particle&#160;filters&#160;/&#160;Sequential&#160;Monte&#160;Carlo&#160;are&#160;famously&#160;successful&#160;in&#160;time&#160;series&#160;modelling,&#160;but&#160;are&#160;more&#160;generally&#160;applicable.<br/>This&#160;may&#160;be&#160;a&#160;good&#160;place&#160;to&#160;start:&#160;http://www.cs.ubc.ca/~arnaud/journals.html<br/>
Exact&#160;or&#160;perfect&#160;sampling&#160;uses&#160;Markov&#160;chain&#160;simulation&#160;but&#160;suffers&#160;no&#160;initialization&#160;bias.&#160;An&#160;amazing&#160;feat&#160;when&#160;it&#160;can&#160;be&#160;performed:<br/>Annotated&#160;bibliography&#160;of&#160;perfectly&#160;random&#160;sampling&#160;with&#160;Markov&#160;chains,&#160;David&#160;B.&#160;Wilson<br/>
http://dbwilson.com/exact/<br/>
MCMC&#160;does&#160;not&#160;apply&#160;to&#160;doubly-intractable&#160;distributions.&#160;For&#160;what&#160;that&#160;even&#160;means&#160;and&#160;possible&#160;solutions&#160;see:<br/>An&#160;efficient&#160;Markov&#160;chain&#160;Monte&#160;Carlo&#160;method&#160;for&#160;distributions&#160;with&#160;intractable&#160;normalising&#160;constants,&#160;J.&#160;M√∏ller,&#160;A.&#160;N.&#160;Pettitt,&#160;R.&#160;Reeves&#160;and<br/>K.&#160;K.&#160;Berthelsen,&#160;Biometrika,&#160;93(2):451‚Äì458,&#160;2006.<br/>MCMC&#160;for&#160;doubly-intractable&#160;distributions,&#160;Iain&#160;Murray,&#160;Zoubin&#160;Ghahramani&#160;and&#160;David&#160;J.&#160;C.&#160;MacKay,&#160;Proceedings&#160;of&#160;the&#160;22nd&#160;Annual<br/>Conference&#160;on&#160;Uncertainty&#160;in&#160;Artificial&#160;Intelligence&#160;(UAI-06),&#160;Rina&#160;Dechter&#160;and&#160;Thomas&#160;S.&#160;Richardson&#160;(editors),&#160;359‚Äì366,&#160;AUAI&#160;Press,&#160;2006.<br/>
http://www.gatsby.ucl.ac.uk/~iam23/pub/06doubly&#160;intractable/doubly&#160;intractable.pdf<br/>
<hr/>
</body>
</html>
