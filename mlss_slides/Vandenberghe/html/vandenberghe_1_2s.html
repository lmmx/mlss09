<!DOCTYPE html><html>
<head>
<title></title>
<style type="text/css">
<!--
.xflip {
    -moz-transform: scaleX(-1);
    -webkit-transform: scaleX(-1);
    -o-transform: scaleX(-1);
    transform: scaleX(-1);
    filter: fliph;
}
.yflip {
    -moz-transform: scaleY(-1);
    -webkit-transform: scaleY(-1);
    -o-transform: scaleY(-1);
    transform: scaleY(-1);
    filter: flipv;
}
.xyflip {
    -moz-transform: scaleX(-1) scaleY(-1);
    -webkit-transform: scaleX(-1) scaleY(-1);
    -o-transform: scaleX(-1) scaleY(-1);
    transform: scaleX(-1) scaleY(-1);
    filter: fliph + flipv;
}
-->
</style>
</head>
<body>
<a name=1></a>Convex&#160;Optimization<br/>
Lieven&#160;Vandenberghe<br/>
University&#160;of&#160;California,&#160;Los&#160;Angeles<br/>
Tutorial&#160;lectures,&#160;Machine&#160;Learning&#160;Summer&#160;School<br/>
University&#160;of&#160;Cambridge,&#160;September&#160;3-4,&#160;2009<br/>
Sources:<br/>
•&#160;Boyd&#160;&amp;&#160;Vandenberghe,&#160;Convex&#160;Optimization,&#160;2004<br/>•&#160;Courses&#160;EE236B,&#160;EE236C&#160;(UCLA),&#160;EE364A,&#160;EE364B&#160;(Stephen&#160;Boyd,&#160;Stanford&#160;Univ.)<br/>
<hr/>
<a name=2></a>Convex&#160;optimization&#160;—&#160;MLSS&#160;2009<br/>
Introduction<br/>
•&#160;mathematical&#160;optimization,&#160;modeling,&#160;complexity<br/>
•&#160;convex&#160;optimization<br/>
•&#160;recent&#160;history<br/>
1<br/>
<hr/>
<a name=3></a>Mathematical&#160;optimization<br/>
minimize<br/>
f0(x1,&#160;.&#160;.&#160;.&#160;,&#160;xn)<br/>
subject&#160;to&#160;f1(x1,&#160;.&#160;.&#160;.&#160;,&#160;xn)&#160;≤&#160;0<br/>
.&#160;.&#160;.<br/>fm(x1,&#160;.&#160;.&#160;.&#160;,&#160;xn)&#160;≤&#160;0<br/>
•&#160;x&#160;=&#160;(x1,&#160;x2,&#160;.&#160;.&#160;.&#160;,&#160;xn)&#160;are&#160;decision&#160;variables<br/>
•&#160;f0(x1,&#160;x2,&#160;.&#160;.&#160;.&#160;,&#160;xn)&#160;gives&#160;the&#160;cost&#160;of&#160;choosing&#160;x<br/>
•&#160;inequalities&#160;give&#160;constraints&#160;that&#160;x&#160;must&#160;satisfy<br/>
a&#160;mathematical&#160;model&#160;of&#160;a&#160;decision,&#160;design,&#160;or&#160;estimation&#160;problem<br/>
Introduction<br/>
2<br/>
<hr/>
<a name=4></a>Limits&#160;of&#160;mathematical&#160;optimization<br/>
•&#160;how&#160;realistic&#160;is&#160;the&#160;model,&#160;and&#160;how&#160;certain&#160;are&#160;we&#160;about&#160;it?<br/>
•&#160;is&#160;the&#160;optimization&#160;problem&#160;tractable&#160;by&#160;existing&#160;numerical&#160;algorithms?<br/>
Optimization&#160;research<br/>
•&#160;modeling<br/>
generic&#160;techniques&#160;for&#160;formulating&#160;tractable&#160;optimization&#160;problems<br/>
•&#160;algorithms<br/>
expand&#160;class&#160;of&#160;problems&#160;that&#160;can&#160;be&#160;efficiently&#160;solved<br/>
Introduction<br/>
3<br/>
<hr/>
<a name=5></a>Complexity&#160;of&#160;nonlinear&#160;optimization<br/>
•&#160;the&#160;general&#160;optimization&#160;problem&#160;is&#160;intractable<br/>
•&#160;even&#160;simple&#160;looking&#160;optimization&#160;problems&#160;can&#160;be&#160;very&#160;hard<br/>
Examples<br/>
•&#160;quadratic&#160;optimization&#160;problem&#160;with&#160;many&#160;constraints<br/>
•&#160;minimizing&#160;a&#160;multivariate&#160;polynomial<br/>
Introduction<br/>
4<br/>
<hr/>
<a name=6></a>The&#160;famous&#160;exception:&#160;Linear&#160;programming<br/>
n<br/>
X<br/>
minimize<br/>
cT&#160;x&#160;=<br/>
cixi<br/>
i=1<br/>
subject&#160;to&#160;aTi&#160;x&#160;≤&#160;bi,&#160;i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;m<br/>
•&#160;widely&#160;used&#160;since&#160;Dantzig&#160;introduced&#160;the&#160;simplex&#160;algorithm&#160;in&#160;1948<br/>
•&#160;since&#160;1950s,&#160;many&#160;applications&#160;in&#160;operations&#160;research,&#160;network<br/>
optimization,&#160;finance,&#160;engineering,.&#160;.&#160;.<br/>
•&#160;extensive&#160;theory&#160;(optimality&#160;conditions,&#160;sensitivity,&#160;.&#160;.&#160;.&#160;)<br/>
•&#160;there&#160;exist&#160;very&#160;efficient&#160;algorithms&#160;for&#160;solving&#160;linear&#160;programs<br/>
Introduction<br/>
5<br/>
<hr/>
<a name=7></a>Solving&#160;linear&#160;programs<br/>
•&#160;no&#160;closed-form&#160;expression&#160;for&#160;solution<br/>
•&#160;widely&#160;available&#160;and&#160;reliable&#160;software<br/>
•&#160;for&#160;some&#160;algorithms,&#160;can&#160;prove&#160;polynomial&#160;time<br/>
•&#160;problems&#160;with&#160;over&#160;105&#160;variables&#160;or&#160;constraints&#160;solved&#160;routinely<br/>
Introduction<br/>
6<br/>
<hr/>
<a name=8></a>Convex&#160;optimization&#160;problem<br/>
minimize<br/>
f0(x)<br/>
subject&#160;to&#160;f1(x)&#160;≤&#160;0<br/>
·&#160;·&#160;·<br/>fm(x)&#160;≤&#160;0<br/>
•&#160;objective&#160;and&#160;constraint&#160;functions&#160;are&#160;convex:&#160;for&#160;0&#160;≤&#160;θ&#160;≤&#160;1<br/>
fi(θx&#160;+&#160;(1&#160;−&#160;θ)y)&#160;≤&#160;θfi(x)&#160;+&#160;(1&#160;−&#160;θ)fi(y)<br/>
•&#160;includes&#160;least-squares&#160;problems&#160;and&#160;linear&#160;programs&#160;as&#160;special&#160;cases<br/>
•&#160;can&#160;be&#160;solved&#160;exactly,&#160;with&#160;similar&#160;complexity&#160;as&#160;LPs<br/>
•&#160;surprisingly&#160;many&#160;problems&#160;can&#160;be&#160;solved&#160;via&#160;convex&#160;optimization<br/>
Introduction<br/>
7<br/>
<hr/>
<a name=9></a>History<br/>
•&#160;1940s:&#160;linear&#160;programming<br/>
minimize<br/>
cT&#160;x<br/>
subject&#160;to&#160;aTi&#160;x&#160;≤&#160;bi,&#160;i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;m<br/>
•&#160;1950s:&#160;quadratic&#160;programming<br/>
•&#160;1960s:&#160;geometric&#160;programming<br/>
•&#160;1990s:&#160;semidefinite&#160;programming,&#160;second-order&#160;cone&#160;programming,<br/>
quadratically&#160;constrained&#160;quadratic&#160;programming,&#160;robust&#160;optimization,<br/>sum-of-squares&#160;programming,&#160;.&#160;.&#160;.<br/>
Introduction<br/>
8<br/>
<hr/>
<a name=10></a>New&#160;applications&#160;since&#160;1990<br/>
•&#160;linear&#160;matrix&#160;inequality&#160;techniques&#160;in&#160;control<br/>
•&#160;circuit&#160;design&#160;via&#160;geometric&#160;programming<br/>
•&#160;support&#160;vector&#160;machine&#160;learning&#160;via&#160;quadratic&#160;programming<br/>
•&#160;semidefinite&#160;programming&#160;relaxations&#160;in&#160;combinatorial&#160;optimization<br/>
•&#160;ℓ1-norm&#160;optimization&#160;for&#160;sparse&#160;signal&#160;reconstruction<br/>
•&#160;applications&#160;in&#160;structural&#160;optimization,&#160;statistics,&#160;signal&#160;processing,<br/>
communications,&#160;image&#160;processing,&#160;computer&#160;vision,&#160;quantum<br/>information&#160;theory,&#160;finance,&#160;.&#160;.&#160;.<br/>
Introduction<br/>
9<br/>
<hr/>
<a name=11></a>Algorithms<br/>
Interior-point&#160;methods<br/>
•&#160;1984&#160;(Karmarkar):&#160;first&#160;practical&#160;polynomial-time&#160;algorithm<br/>
•&#160;1984-1990:&#160;efficient&#160;implementations&#160;for&#160;large-scale&#160;LPs<br/>
•&#160;around&#160;1990&#160;(Nesterov&#160;&amp;&#160;Nemirovski):&#160;polynomial-time&#160;interior-point<br/>
methods&#160;for&#160;nonlinear&#160;convex&#160;programming<br/>
•&#160;since&#160;1990:&#160;extensions&#160;and&#160;high-quality&#160;software&#160;packages<br/>
First-order&#160;algorithms<br/>
•&#160;similar&#160;to&#160;gradient&#160;descent,&#160;but&#160;with&#160;better&#160;convergence&#160;properties<br/>
•&#160;based&#160;on&#160;Nesterov’s&#160;‘optimal’&#160;methods&#160;from&#160;1980s<br/>
•&#160;extend&#160;to&#160;certain&#160;nondifferentiable&#160;or&#160;constrained&#160;problems<br/>
Introduction<br/>
10<br/>
<hr/>
<a name=12></a>Outline<br/>
•&#160;basic&#160;theory<br/>
–&#160;convex&#160;sets&#160;and&#160;functions<br/>–&#160;convex&#160;optimization&#160;problems<br/>–&#160;linear,&#160;quadratic,&#160;and&#160;geometric&#160;programming<br/>
•&#160;cone&#160;linear&#160;programming&#160;and&#160;applications<br/>
–&#160;second-order&#160;cone&#160;programming<br/>–&#160;semidefinite&#160;programming<br/>
•&#160;some&#160;recent&#160;developments&#160;in&#160;algorithms&#160;(since&#160;1990)<br/>
–&#160;interior-point&#160;methods<br/>–&#160;fast&#160;gradient&#160;methods<br/>
10<br/>
<hr/>
<a name=13></a>Convex&#160;optimization&#160;—&#160;MLSS&#160;2009<br/>
Convex&#160;sets&#160;and&#160;functions<br/>
•&#160;definition<br/>
•&#160;basic&#160;examples&#160;and&#160;properties<br/>
•&#160;operations&#160;that&#160;preserve&#160;convexity<br/>
11<br/>
<hr/>
<a name=14></a>Convex&#160;set<br/>
contains&#160;line&#160;segment&#160;between&#160;any&#160;two&#160;points&#160;in&#160;the&#160;set<br/>
x1,&#160;x2&#160;∈&#160;C,<br/>
0&#160;≤&#160;θ&#160;≤&#160;1<br/>
=⇒<br/>
θx1&#160;+&#160;(1&#160;−&#160;θ)x2&#160;∈&#160;C<br/>
Examples:&#160;one&#160;convex,&#160;two&#160;nonconvex&#160;sets<br/>
Convex&#160;sets&#160;and&#160;functions<br/>
12<br/>
<hr/>
<a name=15></a>Examples&#160;and&#160;properties<br/>
•&#160;solution&#160;set&#160;of&#160;linear&#160;equations&#160;Ax&#160;=&#160;b&#160;(affine&#160;set)<br/>
•&#160;solution&#160;set&#160;of&#160;linear&#160;inequalities&#160;Ax&#160;&#160;b&#160;(polyhedron)<br/>
•&#160;norm&#160;balls&#160;{x&#160;|&#160;kxk&#160;≤&#160;R}&#160;and&#160;norm&#160;cones&#160;{(x,&#160;t)&#160;|&#160;kxk&#160;≤&#160;t}<br/>
•&#160;set&#160;of&#160;positive&#160;semidefinite&#160;matrices&#160;Sn+&#160;=&#160;{X&#160;∈&#160;Sn&#160;|&#160;X&#160;&#160;0}<br/>
•&#160;image&#160;of&#160;a&#160;convex&#160;set&#160;under&#160;a&#160;linear&#160;transformation&#160;is&#160;convex<br/>
•&#160;inverse&#160;image&#160;of&#160;a&#160;convex&#160;set&#160;under&#160;a&#160;linear&#160;transformation&#160;is&#160;convex<br/>
•&#160;intersection&#160;of&#160;convex&#160;sets&#160;is&#160;convex<br/>
Convex&#160;sets&#160;and&#160;functions<br/>
13<br/>
<hr/>
<a name=16></a>Example&#160;of&#160;intersection&#160;property<br/>
C&#160;=&#160;{x&#160;∈&#160;Rn&#160;|&#160;|p(t)|&#160;≤&#160;1&#160;for&#160;|t|&#160;≤&#160;π/3}<br/>
where&#160;p(t)&#160;=&#160;x1&#160;cos&#160;t&#160;+&#160;x2&#160;cos&#160;2t&#160;+&#160;·&#160;·&#160;·&#160;+&#160;xn&#160;cos&#160;nt<br/>
2<br/>
1<br/>
1<br/>
0<br/>
(t)<br/>
2&#160;0<br/>
C<br/>
−1<br/>
p<br/>
x<br/>
−1<br/>
−2<br/>
0<br/>
π/3<br/>
2π/3<br/>
π<br/>
−2<br/>
−1<br/>
0<br/>
1<br/>
2<br/>
t<br/>
x1<br/>
C&#160;is&#160;intersection&#160;of&#160;infinitely&#160;many&#160;halfspaces,&#160;hence&#160;convex<br/>
Convex&#160;sets&#160;and&#160;functions<br/>
14<br/>
<hr/>
<a name=17></a>Convex&#160;function<br/>
domain&#160;dom&#160;f&#160;is&#160;a&#160;convex&#160;set&#160;and<br/>
f&#160;(θx&#160;+&#160;(1&#160;−&#160;θ)y)&#160;≤&#160;θf(x)&#160;+&#160;(1&#160;−&#160;θ)f(y)<br/>
for&#160;all&#160;x,&#160;y&#160;∈&#160;dom&#160;f,&#160;0&#160;≤&#160;θ&#160;≤&#160;1<br/>
(y,&#160;f&#160;(y))<br/>
(x,&#160;f&#160;(x))<br/>
f&#160;is&#160;concave&#160;if&#160;−f&#160;is&#160;convex<br/>
Convex&#160;sets&#160;and&#160;functions<br/>
15<br/>
<hr/>
<a name=18></a>Epigraph&#160;and&#160;sublevel&#160;set<br/>
Epigraph:&#160;epi&#160;f&#160;=&#160;{(x,&#160;t)&#160;|&#160;x&#160;∈&#160;dom&#160;f,&#160;f(x)&#160;≤&#160;t}<br/>
epi&#160;f<br/>
a&#160;function&#160;is&#160;convex&#160;if&#160;and&#160;only&#160;its<br/>
f<br/>
epigraph&#160;is&#160;a&#160;convex&#160;set<br/>
Sublevel&#160;sets:&#160;Cα&#160;=&#160;{x&#160;∈&#160;dom&#160;f&#160;|&#160;f(x)&#160;≤&#160;α}<br/>
the&#160;sublevel&#160;sets&#160;of&#160;a&#160;convex&#160;function&#160;are&#160;convex&#160;(converse&#160;is&#160;false)<br/>
Convex&#160;sets&#160;and&#160;functions<br/>
16<br/>
<hr/>
<a name=19></a>Examples<br/>
•&#160;exp&#160;x,&#160;−&#160;log&#160;x,&#160;x&#160;log&#160;x&#160;are&#160;convex<br/>
•&#160;xα&#160;is&#160;convex&#160;for&#160;x&#160;&gt;&#160;0&#160;and&#160;α&#160;≥&#160;1&#160;or&#160;α&#160;≤&#160;0;&#160;|x|α&#160;is&#160;convex&#160;for&#160;α&#160;≥&#160;1<br/>
•&#160;quadratic-over-linear&#160;function&#160;xT&#160;x/t&#160;is&#160;convex&#160;in&#160;x,&#160;t&#160;for&#160;t&#160;&gt;&#160;0<br/>
•&#160;geometric&#160;mean&#160;(x1x2&#160;·&#160;·&#160;·&#160;xn)1/n&#160;is&#160;concave&#160;for&#160;x&#160;&#160;0<br/>
•&#160;log&#160;det&#160;X&#160;is&#160;concave&#160;on&#160;set&#160;of&#160;positive&#160;definite&#160;matrices<br/>
•&#160;log(ex1&#160;+&#160;·&#160;·&#160;·&#160;exn)&#160;is&#160;convex<br/>
•&#160;linear&#160;and&#160;affine&#160;functions&#160;are&#160;convex&#160;and&#160;concave<br/>
•&#160;norms&#160;are&#160;convex<br/>
Convex&#160;sets&#160;and&#160;functions<br/>
17<br/>
<hr/>
<a name=20></a>Differentiable&#160;convex&#160;functions<br/>
differentiable&#160;f&#160;is&#160;convex&#160;if&#160;and&#160;only&#160;if&#160;dom&#160;f&#160;is&#160;convex&#160;and<br/>
f&#160;(y)&#160;≥&#160;f(x)&#160;+&#160;∇f(x)T&#160;(y&#160;−&#160;x)&#160;for&#160;all&#160;x,&#160;y&#160;∈&#160;dom&#160;f<br/>
f&#160;(y)<br/>
f&#160;(x)&#160;+&#160;∇f&#160;(x)T&#160;(y&#160;−&#160;x)<br/>
(x,&#160;f&#160;(x))<br/>
twice&#160;differentiable&#160;f&#160;is&#160;convex&#160;if&#160;and&#160;only&#160;if&#160;dom&#160;f&#160;is&#160;convex&#160;and<br/>
∇2f(x)&#160;&#160;0&#160;for&#160;all&#160;x&#160;∈&#160;dom&#160;f<br/>
Convex&#160;sets&#160;and&#160;functions<br/>
18<br/>
<hr/>
<a name=21></a>Operations&#160;that&#160;preserve&#160;convexity<br/>
methods&#160;for&#160;establishing&#160;convexity&#160;of&#160;a&#160;function<br/>
1.&#160;verify&#160;definition<br/>
2.&#160;for&#160;twice&#160;differentiable&#160;functions,&#160;show&#160;∇2f(x)&#160;&#160;0<br/>
3.&#160;show&#160;that&#160;f&#160;is&#160;obtained&#160;from&#160;simple&#160;convex&#160;functions&#160;by&#160;operations<br/>
that&#160;preserve&#160;convexity<br/>
•&#160;nonnegative&#160;weighted&#160;sum<br/>•&#160;composition&#160;with&#160;affine&#160;function<br/>•&#160;pointwise&#160;maximum&#160;and&#160;supremum<br/>•&#160;composition<br/>•&#160;minimization<br/>•&#160;perspective<br/>
Convex&#160;sets&#160;and&#160;functions<br/>
19<br/>
<hr/>
<a name=22></a>Positive&#160;weighted&#160;sum&#160;&amp;&#160;composition&#160;with&#160;affine&#160;function<br/>
Nonnegative&#160;multiple:&#160;αf&#160;is&#160;convex&#160;if&#160;f&#160;is&#160;convex,&#160;α&#160;≥&#160;0<br/>
Sum:&#160;f1&#160;+&#160;f2&#160;convex&#160;if&#160;f1,&#160;f2&#160;convex&#160;(extends&#160;to&#160;infinite&#160;sums,&#160;integrals)<br/>
Composition&#160;with&#160;affine&#160;function:&#160;f&#160;(Ax&#160;+&#160;b)&#160;is&#160;convex&#160;if&#160;f&#160;is&#160;convex<br/>
Examples<br/>
•&#160;log&#160;barrier&#160;for&#160;linear&#160;inequalities<br/>
m<br/>
X<br/>
f&#160;(x)&#160;=&#160;−<br/>
log(bi&#160;−&#160;aTi&#160;x)<br/>
i=1<br/>
•&#160;(any)&#160;norm&#160;of&#160;affine&#160;function:&#160;f(x)&#160;=&#160;kAx&#160;+&#160;bk<br/>
Convex&#160;sets&#160;and&#160;functions<br/>
20<br/>
<hr/>
<a name=23></a>Pointwise&#160;maximum<br/>
f&#160;(x)&#160;=&#160;max{f1(x),&#160;.&#160;.&#160;.&#160;,&#160;fm(x)}<br/>
is&#160;convex&#160;if&#160;f1,&#160;.&#160;.&#160;.&#160;,&#160;fm&#160;are&#160;convex<br/>
Example:&#160;sum&#160;of&#160;r&#160;largest&#160;components&#160;of&#160;x&#160;∈&#160;Rn<br/>
f&#160;(x)&#160;=&#160;x[1]&#160;+&#160;x[2]&#160;+&#160;·&#160;·&#160;·&#160;+&#160;x[r]<br/>
is&#160;convex&#160;(x[i]&#160;is&#160;ith&#160;largest&#160;component&#160;of&#160;x)<br/>
proof:<br/>
f&#160;(x)&#160;=&#160;max{xi&#160;+&#160;x&#160;+&#160;·&#160;·&#160;·&#160;+&#160;x<br/>
1<br/>
i2<br/>
ir&#160;|&#160;1&#160;≤&#160;i1&#160;&lt;&#160;i2&#160;&lt;&#160;·&#160;·&#160;·&#160;&lt;&#160;ir&#160;≤&#160;n}<br/>
Convex&#160;sets&#160;and&#160;functions<br/>
21<br/>
<hr/>
<a name=24></a>Pointwise&#160;supremum<br/>
g(x)&#160;=&#160;sup&#160;f&#160;(x,&#160;y)<br/>
y∈A<br/>
is&#160;convex&#160;if&#160;f&#160;(x,&#160;y)&#160;is&#160;convex&#160;in&#160;x&#160;for&#160;each&#160;y&#160;∈&#160;A<br/>
Example:&#160;maximum&#160;eigenvalue&#160;of&#160;symmetric&#160;matrix<br/>
λmax(X)&#160;=&#160;sup&#160;yT&#160;Xy<br/>
kyk2=1<br/>
Convex&#160;sets&#160;and&#160;functions<br/>
22<br/>
<hr/>
<a name=25></a>Composition&#160;with&#160;scalar&#160;functions<br/>
composition&#160;of&#160;g&#160;:&#160;Rn&#160;→&#160;R&#160;and&#160;h&#160;:&#160;R&#160;→&#160;R:<br/>
f&#160;(x)&#160;=&#160;h(g(x))<br/>
f&#160;is&#160;convex&#160;if<br/>
g&#160;convex,&#160;h&#160;convex&#160;and&#160;nondecreasing<br/>g&#160;concave,&#160;h&#160;convex&#160;and&#160;nonincreasing<br/>
(if&#160;we&#160;assign&#160;h(x)&#160;=&#160;∞&#160;for&#160;x&#160;∈&#160;dom&#160;h)<br/>
Examples<br/>
•&#160;exp&#160;g(x)&#160;is&#160;convex&#160;if&#160;g&#160;is&#160;convex<br/>
•&#160;1/g(x)&#160;is&#160;convex&#160;if&#160;g&#160;is&#160;concave&#160;and&#160;positive<br/>
Convex&#160;sets&#160;and&#160;functions<br/>
23<br/>
<hr/>
<a name=26></a>Vector&#160;composition<br/>
composition&#160;of&#160;g&#160;:&#160;Rn&#160;→&#160;Rk&#160;and&#160;h&#160;:&#160;Rk&#160;→&#160;R:<br/>
f&#160;(x)&#160;=&#160;h(g(x))&#160;=&#160;h&#160;(g1(x),&#160;g2(x),&#160;.&#160;.&#160;.&#160;,&#160;gk(x))<br/>
f&#160;is&#160;convex&#160;if<br/>
gi&#160;convex,&#160;h&#160;convex&#160;and&#160;nondecreasing&#160;in&#160;each&#160;argument<br/>gi&#160;concave,&#160;h&#160;convex&#160;and&#160;nonincreasing&#160;in&#160;each&#160;argument<br/>
(if&#160;we&#160;assign&#160;h(x)&#160;=&#160;∞&#160;for&#160;x&#160;∈&#160;dom&#160;h)<br/>
Examples<br/>
•&#160;Pm&#160;log&#160;g<br/>
i=1<br/>
i(x)&#160;is&#160;concave&#160;if&#160;gi&#160;are&#160;concave&#160;and&#160;positive<br/>
•&#160;log&#160;Pm&#160;exp&#160;g<br/>
i=1<br/>
i(x)&#160;is&#160;convex&#160;if&#160;gi&#160;are&#160;convex<br/>
Convex&#160;sets&#160;and&#160;functions<br/>
24<br/>
<hr/>
<a name=27></a>Minimization<br/>
g(x)&#160;=&#160;inf&#160;f&#160;(x,&#160;y)<br/>
y∈C<br/>
is&#160;convex&#160;if&#160;f&#160;(x,&#160;y)&#160;is&#160;convex&#160;in&#160;x,&#160;y&#160;and&#160;C&#160;is&#160;a&#160;convex&#160;set<br/>
Examples<br/>
•&#160;distance&#160;to&#160;a&#160;convex&#160;set&#160;C:&#160;g(x)&#160;=&#160;infy∈C&#160;kx&#160;−&#160;yk<br/>
•&#160;optimal&#160;value&#160;of&#160;linear&#160;program&#160;as&#160;function&#160;of&#160;righthand&#160;side<br/>
g(x)&#160;=<br/>
inf<br/>
cT&#160;y<br/>
y:Ayx<br/>
follows&#160;by&#160;taking<br/>
f&#160;(x,&#160;y)&#160;=&#160;cT&#160;y,<br/>
dom&#160;f&#160;=&#160;{(x,&#160;y)&#160;|&#160;Ay&#160;&#160;x}<br/>
Convex&#160;sets&#160;and&#160;functions<br/>
25<br/>
<hr/>
<a name=28></a>Perspective<br/>
the&#160;perspective&#160;of&#160;a&#160;function&#160;f&#160;:&#160;Rn&#160;→&#160;R&#160;is&#160;the&#160;function&#160;g&#160;:&#160;Rn&#160;×&#160;R&#160;→&#160;R,<br/>
g(x,&#160;t)&#160;=&#160;tf&#160;(x/t)<br/>
g&#160;is&#160;convex&#160;if&#160;f&#160;is&#160;convex&#160;on&#160;dom&#160;g&#160;=&#160;{(x,&#160;t)&#160;|&#160;x/t&#160;∈&#160;dom&#160;f,&#160;t&#160;&gt;&#160;0}<br/>
Examples<br/>
•&#160;perspective&#160;of&#160;f(x)&#160;=&#160;xT&#160;x&#160;is&#160;quadratic-over-linear&#160;function<br/>
xT&#160;x<br/>
g(x,&#160;t)&#160;=<br/>
t<br/>
•&#160;perspective&#160;of&#160;negative&#160;logarithm&#160;f(x)&#160;=&#160;−&#160;log&#160;x&#160;is&#160;relative&#160;entropy<br/>
g(x,&#160;t)&#160;=&#160;t&#160;log&#160;t&#160;−&#160;t&#160;log&#160;x<br/>
Convex&#160;sets&#160;and&#160;functions<br/>
26<br/>
<hr/>
<a name=29></a>Convex&#160;optimization&#160;—&#160;MLSS&#160;2009<br/>
Convex&#160;optimization&#160;problems<br/>
•&#160;standard&#160;form<br/>
•&#160;linear,&#160;quadratic,&#160;geometric&#160;programming<br/>
•&#160;modeling&#160;languages<br/>
27<br/>
<hr/>
<a name=30></a>Convex&#160;optimization&#160;problem<br/>
minimize<br/>
f0(x)<br/>
subject&#160;to&#160;fi(x)&#160;≤&#160;0,<br/>
i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;m<br/>
Ax&#160;=&#160;b<br/>
f0,&#160;f1,&#160;.&#160;.&#160;.&#160;,&#160;fm&#160;are&#160;convex&#160;functions<br/>
•&#160;feasible&#160;set&#160;is&#160;convex<br/>
•&#160;locally&#160;optimal&#160;points&#160;are&#160;globally&#160;optimal<br/>
•&#160;tractable,&#160;both&#160;in&#160;theory&#160;and&#160;practice<br/>
Convex&#160;optimization&#160;problems<br/>
28<br/>
<hr/>
<a name=31></a>Linear&#160;program&#160;(LP)<br/>
minimize<br/>
cT&#160;x&#160;+&#160;d<br/>
subject&#160;to&#160;Gx&#160;&#160;h<br/>
Ax&#160;=&#160;b<br/>
•&#160;inequality&#160;is&#160;componentwise&#160;vector&#160;inequality<br/>
•&#160;convex&#160;problem&#160;with&#160;affine&#160;objective&#160;and&#160;constraint&#160;functions<br/>
•&#160;feasible&#160;set&#160;is&#160;a&#160;polyhedron<br/>
−c<br/>
P<br/>
x⋆<br/>
Convex&#160;optimization&#160;problems<br/>
29<br/>
<hr/>
<a name=32></a>Piecewise-linear&#160;minimization<br/>
minimize&#160;f&#160;(x)&#160;=&#160;max&#160;(aTi&#160;x&#160;+&#160;bi)<br/>
i=1,...,m<br/>
f&#160;(x)<br/>
aT&#160;x&#160;+&#160;b<br/>
i<br/>
i<br/>
x<br/>
Equivalent&#160;linear&#160;program<br/>
minimize<br/>
t<br/>
subject&#160;to&#160;aTi&#160;x&#160;+&#160;bi&#160;≤&#160;t,&#160;i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;m<br/>
an&#160;LP&#160;with&#160;variables&#160;x,&#160;t&#160;∈&#160;R<br/>
Convex&#160;optimization&#160;problems<br/>
30<br/>
<hr/>
<a name=33></a>Linear&#160;discrimination<br/>
separate&#160;two&#160;sets&#160;of&#160;points&#160;{x1,&#160;.&#160;.&#160;.&#160;,&#160;xN},&#160;{y1,&#160;.&#160;.&#160;.&#160;,&#160;yM}&#160;by&#160;a&#160;hyperplane<br/>
aT&#160;xi&#160;+&#160;b&#160;&gt;&#160;0,<br/>
i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;N<br/>
aT&#160;yi&#160;+&#160;b&#160;&lt;&#160;0,<br/>
i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;M<br/>
homogeneous&#160;in&#160;a,&#160;b,&#160;hence&#160;equivalent&#160;to&#160;the&#160;linear&#160;inequalities&#160;(in&#160;a,&#160;b)<br/>
aT&#160;xi&#160;+&#160;b&#160;≥&#160;1,<br/>
i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;N,<br/>
aT&#160;yi&#160;+&#160;b&#160;≤&#160;−1,<br/>
i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;M<br/>
Convex&#160;optimization&#160;problems<br/>
31<br/>
<hr/>
<a name=34></a>Approximate&#160;linear&#160;separation&#160;of&#160;non-separable&#160;sets<br/>
N<br/>
M<br/>
X<br/>
X<br/>
minimize<br/>
max{0,&#160;1&#160;−&#160;aT&#160;xi&#160;−&#160;b}&#160;+<br/>
max{0,&#160;1&#160;+&#160;aT&#160;yi&#160;+&#160;b}<br/>
i=1<br/>
i=1<br/>
•&#160;a&#160;piecewise-linear&#160;minimization&#160;problem&#160;in&#160;a,&#160;b;&#160;equivalent&#160;to&#160;an&#160;LP<br/>
•&#160;can&#160;be&#160;interpreted&#160;as&#160;a&#160;heuristic&#160;for&#160;minimizing&#160;#misclassified&#160;points<br/>
Convex&#160;optimization&#160;problems<br/>
32<br/>
<hr/>
<a name=35></a>ℓ1-Norm&#160;and&#160;ℓ∞-norm&#160;minimization<br/>
ℓ1-Norm&#160;approximation&#160;and&#160;equivalent&#160;LP&#160;(kyk1&#160;=&#160;Pk&#160;|yk|)<br/>
n<br/>
X<br/>
minimize&#160;kAx&#160;−&#160;bk1<br/>
minimize<br/>
yi<br/>
i=1<br/>
subject&#160;to&#160;−y&#160;&#160;Ax&#160;−&#160;b&#160;&#160;y<br/>
ℓ∞-Norm&#160;approximation&#160;(kyk∞&#160;=&#160;maxk&#160;|yk|)<br/>
minimize&#160;kAx&#160;−&#160;bk∞<br/>
minimize<br/>
y<br/>
subject&#160;to&#160;−y1&#160;&#160;Ax&#160;−&#160;b&#160;&#160;y1<br/>
(1&#160;is&#160;vector&#160;of&#160;ones)<br/>
Convex&#160;optimization&#160;problems<br/>
33<br/>
<hr/>
<a name=36></a>Quadratic&#160;program&#160;(QP)<br/>
minimize<br/>
(1/2)xT&#160;P&#160;x&#160;+&#160;qT&#160;x&#160;+&#160;r<br/>
subject&#160;to&#160;Gx&#160;&#160;h<br/>
Ax&#160;=&#160;b<br/>
•&#160;P&#160;∈&#160;Sn+,&#160;so&#160;objective&#160;is&#160;convex&#160;quadratic<br/>•&#160;minimize&#160;a&#160;convex&#160;quadratic&#160;function&#160;over&#160;a&#160;polyhedron<br/>
−∇f0(x⋆)<br/>
x⋆<br/>
P<br/>
Convex&#160;optimization&#160;problems<br/>
34<br/>
<hr/>
<a name=37></a>Linear&#160;program&#160;with&#160;random&#160;cost<br/>
minimize<br/>
cT&#160;x<br/>
subject&#160;to&#160;Gx&#160;&#160;h<br/>
•&#160;c&#160;is&#160;random&#160;vector&#160;with&#160;mean&#160;¯c&#160;and&#160;covariance&#160;Σ<br/>
•&#160;hence,&#160;cT&#160;x&#160;is&#160;random&#160;variable&#160;with&#160;mean&#160;¯cT&#160;x&#160;and&#160;variance&#160;xT&#160;Σx<br/>
Expected&#160;cost-variance&#160;trade-off<br/>
minimize<br/>
E&#160;cT&#160;x&#160;+&#160;γ&#160;var(cT&#160;x)&#160;=&#160;¯<br/>
cT&#160;x&#160;+&#160;γxT&#160;Σx<br/>
subject&#160;to&#160;Gx&#160;&#160;h<br/>
γ&#160;&gt;&#160;0&#160;is&#160;risk&#160;aversion&#160;parameter<br/>
Convex&#160;optimization&#160;problems<br/>
35<br/>
<hr/>
<a name=38></a>Robust&#160;linear&#160;discrimination<br/>
H1&#160;=&#160;{z&#160;|&#160;aT&#160;z&#160;+&#160;b&#160;=&#160;1}<br/>H2&#160;=&#160;{z&#160;|&#160;aT&#160;z&#160;+&#160;b&#160;=&#160;−1}<br/>
distance&#160;between&#160;hyperplanes&#160;is&#160;2/kak2<br/>
to&#160;separate&#160;two&#160;sets&#160;of&#160;points&#160;by&#160;maximum&#160;margin,<br/>
minimize<br/>
kak22&#160;=&#160;aT&#160;a<br/>
subject&#160;to&#160;aT&#160;xi&#160;+&#160;b&#160;≥&#160;1,<br/>
i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;N<br/>
aT&#160;yi&#160;+&#160;b&#160;≤&#160;−1,<br/>
i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;M<br/>
a&#160;quadratic&#160;program&#160;in&#160;a,&#160;b<br/>
Convex&#160;optimization&#160;problems<br/>
36<br/>
<hr/>
<a name=39></a>Support&#160;vector&#160;classifier<br/>
N<br/>
M<br/>
X<br/>
X<br/>
min.&#160;γkak22&#160;+<br/>
max{0,&#160;1&#160;−&#160;aT&#160;xi&#160;−&#160;b}&#160;+<br/>
max{0,&#160;1&#160;+&#160;aT&#160;yi&#160;+&#160;b}<br/>
i=1<br/>
i=1<br/>
γ&#160;=&#160;0<br/>
γ&#160;=&#160;10<br/>
equivalent&#160;to&#160;a&#160;QP<br/>
Convex&#160;optimization&#160;problems<br/>
37<br/>
<hr/>
<a name=40></a>Sparse&#160;signal&#160;reconstruction<br/>
2<br/>
1<br/>
•&#160;signal&#160;ˆx&#160;of&#160;length&#160;1000<br/>
0<br/>
•&#160;ten&#160;nonzero&#160;components<br/>
-1<br/>
-2<br/>
0<br/>
200<br/>
400<br/>
600<br/>
800<br/>
1000<br/>
reconstruct&#160;signal&#160;from&#160;m&#160;=&#160;100&#160;random&#160;noisy&#160;measurements<br/>
b&#160;=&#160;Aˆ<br/>
x&#160;+&#160;v<br/>
(Aij&#160;∼&#160;N&#160;(0,&#160;1)&#160;i.i.d.&#160;and&#160;v&#160;∼&#160;N&#160;(0,&#160;σ2I)&#160;with&#160;σ&#160;=&#160;0.01)<br/>
Convex&#160;optimization&#160;problems<br/>
38<br/>
<hr/>
<a name=41></a>ℓ2-Norm&#160;regularization<br/>
minimize&#160;kAx&#160;−&#160;bk22&#160;+&#160;γkxk22<br/>
a&#160;least-squares&#160;problem<br/>
2<br/>
2<br/>
1<br/>
1<br/>
0<br/>
0<br/>
-1<br/>
-1<br/>
-2<br/>
-2<br/>
0<br/>
200<br/>
400<br/>
600<br/>
800<br/>
1000<br/>
0<br/>
200<br/>
400<br/>
600<br/>
800<br/>
1000<br/>
left:&#160;exact&#160;signal&#160;ˆ<br/>
x;&#160;right:&#160;2-norm&#160;reconstruction<br/>
Convex&#160;optimization&#160;problems<br/>
39<br/>
<hr/>
<a name=42></a>ℓ1-Norm&#160;regularization<br/>
minimize&#160;kAx&#160;−&#160;bk22&#160;+&#160;γkxk1<br/>
equivalent&#160;to&#160;a&#160;QP<br/>
2<br/>
2<br/>
1<br/>
1<br/>
0<br/>
0<br/>
-1<br/>
-1<br/>
-2<br/>
-2<br/>
0<br/>
200<br/>
400<br/>
600<br/>
800<br/>
1000<br/>
0<br/>
200<br/>
400<br/>
600<br/>
800<br/>
1000<br/>
left:&#160;exact&#160;signal&#160;ˆ<br/>
x;&#160;right:&#160;1-norm&#160;reconstruction<br/>
Convex&#160;optimization&#160;problems<br/>
40<br/>
<hr/>
<a name=43></a>Geometric&#160;programming<br/>
Posynomial&#160;function:<br/>
K<br/>
X<br/>
f&#160;(x)&#160;=<br/>
ckxa1k<br/>
1<br/>
xa2k<br/>
2<br/>
·&#160;·&#160;·&#160;xank<br/>
n<br/>
,<br/>
dom&#160;f&#160;=&#160;Rn<br/>
++<br/>
k=1<br/>
with&#160;ck&#160;&gt;&#160;0<br/>
Geometric&#160;program&#160;(GP)<br/>
minimize<br/>
f0(x)<br/>
subject&#160;to&#160;fi(x)&#160;≤&#160;1,<br/>
i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;m<br/>
with&#160;fi&#160;posynomial<br/>
Convex&#160;optimization&#160;problems<br/>
41<br/>
<hr/>
<a name=44></a>Geometric&#160;program&#160;in&#160;convex&#160;form<br/>
change&#160;variables&#160;to<br/>
yi&#160;=&#160;log&#160;xi,<br/>
and&#160;take&#160;logarithm&#160;of&#160;cost,&#160;constraints<br/>
Geometric&#160;program&#160;in&#160;convex&#160;form:<br/>
&#160;&#160;K<br/>
!<br/>
X<br/>
minimize<br/>
log<br/>
exp(aT0ky&#160;+&#160;b0k)<br/>
k=1<br/>
&#160;&#160;K<br/>
!<br/>
X<br/>
subject&#160;to&#160;log<br/>
exp(aTiky&#160;+&#160;bik)&#160;≤&#160;0,&#160;i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;m<br/>
k=1<br/>
bik&#160;=&#160;log&#160;cik<br/>
Convex&#160;optimization&#160;problems<br/>
42<br/>
<hr/>
<a name=45></a>Modeling&#160;software<br/>
Modeling&#160;packages&#160;for&#160;convex&#160;optimization<br/>
•&#160;CVX,&#160;Yalmip&#160;(Matlab)<br/>
•&#160;CVXMOD&#160;(Python)<br/>
assist&#160;in&#160;formulating&#160;convex&#160;problems&#160;by&#160;automating&#160;two&#160;tasks:<br/>
•&#160;verifying&#160;convexity&#160;from&#160;convex&#160;calculus&#160;rules<br/>
•&#160;transforming&#160;problem&#160;in&#160;input&#160;format&#160;required&#160;by&#160;standard&#160;solvers<br/>
Related&#160;packages<br/>
general&#160;purpose&#160;optimization&#160;modeling:&#160;AMPL,&#160;GAMS<br/>
Convex&#160;optimization&#160;problems<br/>
43<br/>
<hr/>
<a name=46></a>CVX&#160;example<br/>
minimize<br/>
kAx&#160;−&#160;bk1<br/>
subject&#160;to&#160;−0.5&#160;≤&#160;xk&#160;≤&#160;0.3,<br/>
k&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;n<br/>
Matlab&#160;code<br/>
A&#160;=&#160;randn(5,&#160;3);<br/>
b&#160;=&#160;randn(5,&#160;1);<br/>
cvx_begin<br/>
variable&#160;x(3);<br/>minimize(norm(A*x&#160;-&#160;b,&#160;1))<br/>subject&#160;to<br/>
-0.5&#160;&lt;=&#160;x;<br/>
x&#160;&lt;=&#160;0.3;<br/>
cvx_end<br/>
•&#160;between&#160;cvx_begin&#160;and&#160;cvx_end,&#160;x&#160;is&#160;a&#160;CVX&#160;variable<br/>
•&#160;after&#160;execution,&#160;x&#160;is&#160;Matlab&#160;variable&#160;with&#160;optimal&#160;solution<br/>
Convex&#160;optimization&#160;problems<br/>
44<br/>
<hr/>
<a name=47></a>Convex&#160;optimization&#160;—&#160;MLSS&#160;2009<br/>
Cone&#160;programming<br/>
•&#160;generalized&#160;inequalities<br/>
•&#160;second-order&#160;cone&#160;programming<br/>
•&#160;semidefinite&#160;programming<br/>
45<br/>
<hr/>
<a name=48></a>Cone&#160;linear&#160;program<br/>
minimize<br/>
cT&#160;x<br/>
subject&#160;to&#160;Gx&#160;K&#160;h<br/>
Ax&#160;=&#160;b<br/>
•&#160;y&#160;K&#160;z&#160;means&#160;z&#160;−&#160;y&#160;∈&#160;K,&#160;where&#160;K&#160;is&#160;a&#160;proper&#160;convex&#160;cone<br/>
•&#160;extends&#160;linear&#160;programming&#160;(K&#160;=&#160;Rm<br/>
+&#160;)&#160;to&#160;nonpolyhedral&#160;cones<br/>
•&#160;popular&#160;as&#160;standard&#160;format&#160;for&#160;nonlinear&#160;convex&#160;optimization<br/>
•&#160;theory&#160;and&#160;algorithms&#160;very&#160;similar&#160;to&#160;linear&#160;programming<br/>
Cone&#160;programming<br/>
46<br/>
<hr/>
<a name=49></a>Second-order&#160;cone&#160;program&#160;(SOCP)<br/>
minimize<br/>
f&#160;T&#160;x<br/>
subject&#160;to&#160;kAix&#160;+&#160;bik2&#160;≤&#160;cTi&#160;x&#160;+&#160;di,&#160;i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;m<br/>
•&#160;k&#160;·&#160;k2&#160;is&#160;Euclidean&#160;norm&#160;kyk2&#160;=&#160;py21&#160;+&#160;·&#160;·&#160;·&#160;+&#160;y2n<br/>
•&#160;constraints&#160;are&#160;nonlinear,&#160;nondifferentiable,&#160;convex<br/>
1<br/>
constraints&#160;are&#160;inequalities<br/>w.r.t.&#160;second-order&#160;cone:<br/>
3&#160;0.5<br/>
y<br/>
n<br/>
q<br/>
o<br/>
y&#160;<br/>
y2<br/>
<br/>
1&#160;+&#160;·&#160;·&#160;·&#160;+&#160;y2<br/>
p−1&#160;≤&#160;yp<br/>
0<br/>
1<br/>
1<br/>
0<br/>
0<br/>
y2<br/>
−1&#160;−1<br/>
y1<br/>
Cone&#160;programming<br/>
47<br/>
<hr/>
<a name=50></a>Examples&#160;of&#160;SOC-representable&#160;constraints<br/>
Convex&#160;quadratic&#160;constraint&#160;(A&#160;=&#160;LLT&#160;positive&#160;definite)<br/>
xT&#160;Ax&#160;+&#160;2bT&#160;x&#160;+&#160;c&#160;≤&#160;0<br/>
⇐⇒<br/>
<br/>LT&#160;x&#160;+&#160;L−1b<br/>
<br/>
≤&#160;(bT&#160;A−1b&#160;−&#160;c)1/2<br/>
2<br/>
also&#160;extends&#160;to&#160;positive&#160;semidefinite&#160;singular&#160;A<br/>
Hyperbolic&#160;constraint<br/>
<br/>
2x<br/>
<br/>
xT&#160;x&#160;≤&#160;yz,<br/>
y,&#160;z&#160;≥&#160;0<br/>
⇐⇒<br/>
<br/>
<br/>
<br/>
<br/>
≤&#160;y&#160;+&#160;z,&#160;y,&#160;z&#160;≥&#160;0<br/>
<br/>
y&#160;−&#160;z&#160;2<br/>
Cone&#160;programming<br/>
48<br/>
<hr/>
<a name=51></a>Examples&#160;of&#160;SOC-representable&#160;constraints<br/>
Positive&#160;powers<br/>
x1.5&#160;≤&#160;t,<br/>
x&#160;≥&#160;0<br/>
⇐⇒<br/>
∃z&#160;:<br/>
x2&#160;≤&#160;tz,<br/>
z2&#160;≤&#160;x,<br/>
x,&#160;z&#160;≥&#160;0<br/>
•&#160;two&#160;hyperbolic&#160;constraints&#160;can&#160;be&#160;converted&#160;to&#160;SOC&#160;constraints<br/>
•&#160;extends&#160;to&#160;powers&#160;xp&#160;for&#160;rational&#160;p&#160;≥&#160;1<br/>
Negative&#160;powers<br/>
x−3&#160;≤&#160;t,<br/>
x&#160;&gt;&#160;0<br/>
⇐⇒<br/>
∃z&#160;:<br/>
1&#160;≤&#160;tz,<br/>
z2&#160;≤&#160;tx,<br/>
x,&#160;z&#160;≥&#160;0<br/>
•&#160;two&#160;hyperbolic&#160;constraints&#160;can&#160;be&#160;converted&#160;to&#160;SOC&#160;constraints<br/>
•&#160;extends&#160;to&#160;powers&#160;xp&#160;for&#160;rational&#160;p&#160;&lt;&#160;0<br/>
Cone&#160;programming<br/>
49<br/>
<hr/>
<a name=52></a>Robust&#160;linear&#160;program&#160;(stochastic)<br/>
minimize<br/>
cT&#160;x<br/>
subject&#160;to&#160;prob(aTi&#160;x&#160;≤&#160;bi)&#160;≥&#160;η,&#160;i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;m<br/>
•&#160;ai&#160;random&#160;and&#160;normally&#160;distributed&#160;with&#160;mean&#160;¯ai,&#160;covariance&#160;Σi<br/>
•&#160;we&#160;require&#160;that&#160;x&#160;satisfies&#160;each&#160;constraint&#160;with&#160;probability&#160;exceeding&#160;η<br/>
η&#160;=&#160;10%<br/>
η&#160;=&#160;50%<br/>
η&#160;=&#160;90%<br/>
Cone&#160;programming<br/>
50<br/>
<hr/>
<a name=53></a>SOCP&#160;formulation<br/>
the&#160;‘chance&#160;constraint’&#160;prob(aTi&#160;x&#160;≤&#160;bi)&#160;≥&#160;η&#160;is&#160;equivalent&#160;to&#160;the&#160;constraint<br/>
¯<br/>
aTi&#160;x&#160;+&#160;Φ−1(η)kΣ1/2x<br/>
i<br/>
k2&#160;≤&#160;bi<br/>
Φ&#160;is&#160;the&#160;(unit)&#160;normal&#160;cumulative&#160;density&#160;function<br/>
1<br/>
η<br/>
(t)&#160;0.5<br/>
Φ<br/>
Φ−1(η)<br/>
0<br/>
0<br/>t<br/>
robust&#160;LP&#160;is&#160;a&#160;second-order&#160;cone&#160;program&#160;for&#160;η&#160;≥&#160;0.5<br/>
Cone&#160;programming<br/>
51<br/>
<hr/>
<a name=54></a>Robust&#160;linear&#160;program&#160;(deterministic)<br/>
minimize<br/>
cT&#160;x<br/>
subject&#160;to&#160;aTi&#160;x&#160;≤&#160;bi&#160;for&#160;all&#160;ai&#160;∈&#160;Ei,&#160;i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;m<br/>
•&#160;ai&#160;uncertain&#160;but&#160;bounded&#160;by&#160;ellipsoid&#160;Ei&#160;=&#160;{¯ai&#160;+&#160;Piu&#160;|&#160;kuk2&#160;≤&#160;1}<br/>
•&#160;we&#160;require&#160;that&#160;x&#160;satisfies&#160;each&#160;constraint&#160;for&#160;all&#160;possible&#160;ai<br/>
SOCP&#160;formulation<br/>
minimize<br/>
cT&#160;x<br/>
subject&#160;to&#160;¯<br/>
aTi&#160;x&#160;+&#160;kP&#160;T<br/>
i&#160;xk2&#160;≤&#160;bi,<br/>
i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;m<br/>
follows&#160;from<br/>
sup&#160;(¯<br/>
ai&#160;+&#160;Piu)T&#160;x&#160;=&#160;¯aTi&#160;+&#160;kP&#160;T<br/>
i&#160;xk2<br/>
kuk2≤1<br/>
Cone&#160;programming<br/>
52<br/>
<hr/>
<a name=55></a>Semidefinite&#160;program&#160;(SDP)<br/>
minimize<br/>
cT&#160;x<br/>
subject&#160;to&#160;x1A1&#160;+&#160;x2A2&#160;+&#160;·&#160;·&#160;·&#160;+&#160;xnAn&#160;&#160;B<br/>
•&#160;A1,&#160;A2,&#160;.&#160;.&#160;.&#160;,&#160;An,&#160;B&#160;are&#160;symmetric&#160;matrices<br/>
•&#160;inequality&#160;X&#160;&#160;Y&#160;means&#160;Y&#160;−&#160;X&#160;is&#160;positive&#160;semidefinite,&#160;i.e.,<br/>
X<br/>
zT&#160;(Y&#160;−&#160;X)z&#160;=<br/>
(Yij&#160;−&#160;Xij)zizj&#160;≥&#160;0&#160;for&#160;all&#160;z<br/>
i,j<br/>
•&#160;includes&#160;many&#160;nonlinear&#160;constraints&#160;as&#160;special&#160;cases<br/>
Cone&#160;programming<br/>
53<br/>
<hr/>
<a name=56></a>Geometry<br/>
1<br/>
z&#160;0.5<br/>
<br/>
x&#160;y&#160;&#160;&#160;0<br/>
y<br/>
z<br/>
0<br/>
1<br/>
1<br/>
0<br/>
0.5<br/>
y<br/>
−1&#160;0<br/>
x<br/>
•&#160;a&#160;nonpolyhedral&#160;convex&#160;cone<br/>
•&#160;feasible&#160;set&#160;of&#160;a&#160;semidefinite&#160;program&#160;is&#160;the&#160;intersection&#160;of&#160;the&#160;positive<br/>
semidefinite&#160;cone&#160;in&#160;high&#160;dimension&#160;with&#160;planes<br/>
Cone&#160;programming<br/>
54<br/>
<hr/>
<a name=57></a>Examples<br/>
A(x)&#160;=&#160;A0&#160;+&#160;x1A1&#160;+&#160;·&#160;·&#160;·&#160;+&#160;xmAm<br/>
(Ai&#160;∈&#160;Sn)<br/>
Eigenvalue&#160;minimization&#160;(and&#160;equivalent&#160;SDP)<br/>
minimize&#160;λmax(A(x))<br/>
minimize<br/>
t<br/>
subject&#160;to&#160;A(x)&#160;&#160;tI<br/>
Matrix-fractional&#160;function<br/>
minimize<br/>
bT&#160;A(x)−1b<br/>
minimize<br/>
t<br/>
subject&#160;to&#160;A(x)&#160;&#160;0<br/>
<br/>
A(x)&#160;b&#160;<br/>
subject&#160;to<br/>
&#160;0<br/>
bT<br/>
t<br/>
Cone&#160;programming<br/>
55<br/>
<hr/>
<a name=58></a>Matrix&#160;norm&#160;minimization<br/>
A(x)&#160;=&#160;A0&#160;+&#160;x1A1&#160;+&#160;x2A2&#160;+&#160;·&#160;·&#160;·&#160;+&#160;xnAn<br/>
(Ai&#160;∈&#160;Rp×q)<br/>
Matrix&#160;norm&#160;approximation&#160;(kXk2&#160;=&#160;maxk&#160;σk(X))<br/>
minimize&#160;kA(x)k2<br/>
minimize<br/>
t<br/><br/>
tI<br/>
A(x)T&#160;<br/>
subject&#160;to<br/>
&#160;0<br/>
A(x)<br/>
tI<br/>
Nuclear&#160;norm&#160;approximation&#160;(kXk∗&#160;=&#160;P&#160;σ<br/>
k<br/>
k(X&#160;))<br/>
minimize&#160;kA(x)k∗<br/>
minimize<br/>
(tr&#160;U&#160;+&#160;tr&#160;V&#160;)/2<br/>
<br/>
U<br/>
A(x)T&#160;<br/>
subject&#160;to<br/>
&#160;0<br/>
A(x)<br/>
V<br/>
Cone&#160;programming<br/>
56<br/>
<hr/>
<a name=59></a>Semidefinite&#160;relaxations&#160;&amp;&#160;randomization<br/>
semidefinite&#160;programming&#160;is&#160;increasingly&#160;used<br/>
•&#160;to&#160;find&#160;good&#160;bounds&#160;for&#160;hard&#160;(i.e.,&#160;nonconvex)&#160;problems,&#160;via&#160;relaxation<br/>
•&#160;as&#160;a&#160;heuristic&#160;for&#160;good&#160;suboptimal&#160;points,&#160;often&#160;via&#160;randomization<br/>
Example:&#160;Boolean&#160;least-squares<br/>
minimize<br/>
kAx&#160;−&#160;bk22<br/>
subject&#160;to&#160;x2i&#160;=&#160;1,&#160;i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;n<br/>
•&#160;basic&#160;problem&#160;in&#160;digital&#160;communications<br/>
•&#160;could&#160;check&#160;all&#160;2n&#160;possible&#160;values&#160;of&#160;x&#160;∈&#160;{−1,&#160;1}n&#160;.&#160;.&#160;.<br/>
•&#160;an&#160;NP-hard&#160;problem,&#160;and&#160;very&#160;hard&#160;in&#160;practice<br/>
Cone&#160;programming<br/>
57<br/>
<hr/>
<a name=60></a>Semidefinite&#160;lifting<br/>
with&#160;P&#160;=&#160;AT&#160;A,&#160;q&#160;=&#160;−AT&#160;b,&#160;r&#160;=&#160;bT&#160;b<br/>
n<br/>
n<br/>
k<br/>
X<br/>
X<br/>
Ax&#160;−&#160;bk22&#160;=<br/>
Pijxixj&#160;+&#160;2<br/>
qixi&#160;+&#160;r<br/>
i,j=1<br/>
i=1<br/>
after&#160;introducing&#160;new&#160;variables&#160;Xij&#160;=&#160;xixj<br/>
n<br/>
n<br/>
X<br/>
X<br/>
minimize<br/>
PijXij&#160;+&#160;2<br/>
qixi&#160;+&#160;r<br/>
i,j=1<br/>
i=1<br/>
subject&#160;to&#160;Xii&#160;=&#160;1,<br/>
i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;n<br/>
Xij&#160;=&#160;xixj,<br/>
i,&#160;j&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;n<br/>
•&#160;cost&#160;function&#160;and&#160;first&#160;constraints&#160;are&#160;linear<br/>•&#160;last&#160;constraint&#160;in&#160;matrix&#160;form&#160;is&#160;X&#160;=&#160;xxT&#160;,&#160;nonlinear&#160;and&#160;nonconvex,<br/>
.&#160;.&#160;.&#160;still&#160;a&#160;very&#160;hard&#160;problem<br/>
Cone&#160;programming<br/>
58<br/>
<hr/>
<a name=61></a>Semidefinite&#160;relaxation<br/>
replace&#160;X&#160;=&#160;xxT&#160;with&#160;weaker&#160;constraint&#160;X&#160;&#160;xxT&#160;,&#160;to&#160;obtain&#160;relaxation<br/>
n<br/>
n<br/>
X<br/>
X<br/>
minimize<br/>
PijXij&#160;+&#160;2<br/>
qixi&#160;+&#160;r<br/>
i,j=1<br/>
i=1<br/>
subject&#160;to&#160;Xii&#160;=&#160;1,<br/>
i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;n<br/>
X&#160;&#160;xxT<br/>
•&#160;convex;&#160;can&#160;be&#160;solved&#160;as&#160;an&#160;semidefinite&#160;program<br/>
•&#160;optimal&#160;value&#160;gives&#160;lower&#160;bound&#160;for&#160;BLS<br/>
•&#160;if&#160;X&#160;=&#160;xxT&#160;at&#160;the&#160;optimum,&#160;we&#160;have&#160;solved&#160;the&#160;exact&#160;problem<br/>
•&#160;otherwise,&#160;can&#160;use&#160;randomized&#160;rounding<br/>
generate&#160;z&#160;from&#160;N&#160;(x,&#160;X&#160;−&#160;xxT&#160;)&#160;and&#160;take&#160;x&#160;=&#160;sign(z)<br/>
Cone&#160;programming<br/>
59<br/>
<hr/>
<a name=62></a>Example<br/>
0.5<br/>
0.4<br/>
SDP&#160;bound<br/>
LS&#160;solution<br/>
0.3<br/>
cy<br/>en<br/>u<br/>
0.2<br/>
freq<br/>
0.1<br/>
0<br/>
1<br/>
1.2<br/>
kAx&#160;−&#160;bk2/(SDP&#160;bound)<br/>
•&#160;feasible&#160;set&#160;has&#160;2100&#160;≈&#160;1030&#160;points<br/>
•&#160;histogram&#160;of&#160;1000&#160;randomized&#160;solutions&#160;from&#160;SDP&#160;relaxation<br/>
Cone&#160;programming<br/>
60<br/>
<hr/>
<a name=63></a>Nonnegative&#160;polynomial&#160;on&#160;R<br/>
f&#160;(t)&#160;=&#160;x0&#160;+&#160;x1t&#160;+&#160;·&#160;·&#160;·&#160;+&#160;x2mt2m&#160;≥&#160;0&#160;for&#160;all&#160;t&#160;∈&#160;R<br/>
•&#160;a&#160;convex&#160;constraint&#160;on&#160;x<br/>•&#160;holds&#160;if&#160;and&#160;only&#160;if&#160;f&#160;is&#160;a&#160;sum&#160;of&#160;squares&#160;of&#160;(two)&#160;polynomials:<br/>
X<br/>
f&#160;(t)&#160;=<br/>
(yk0&#160;+&#160;yk1t&#160;+&#160;·&#160;·&#160;·&#160;+&#160;ykmtm)2<br/>
k<br/>
<br/>
T<br/>
1&#160;<br/>
<br/>
1&#160;<br/>
X<br/>
=<br/>
..<br/>
y<br/>
..<br/>
<br/>
<br/>
kyT<br/>
k&#160;<br/>
<br/>
tm<br/>
k<br/>
tm<br/>
<br/>
T<br/>
1&#160;<br/>
<br/>
1&#160;<br/>
=<br/>
..<br/>
Y<br/>
..<br/>
<br/>
<br/>
<br/>
<br/>
tm<br/>
tm<br/>
where&#160;Y&#160;=&#160;P&#160;y<br/>
k&#160;kyT<br/>
k&#160;&#160;0<br/>
Cone&#160;programming<br/>
61<br/>
<hr/>
<a name=64></a>SDP&#160;formulation<br/>
f&#160;(t)&#160;≥&#160;0&#160;if&#160;and&#160;only&#160;if&#160;for&#160;some&#160;Y&#160;&#160;0,<br/>
<br/>
T<br/>
T<br/>
1&#160;&#160;&#160;x<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
0<br/>
1<br/>
1<br/>
t<br/>
x<br/>
t<br/>
t<br/>
f&#160;(t)&#160;=&#160;<br/>
<br/>
<br/>
1<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
.<br/>
=<br/>
Y<br/>
.<br/>
<br/>
<br/>
..&#160;&#160;&#160;..&#160;<br/>
<br/>
..&#160;<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
t2m<br/>
x2m<br/>
tm<br/>
tm<br/>
this&#160;is&#160;an&#160;SDP&#160;constraint:&#160;there&#160;exists&#160;Y&#160;&#160;0&#160;such&#160;that<br/>
x0&#160;=&#160;Y11<br/>
x1&#160;=&#160;Y12&#160;+&#160;Y21<br/>
x2&#160;=&#160;Y13&#160;+&#160;Y22&#160;+&#160;Y32<br/>
..<br/>
x2m&#160;=&#160;Ym+1,m+1<br/>
Cone&#160;programming<br/>
62<br/>
<hr/>
<a name=65></a>General&#160;sum-of-squares&#160;constraints<br/>
f&#160;(t)&#160;=&#160;xT&#160;p(t)&#160;is&#160;a&#160;sum&#160;of&#160;squares&#160;if<br/>
s<br/>
&#160;<br/>
s<br/>
!<br/>
X<br/>
X<br/>
xT&#160;p(t)&#160;=<br/>
(yT<br/>
k&#160;q(t))2&#160;=&#160;q(t)T<br/>
ykyTk&#160;q(t)<br/>
k=1<br/>
k=1<br/>
•&#160;p,&#160;q:&#160;basis&#160;functions&#160;(of&#160;polynomials,&#160;trigonometric&#160;polynomials,&#160;.&#160;.&#160;.&#160;)<br/>
•&#160;independent&#160;variable&#160;t&#160;can&#160;be&#160;one-&#160;or&#160;multidimensional<br/>
•&#160;a&#160;sufficient&#160;condition&#160;for&#160;nonnegativity&#160;of&#160;xT&#160;p(t),&#160;useful&#160;in&#160;nonconvex<br/>
polynomial&#160;optimization&#160;in&#160;several&#160;variables<br/>
•&#160;in&#160;some&#160;nontrivial&#160;cases&#160;(e.g.,&#160;polynomial&#160;on&#160;R),&#160;necessary&#160;and&#160;sufficient<br/>
Equivalent&#160;SDP&#160;constraint&#160;(on&#160;the&#160;variables&#160;x,&#160;X)<br/>
xT&#160;p(t)&#160;=&#160;q(t)T&#160;Xq(t),<br/>
X&#160;&#160;0<br/>
Cone&#160;programming<br/>
63<br/>
<hr/>
<a name=66></a>Example:&#160;Cosine&#160;polynomials<br/>
f&#160;(ω)&#160;=&#160;x0&#160;+&#160;x1&#160;cos&#160;ω&#160;+&#160;·&#160;·&#160;·&#160;+&#160;x2n&#160;cos&#160;2nω&#160;≥&#160;0<br/>
Sum&#160;of&#160;squares&#160;theorem:&#160;f&#160;(ω)&#160;≥&#160;0&#160;for&#160;α&#160;≤&#160;ω&#160;≤&#160;β&#160;if&#160;and&#160;only&#160;if<br/>
f&#160;(ω)&#160;=&#160;g1(ω)2&#160;+&#160;s(ω)g2(ω)2<br/>
•&#160;g1,&#160;g2:&#160;cosine&#160;polynomials&#160;of&#160;degree&#160;n&#160;and&#160;n&#160;−&#160;1<br/>
•&#160;s(ω)&#160;=&#160;(cos&#160;ω&#160;−&#160;cos&#160;β)(cos&#160;α&#160;−&#160;cos&#160;ω)&#160;is&#160;a&#160;given&#160;weight&#160;function<br/>
Equivalent&#160;SDP&#160;formulation:&#160;f&#160;(ω)&#160;≥&#160;0&#160;for&#160;α&#160;≤&#160;ω&#160;≤&#160;β&#160;if&#160;and&#160;only&#160;if<br/>
xT&#160;p(ω)&#160;=&#160;q1(ω)T&#160;X1q1(ω)&#160;+&#160;s(ω)q2(ω)T&#160;X2q2(ω),<br/>
X1&#160;&#160;0,<br/>
X2&#160;&#160;0<br/>
p,&#160;q1,&#160;q2:&#160;basis&#160;vectors&#160;(1,&#160;cos&#160;ω,&#160;cos(2ω),&#160;.&#160;.&#160;.)&#160;up&#160;to&#160;order&#160;2n,&#160;n,&#160;n&#160;−&#160;1<br/>
Cone&#160;programming<br/>
64<br/>
<hr/>
<a name=67></a>Example:&#160;Linear-phase&#160;Nyquist&#160;filter<br/>
minimize&#160;supω≥ω&#160;|h<br/>
s<br/>
0&#160;+&#160;h1&#160;cos&#160;ω&#160;+&#160;·&#160;·&#160;·&#160;+&#160;h2n&#160;cos&#160;2nω|<br/>
with&#160;h0&#160;=&#160;1/M&#160;,&#160;hkM&#160;=&#160;0&#160;for&#160;positive&#160;integer&#160;k<br/>
0<br/>
10<br/>
−1<br/>
10<br/>
)|<br/>(ω<br/>|H<br/>
−2<br/>
10<br/>
−3<br/>
10&#160;0<br/>
0.5<br/>
1<br/>
1.5<br/>
2<br/>
2.5<br/>
3<br/>
ω<br/>
(Example&#160;with&#160;n&#160;=&#160;25,&#160;M&#160;=&#160;5,&#160;ωs&#160;=&#160;0.69)<br/>
Cone&#160;programming<br/>
65<br/>
<hr/>
<a name=68></a>SDP&#160;formulation<br/>
minimize<br/>
t<br/>
subject&#160;to&#160;−t&#160;≤&#160;H(ω)&#160;≤&#160;t,<br/>
ωs&#160;≤&#160;ω&#160;≤&#160;π<br/>
where&#160;H(ω)&#160;=&#160;h0&#160;+&#160;h1&#160;cos&#160;ω&#160;+&#160;·&#160;·&#160;·&#160;+&#160;h2n&#160;cos&#160;2nω<br/>
Equivalent&#160;SDP<br/>
minimize<br/>
t<br/>
subject&#160;to&#160;t&#160;−&#160;H(ω)&#160;=&#160;q1(ω)T&#160;X1q1(ω)&#160;+&#160;s(ω)q2(ω)T&#160;X2q2(ω)<br/>
t&#160;+&#160;H(ω)&#160;=&#160;q1(ω)T&#160;X3q1(ω)&#160;+&#160;s(ω)q2(ω)T&#160;X3q2(ω)<br/>
X1&#160;&#160;0,<br/>
X2&#160;&#160;0,<br/>
X3&#160;&#160;0,<br/>
X4&#160;&#160;0<br/>
Variables&#160;t,&#160;hi&#160;(i&#160;6=&#160;kM),&#160;4&#160;matrices&#160;Xi&#160;of&#160;size&#160;roughly&#160;n<br/>
Cone&#160;programming<br/>
66<br/>
<hr/>
<a name=69></a>Chebyshev&#160;inequalities<br/>
Classical&#160;(two-sided)&#160;Chebyshev&#160;inequality<br/>
prob(|X|&#160;&lt;&#160;1)&#160;≥&#160;1&#160;−&#160;σ2<br/>
•&#160;holds&#160;for&#160;all&#160;random&#160;X&#160;with&#160;E&#160;X&#160;=&#160;0,&#160;E&#160;X2&#160;=&#160;σ2<br/>
•&#160;there&#160;exists&#160;a&#160;distribution&#160;that&#160;achieves&#160;the&#160;bound<br/>
Generalized&#160;Chebyshev&#160;inequalities<br/>
give&#160;lower&#160;bound&#160;on&#160;prob(X&#160;∈&#160;C),&#160;given&#160;moments&#160;of&#160;X<br/>
Cone&#160;programming<br/>
67<br/>
<hr/>
<a name=70></a>Chebyshev&#160;inequality&#160;for&#160;quadratic&#160;constraints<br/>
•&#160;C&#160;is&#160;defined&#160;by&#160;quadratic&#160;inequalities<br/>
C&#160;=&#160;{x&#160;∈&#160;Rn&#160;|&#160;xT&#160;Aix&#160;+&#160;2bTi&#160;x&#160;+&#160;ci&#160;≤&#160;0,&#160;i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;m}<br/>
•&#160;X&#160;is&#160;random&#160;vector&#160;with&#160;E&#160;X&#160;=&#160;a,&#160;E&#160;XXT&#160;=&#160;S<br/>
SDP&#160;formulation&#160;(variables&#160;P&#160;∈&#160;Sn,&#160;q&#160;∈&#160;Rn,&#160;r,&#160;τ1,&#160;.&#160;.&#160;.&#160;,&#160;τm&#160;∈&#160;R)<br/>
maximize<br/>
1&#160;−&#160;tr(SP&#160;)&#160;−&#160;2aT&#160;q&#160;−&#160;r<br/><br/>
P<br/>
q<br/>
<br/>
<br/>
A<br/>
<br/>
subject&#160;to<br/>
&#160;τ<br/>
i<br/>
bi<br/>
,<br/>
τ<br/>
qT<br/>
r&#160;−&#160;1<br/>
i<br/>
bT<br/>
i&#160;≥&#160;0<br/>
i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;m<br/>
i<br/>
ci<br/>
<br/>
P<br/>
q&#160;&#160;&#160;0<br/>
qT<br/>
r<br/>
optimal&#160;value&#160;is&#160;tight&#160;lower&#160;bound&#160;on&#160;prob(X&#160;∈&#160;S)<br/>
Cone&#160;programming<br/>
68<br/>
<hr/>
<a name=71></a>Example<br/>
C<br/>
a<br/>
•&#160;a&#160;=&#160;E&#160;X;&#160;dashed&#160;line&#160;shows&#160;{x&#160;|&#160;(x&#160;−&#160;a)T&#160;(S&#160;−&#160;aaT&#160;)−1(x&#160;−&#160;a)&#160;=&#160;1}<br/>
•&#160;lower&#160;bound&#160;on&#160;prob(X&#160;∈&#160;C)&#160;is&#160;achieved&#160;by&#160;distribution&#160;shown&#160;in&#160;red<br/>
•&#160;ellipse&#160;is&#160;defined&#160;by&#160;xT&#160;P&#160;x&#160;+&#160;2qT&#160;x&#160;+&#160;r&#160;=&#160;1<br/>
Cone&#160;programming<br/>
69<br/>
<hr/>
<a name=72></a>Detection&#160;example<br/>
x&#160;=&#160;s&#160;+&#160;v<br/>
•&#160;x&#160;∈&#160;Rn:&#160;received&#160;signal<br/>
•&#160;s:&#160;transmitted&#160;signal&#160;s&#160;∈&#160;{s1,&#160;s2,&#160;.&#160;.&#160;.&#160;,&#160;sN}&#160;(one&#160;of&#160;N&#160;possible&#160;symbols)<br/>
•&#160;v:&#160;noise&#160;with&#160;E&#160;v&#160;=&#160;0,&#160;E&#160;vvT&#160;=&#160;σ2I<br/>
Detection&#160;problem:&#160;given&#160;observed&#160;value&#160;of&#160;x,&#160;estimate&#160;s<br/>
Cone&#160;programming<br/>
70<br/>
<hr/>
<a name=73></a>Example&#160;(N&#160;=&#160;7):&#160;bound&#160;on&#160;probability&#160;of&#160;correct&#160;detection&#160;of&#160;s1&#160;is&#160;0.205<br/>
s3<br/>
s2<br/>
s7<br/>
s4<br/>
s1<br/>
s<br/>
s<br/>
6<br/>
5<br/>
dots:&#160;distribution&#160;with&#160;probability&#160;of&#160;correct&#160;detection&#160;0.205<br/>
Cone&#160;programming<br/>
71<br/>
<hr/>
<a name=74></a>Cone&#160;programming&#160;duality<br/>
Primal&#160;and&#160;dual&#160;cone&#160;program<br/>
P:<br/>
minimize<br/>
cT&#160;x<br/>
D:<br/>
maximize<br/>
−bT&#160;z<br/>
subject&#160;to&#160;Ax&#160;K&#160;b<br/>
subject&#160;to&#160;AT&#160;z&#160;+&#160;c&#160;=&#160;0<br/>
z&#160;K∗&#160;0<br/>
•&#160;optimal&#160;values&#160;are&#160;equal&#160;(if&#160;primal&#160;or&#160;dual&#160;is&#160;strictly&#160;feasible)<br/>
•&#160;dual&#160;inequality&#160;is&#160;with&#160;respect&#160;to&#160;the&#160;dual&#160;cone<br/>
K∗&#160;=&#160;{z&#160;|&#160;xT&#160;z&#160;≥&#160;0&#160;for&#160;all&#160;x&#160;∈&#160;K}<br/>
•&#160;K&#160;=&#160;K∗&#160;for&#160;linear,&#160;second-order&#160;cone,&#160;semidefinite&#160;programming<br/>
Applications:&#160;optimality&#160;conditions,&#160;sensitivity&#160;analysis,&#160;algorithms,&#160;.&#160;.&#160;.<br/>
Cone&#160;programming<br/>
72<br/>
<hr/>
<a name=75></a>Convex&#160;optimization&#160;—&#160;MLSS&#160;2009<br/>
Interior-point&#160;methods<br/>
•&#160;Newton’s&#160;method<br/>
•&#160;barrier&#160;method<br/>
•&#160;primal-dual&#160;interior-point&#160;methods<br/>
•&#160;problem&#160;structure<br/>
73<br/>
<hr/>
<a name=76></a>Equality-constrained&#160;convex&#160;optimization<br/>
minimize<br/>
f&#160;(x)<br/>
subject&#160;to&#160;Ax&#160;=&#160;b<br/>
f&#160;twice&#160;continuously&#160;differentiable&#160;and&#160;convex<br/>
Optimality&#160;(Karush-Kuhn-Tucker&#160;or&#160;KKT)&#160;condition<br/>
∇f(x)&#160;+&#160;AT&#160;y&#160;=&#160;0,<br/>
Ax&#160;=&#160;b<br/>
Example:&#160;f&#160;(x)&#160;=&#160;(1/2)xT&#160;P&#160;x&#160;+&#160;qT&#160;x&#160;+&#160;r&#160;with&#160;P&#160;&#160;0<br/>
<br/>
P<br/>
AT&#160;&#160;&#160;x&#160;<br/>
<br/>
−q&#160;<br/>
=<br/>
A<br/>
0<br/>
y<br/>
b<br/>
a&#160;symmetric&#160;indefinite&#160;set&#160;of&#160;equations,&#160;known&#160;as&#160;a&#160;KKT&#160;system<br/>
Interior-point&#160;methods<br/>
74<br/>
<hr/>
<a name=77></a>Newton&#160;step<br/>
replace&#160;f&#160;with&#160;second-order&#160;approximation&#160;fq&#160;at&#160;feasible&#160;ˆ<br/>
x:<br/>
1<br/>
minimize<br/>
fq(x)&#160;∆<br/>
=&#160;f&#160;(ˆ<br/>
x)&#160;+&#160;∇f(ˆx)T&#160;(x&#160;−&#160;ˆx)&#160;+&#160;(x&#160;−&#160;ˆx)T&#160;∇2f(ˆx)(x&#160;−&#160;ˆx)<br/>
2<br/>
subject&#160;to&#160;Ax&#160;=&#160;b<br/>
solution&#160;is&#160;x&#160;=&#160;ˆ<br/>
x&#160;+&#160;∆xnt&#160;with&#160;∆xnt&#160;defined&#160;by<br/>
<br/>
∇2f(ˆx)&#160;AT&#160;&#160;&#160;∆x<br/>
<br/>
<br/>
<br/>
nt<br/>
−∇f(ˆx)<br/>
=<br/>
A<br/>
0<br/>
w<br/>
0<br/>
∆xnt&#160;is&#160;called&#160;the&#160;Newton&#160;step&#160;at&#160;ˆ<br/>
x<br/>
Interior-point&#160;methods<br/>
75<br/>
<hr/>
<a name=78></a>Interpretation&#160;(for&#160;unconstrained&#160;problem)<br/>
ˆ<br/>
x&#160;+&#160;∆xnt&#160;minimizes&#160;2nd-order<br/>
fq<br/>
approximation&#160;fq<br/>
(ˆ<br/>
x,&#160;f&#160;(ˆ<br/>
x))<br/>
f<br/>
(ˆ<br/>
x&#160;+&#160;∆xnt,&#160;f(ˆ<br/>
x&#160;+&#160;∆xnt))<br/>
ˆ<br/>
x&#160;+&#160;∆xnt&#160;solves&#160;linearized&#160;optimality<br/>condition<br/>
f&#160;′q<br/>
f&#160;′<br/>
∇fq(x)<br/>
(ˆ<br/>
x&#160;+&#160;∆xnt,&#160;f&#160;′(ˆ<br/>
x&#160;+&#160;∆xnt))<br/>
=&#160;∇f(ˆx)&#160;+&#160;∇2f(ˆx)(x&#160;−&#160;ˆx)<br/>
(ˆ<br/>
x,&#160;f&#160;′(ˆ<br/>
x))<br/>
=&#160;0<br/>
Interior-point&#160;methods<br/>
76<br/>
<hr/>
<a name=79></a>Newton’s&#160;algorithm<br/>
given&#160;starting&#160;point&#160;x(0)&#160;∈&#160;dom&#160;f&#160;with&#160;Ax(0)&#160;=&#160;b,&#160;tolerance&#160;ǫ<br/>repeat&#160;for&#160;k&#160;=&#160;0,&#160;1,&#160;.&#160;.&#160;.<br/>
1.&#160;compute&#160;Newton&#160;step&#160;∆xnt&#160;at&#160;x(k)&#160;by&#160;solving<br/>
<br/>
∇2f(x(k))&#160;AT&#160;&#160;&#160;∆x<br/>
<br/>
<br/>
<br/>
nt<br/>
−∇f(x(k))<br/>
=<br/>
A<br/>
0<br/>
w<br/>
0<br/>
2.&#160;terminate&#160;if&#160;−∇f(x(k))T&#160;∆xnt&#160;≤&#160;ǫ<br/>3.&#160;x(k+1)&#160;=&#160;x(k)&#160;+&#160;t∆xnt,&#160;with&#160;t&#160;determined&#160;by&#160;line&#160;search<br/>
Comments<br/>
•&#160;∇f(x(k))T&#160;∆xnt&#160;is&#160;directional&#160;derivative&#160;at&#160;x(k)&#160;in&#160;Newton&#160;direction<br/>
•&#160;line&#160;search&#160;needed&#160;to&#160;guarantee&#160;f(x(k+1))&#160;&lt;&#160;f(x(k)),&#160;global&#160;convergence<br/>
Interior-point&#160;methods<br/>
77<br/>
<hr/>
<a name=80></a>Example<br/>
n<br/>
m<br/>
X<br/>
X<br/>
f&#160;(x)&#160;=&#160;−<br/>
log(1&#160;−&#160;x2i)&#160;−<br/>
log(bi&#160;−&#160;aTi&#160;x)<br/>
(with&#160;n&#160;=&#160;104,&#160;m&#160;=&#160;105)<br/>
i=1<br/>
i=1<br/>
105<br/>
)<br/>(x<br/>f<br/>f&#160;100<br/>
in<br/>−<br/>))<br/>
(k<br/>
(x&#160;10−5<br/>
f<br/>
0<br/>
5<br/>
10<br/>
15<br/>
20<br/>
k<br/>
•&#160;high&#160;accuracy&#160;after&#160;small&#160;number&#160;of&#160;iterations<br/>
•&#160;fast&#160;asymptotic&#160;convergence<br/>
Interior-point&#160;methods<br/>
78<br/>
<hr/>
<a name=81></a>Classical&#160;convergence&#160;analysis<br/>
Assumptions&#160;(m,&#160;L&#160;are&#160;positive&#160;constants)<br/>
•&#160;f&#160;strongly&#160;convex:&#160;∇2f(x)&#160;&#160;mI<br/>
•&#160;∇2f&#160;Lipschitz&#160;continuous:&#160;k∇2f(x)&#160;−&#160;∇2f(y)k2&#160;≤&#160;Lkx&#160;−&#160;yk2<br/>
Summary:&#160;two&#160;regimes<br/>
•&#160;damped&#160;phase&#160;(k∇f(x)k2&#160;large):&#160;for&#160;some&#160;constant&#160;γ&#160;&gt;&#160;0<br/>
f&#160;(x(k+1))&#160;−&#160;f(x(k))&#160;≤&#160;−γ<br/>
•&#160;quadratic&#160;convergence&#160;(k∇f(x)k2&#160;small)<br/>
k∇f(x(k))k2&#160;decreases&#160;quadratically<br/>
Interior-point&#160;methods<br/>
79<br/>
<hr/>
<a name=82></a>Self-concordant&#160;functions<br/>
Shortcomings&#160;of&#160;classical&#160;convergence&#160;analysis<br/>
•&#160;depends&#160;on&#160;unknown&#160;constants&#160;(m,&#160;L,&#160;.&#160;.&#160;.&#160;)<br/>
•&#160;bound&#160;is&#160;not&#160;affinely&#160;invariant,&#160;although&#160;Newton’s&#160;method&#160;is<br/>
Analysis&#160;for&#160;self-concordant&#160;functions&#160;(Nesterov&#160;and&#160;Nemirovski,&#160;1994)<br/>
•&#160;a&#160;convex&#160;function&#160;of&#160;one&#160;variable&#160;is&#160;self-concordant&#160;if<br/>
|f′′′(x)|&#160;≤&#160;2f′′(x)3/2&#160;for&#160;all&#160;x&#160;∈&#160;dom&#160;f<br/>
a&#160;function&#160;of&#160;several&#160;variables&#160;is&#160;s.c.&#160;if&#160;its&#160;restriction&#160;to&#160;lines&#160;is&#160;s.c.<br/>
•&#160;analysis&#160;is&#160;affine-invariant,&#160;does&#160;not&#160;depend&#160;on&#160;unknown&#160;constants<br/>
•&#160;developed&#160;for&#160;complexity&#160;theory&#160;of&#160;interior-point&#160;methods<br/>
Interior-point&#160;methods<br/>
80<br/>
<hr/>
<a name=83></a>Interior-point&#160;methods<br/>
minimize<br/>
f0(x)<br/>
subjec&#160;to&#160;fi(x)&#160;≤&#160;0,<br/>
i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;m<br/>
Ax&#160;=&#160;b<br/>
functions&#160;fi,&#160;i&#160;=&#160;0,&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;m,&#160;are&#160;convex<br/>
Basic&#160;idea:&#160;follow&#160;‘central&#160;path’&#160;through&#160;interior&#160;feasible&#160;set&#160;to&#160;solution<br/>
c<br/>
Interior-point&#160;methods<br/>
81<br/>
<hr/>
<a name=84></a>General&#160;properties<br/>
•&#160;path-following&#160;mechanism&#160;relies&#160;on&#160;Newton’s&#160;method<br/>
•&#160;every&#160;iteration&#160;requires&#160;solving&#160;a&#160;set&#160;of&#160;linear&#160;equations&#160;(KKT&#160;system)<br/>
•&#160;number&#160;of&#160;iterations&#160;small&#160;(10–50),&#160;fairly&#160;independent&#160;of&#160;problem&#160;size<br/>
•&#160;some&#160;versions&#160;known&#160;to&#160;have&#160;polynomial&#160;worst-case&#160;complexity<br/>
History<br/>
•&#160;introduced&#160;in&#160;1950s&#160;and&#160;1960s<br/>
•&#160;used&#160;in&#160;polynomial-time&#160;methods&#160;for&#160;linear&#160;programming&#160;(1980s)<br/>
•&#160;polynomial-time&#160;algorithms&#160;for&#160;general&#160;convex&#160;optimization&#160;(ca.&#160;1990)<br/>
Interior-point&#160;methods<br/>
82<br/>
<hr/>
<a name=85></a>Reformulation&#160;via&#160;indicator&#160;function<br/>
minimize<br/>
f0(x)<br/>
subject&#160;to&#160;fi(x)&#160;≤&#160;0,<br/>
i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;m<br/>
Ax&#160;=&#160;b<br/>
Reformulation<br/>
minimize<br/>
f0(x)&#160;+&#160;Pm&#160;I<br/>
i=1&#160;−(fi(x))<br/>
subject&#160;to&#160;Ax&#160;=&#160;b<br/>
where&#160;I−&#160;is&#160;indicator&#160;function&#160;of&#160;R−:<br/>
I−(u)&#160;=&#160;0&#160;if&#160;u&#160;≤&#160;0,<br/>
I−(u)&#160;=&#160;∞&#160;otherwise<br/>
•&#160;reformulated&#160;problem&#160;has&#160;no&#160;inequality&#160;constraints<br/>
•&#160;however,&#160;objective&#160;function&#160;is&#160;not&#160;differentiable<br/>
Interior-point&#160;methods<br/>
83<br/>
<hr/>
<a name=86></a>Approximation&#160;via&#160;logarithmic&#160;barrier<br/>
1&#160;m<br/>
X<br/>
minimize<br/>
f0(x)&#160;−<br/>
log(−f<br/>
t<br/>
i(x))<br/>
i=1<br/>
subject&#160;to&#160;Ax&#160;=&#160;b<br/>
•&#160;for&#160;t&#160;&gt;&#160;0,&#160;−(1/t)&#160;log(−u)&#160;is&#160;a&#160;smooth&#160;approximation&#160;of&#160;I−<br/>
•&#160;approximation&#160;improves&#160;as&#160;t&#160;→&#160;∞<br/>
10<br/>
t<br/>)/&#160;5<br/>
u<br/>(−<br/>g<br/>lo&#160;0<br/>
−<br/>
−5<br/>
−3<br/>
−2<br/>
−1<br/>
0<br/>
1<br/>
u<br/>
Interior-point&#160;methods<br/>
84<br/>
<hr/>
<a name=87></a>Logarithmic&#160;barrier&#160;function<br/>
m<br/>
X<br/>
φ(x)&#160;=&#160;−<br/>
log(−fi(x))<br/>
i=1<br/>
with&#160;dom&#160;φ&#160;=&#160;{x&#160;|&#160;f1(x)&#160;&lt;&#160;0,&#160;.&#160;.&#160;.&#160;,&#160;fm(x)&#160;&lt;&#160;0}<br/>
•&#160;convex&#160;(follows&#160;from&#160;composition&#160;rules&#160;and&#160;convexity&#160;of&#160;fi)<br/>
•&#160;twice&#160;continuously&#160;differentiable,&#160;with&#160;derivatives<br/>
m<br/>
1<br/>
∇<br/>
X<br/>
φ(x)&#160;=<br/>
∇f<br/>
−f<br/>
i(x)<br/>
i=1<br/>
i(x)<br/>
m<br/>
1<br/>
m<br/>
1<br/>
∇2<br/>
X<br/>
X<br/>
φ(x)&#160;=<br/>
∇f<br/>
∇2f<br/>
f<br/>
i(x)∇fi(x)T&#160;+<br/>
−f<br/>
i(x)<br/>
i=1<br/>
i(x)2<br/>
i=1<br/>
i(x)<br/>
Interior-point&#160;methods<br/>
85<br/>
<hr/>
<a name=88></a>Central&#160;path<br/>
central&#160;path&#160;is&#160;{x⋆(t)&#160;|&#160;t&#160;&gt;&#160;0},&#160;where&#160;x⋆(t)&#160;is&#160;the&#160;solution&#160;of<br/>
minimize<br/>
tf0(x)&#160;+&#160;φ(x)<br/>
subject&#160;to&#160;Ax&#160;=&#160;b<br/>
Example:&#160;central&#160;path&#160;for&#160;an&#160;LP<br/>
c<br/>
minimize<br/>
cT&#160;x<br/>
subject&#160;to&#160;aTi&#160;x&#160;≤&#160;bi,&#160;i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;6<br/>
x⋆<br/>
x⋆(10)<br/>
hyperplane&#160;cT&#160;x&#160;=&#160;cT&#160;x⋆(t)&#160;is&#160;tangent&#160;to<br/>level&#160;curve&#160;of&#160;φ&#160;through&#160;x⋆(t)<br/>
Interior-point&#160;methods<br/>
86<br/>
<hr/>
<a name=89></a>Barrier&#160;method<br/>
given&#160;strictly&#160;feasible&#160;x,&#160;t&#160;:=&#160;t(0)&#160;&gt;&#160;0,&#160;µ&#160;&gt;&#160;1,&#160;tolerance&#160;ǫ&#160;&gt;&#160;0<br/>repeat:<br/>
1.&#160;Centering&#160;step.&#160;Compute&#160;x⋆(t)&#160;and&#160;set&#160;x&#160;:=&#160;x⋆(t)<br/>2.&#160;Stopping&#160;criterion.&#160;Terminate&#160;if&#160;m/t&#160;&lt;&#160;ǫ<br/>3.&#160;Increase&#160;t.&#160;t&#160;:=&#160;µt<br/>
•&#160;stopping&#160;criterion&#160;m/t&#160;≤&#160;ǫ&#160;guarantees<br/>
f0(x)&#160;−&#160;optimal&#160;value&#160;≤&#160;ǫ<br/>
(follows&#160;from&#160;duality)<br/>
•&#160;typical&#160;value&#160;of&#160;µ&#160;is&#160;10–20<br/>•&#160;several&#160;heuristics&#160;for&#160;choice&#160;of&#160;t(0)<br/>•&#160;centering&#160;usually&#160;done&#160;using&#160;Newton’s&#160;method,&#160;starting&#160;at&#160;current&#160;x<br/>
Interior-point&#160;methods<br/>
87<br/>
<hr/>
<a name=90></a>Example:&#160;Inequality&#160;form&#160;LP<br/>
m&#160;=&#160;100&#160;inequalities,&#160;n&#160;=&#160;50&#160;variables<br/>
102<br/>
140<br/>
s<br/>n&#160;120<br/>
100<br/>
100<br/>
gap<br/>y<br/>
eratio<br/>
10−2<br/>
it&#160;80<br/>
alit<br/>u<br/>
60<br/>
d<br/>
ton<br/>
10−4<br/>
ew&#160;40<br/>
N<br/>
10−6<br/>
µ&#160;=&#160;50&#160;µ&#160;=&#160;150<br/>
µ&#160;=&#160;2<br/>
20<br/>
0<br/>
0<br/>
20<br/>
40<br/>
60<br/>
80<br/>
0<br/>
40<br/>
80<br/>
120<br/>
160<br/>
200<br/>
Newton&#160;iterations<br/>
µ<br/>
•&#160;starts&#160;with&#160;x&#160;on&#160;central&#160;path&#160;(t(0)&#160;=&#160;1,&#160;duality&#160;gap&#160;100)<br/>
•&#160;terminates&#160;when&#160;t&#160;=&#160;108&#160;(gap&#160;m/t&#160;=&#160;10−6)<br/>
•&#160;total&#160;number&#160;of&#160;Newton&#160;iterations&#160;not&#160;very&#160;sensitive&#160;for&#160;µ&#160;≥&#160;10<br/>
Interior-point&#160;methods<br/>
88<br/>
<hr/>
<a name=91></a>Family&#160;of&#160;standard&#160;LPs<br/>
minimize<br/>
cT&#160;x<br/>
subject&#160;to&#160;Ax&#160;=&#160;b,<br/>
x&#160;&#160;0<br/>
A&#160;∈&#160;Rm×2m;&#160;for&#160;each&#160;m,&#160;solve&#160;100&#160;randomly&#160;generated&#160;instances<br/>
35<br/>
s<br/>n&#160;30<br/>
eratio<br/>it&#160;25<br/>
ton<br/>ew<br/>N&#160;20<br/>
15101<br/>
102<br/>
103<br/>
m<br/>
number&#160;of&#160;iterations&#160;grows&#160;very&#160;slowly&#160;as&#160;m&#160;ranges&#160;over&#160;a&#160;100&#160;:&#160;1&#160;ratio<br/>
Interior-point&#160;methods<br/>
89<br/>
<hr/>
<a name=92></a>Second-order&#160;cone&#160;programming<br/>
minimize<br/>
f&#160;T&#160;x<br/>
subject&#160;to&#160;kAix&#160;+&#160;bik2&#160;≤&#160;cTi&#160;x&#160;+&#160;di,&#160;i&#160;=&#160;1,&#160;.&#160;.&#160;.&#160;,&#160;m<br/>
Logarithmic&#160;barrier&#160;function<br/>
m<br/>
X<br/>
φ(x)&#160;=&#160;−<br/>
log&#160;(cT<br/>
<br/>
i&#160;x&#160;+&#160;di)2&#160;−&#160;kAix&#160;+&#160;bik2<br/>
2<br/>
i=1<br/>
•&#160;a&#160;convex&#160;function<br/>
•&#160;log(v2&#160;−&#160;uT&#160;u)&#160;is&#160;‘logarithm’&#160;for&#160;2nd-order&#160;cone&#160;{(u,&#160;v)&#160;|&#160;kuk2&#160;≤&#160;v}<br/>
Barrier&#160;method:&#160;follows&#160;central&#160;path&#160;x⋆(t)&#160;=&#160;argmin(tf&#160;T&#160;x&#160;+&#160;φ(x))<br/>
Interior-point&#160;methods<br/>
90<br/>
<hr/>
<a name=93></a>Example<br/>
50&#160;variables,&#160;50&#160;second-order&#160;cone&#160;constraints&#160;in&#160;R6<br/>
102<br/>
s&#160;120<br/>
n<br/>
100<br/>
gap<br/>y<br/>
eratio<br/>
10−2<br/>
it&#160;80<br/>
alit<br/>u<br/>d<br/>
ton<br/>
10−4<br/>
ew&#160;40<br/>
N<br/>
10−6<br/>
µ&#160;=&#160;50&#160;µ&#160;=&#160;200<br/>
µ&#160;=&#160;2<br/>
0<br/>
0<br/>
20<br/>
40<br/>
60<br/>
80<br/>
20<br/>
60<br/>
100<br/>
140<br/>
180<br/>
Newton&#160;iterations<br/>
µ<br/>
Interior-point&#160;methods<br/>
91<br/>
<hr/>
<a name=94></a>Semidefinite&#160;programming<br/>
minimize<br/>
cT&#160;x<br/>
subject&#160;to&#160;x1A1&#160;+&#160;·&#160;·&#160;·&#160;+&#160;xnAn&#160;&#160;B<br/>
Logarithmic&#160;barrier&#160;function<br/>
φ(x)&#160;=&#160;−&#160;log&#160;det(B&#160;−&#160;x1A1&#160;−&#160;·&#160;·&#160;·&#160;−&#160;xnAn)<br/>
•&#160;a&#160;convex&#160;function<br/>
•&#160;log&#160;det&#160;X&#160;is&#160;‘logarithm’&#160;for&#160;p.s.d.&#160;cone<br/>
Barrier&#160;method:&#160;follows&#160;central&#160;path&#160;x⋆(t)&#160;=&#160;argmin(tf&#160;T&#160;x&#160;+&#160;φ(x))<br/>
Interior-point&#160;methods<br/>
92<br/>
<hr/>
<a name=95></a>Example<br/>
100&#160;variables,&#160;one&#160;linear&#160;matrix&#160;inequality&#160;in&#160;S100<br/>
140<br/>
102<br/>
s<br/>n<br/>
100<br/>
100<br/>
gap<br/>y<br/>
eratio<br/>
10−2<br/>
it<br/>
alit<br/>
60<br/>
u<br/>d<br/>
ton<br/>
10−4<br/>
ew<br/>N<br/>
10−6<br/>
µ&#160;=&#160;150&#160;µ&#160;=&#160;50<br/>
µ&#160;=&#160;2<br/>
20<br/>
0<br/>
20<br/>
40<br/>
60<br/>
80<br/>
100<br/>
0<br/>
20<br/>
40<br/>
60<br/>
80<br/>
100<br/>
120<br/>
Newton&#160;iterations<br/>
µ<br/>
Interior-point&#160;methods<br/>
93<br/>
<hr/>
<a name=96></a>Complexity&#160;of&#160;barrier&#160;method<br/>
Iteration&#160;complexity<br/>
•&#160;can&#160;be&#160;bounded&#160;by&#160;polynomial&#160;function&#160;of&#160;problem&#160;dimensions&#160;(with<br/>
correct&#160;formulation,&#160;barrier&#160;function)<br/>
√<br/>
•&#160;examples:&#160;O(&#160;m)&#160;iteration&#160;bound&#160;for&#160;LP&#160;or&#160;SOCP&#160;with&#160;m&#160;inequalities,<br/>
SDP&#160;with&#160;constraint&#160;of&#160;order&#160;m<br/>
•&#160;proofs&#160;rely&#160;on&#160;theory&#160;of&#160;Newton’s&#160;method&#160;for&#160;self-concordant&#160;functions<br/>
•&#160;in&#160;practice:&#160;#iterations&#160;roughly&#160;constant&#160;as&#160;a&#160;function&#160;of&#160;problem&#160;size<br/>
Linear&#160;algebra&#160;complexity<br/>
dominated&#160;by&#160;solution&#160;of&#160;Newton&#160;system<br/>
Interior-point&#160;methods<br/>
94<br/>
<hr/>
<a name=97></a>Primal-dual&#160;interior-point&#160;methods<br/>
Similarities&#160;with&#160;barrier&#160;method<br/>
•&#160;follow&#160;the&#160;same&#160;central&#160;path<br/>
•&#160;linear&#160;algebra&#160;(KKT&#160;system)&#160;per&#160;iteration&#160;is&#160;similar<br/>
Differences<br/>
•&#160;faster&#160;and&#160;more&#160;robust<br/>
•&#160;update&#160;primal&#160;and&#160;dual&#160;variables&#160;in&#160;each&#160;step<br/>
•&#160;no&#160;distinction&#160;between&#160;inner&#160;(centering)&#160;and&#160;outer&#160;iterations<br/>
•&#160;include&#160;heuristics&#160;for&#160;adaptive&#160;choice&#160;of&#160;barrier&#160;parameter&#160;t<br/>
•&#160;can&#160;start&#160;at&#160;infeasible&#160;points<br/>
•&#160;often&#160;exhibit&#160;superlinear&#160;asymptotic&#160;convergence<br/>
Interior-point&#160;methods<br/>
95<br/>
<hr/>
<a name=98></a>Software&#160;implementations<br/>
General-purpose&#160;software&#160;for&#160;nonlinear&#160;convex&#160;optimization<br/>
•&#160;several&#160;high-quality&#160;packages&#160;(MOSEK,&#160;Sedumi,&#160;SDPT3,&#160;.&#160;.&#160;.&#160;)<br/>
•&#160;exploit&#160;sparsity&#160;to&#160;achieve&#160;scalability<br/>
Customized&#160;implementations<br/>
•&#160;can&#160;exploit&#160;non-sparse&#160;types&#160;of&#160;problem&#160;structure<br/>
•&#160;often&#160;orders&#160;of&#160;magnitude&#160;faster&#160;than&#160;general-purpose&#160;solvers<br/>
Interior-point&#160;methods<br/>
96<br/>
<hr/>
<a name=99></a>Example:&#160;ℓ1-regularized&#160;least-squares<br/>
minimize&#160;kAx&#160;−&#160;bk22&#160;+&#160;kxk1<br/>
A&#160;is&#160;m&#160;×&#160;n&#160;(with&#160;m&#160;≤&#160;n)&#160;and&#160;dense<br/>
Quadratic&#160;program&#160;formulation<br/>
minimize<br/>
kAx&#160;−&#160;bk22&#160;+&#160;1T&#160;u<br/>
subject&#160;to&#160;−u&#160;&#160;x&#160;&#160;u<br/>
•&#160;coefficient&#160;of&#160;Newton&#160;system&#160;in&#160;interior-point&#160;method&#160;is<br/>
<br/>
AT&#160;A&#160;0&#160;&#160;&#160;D<br/>
<br/>
+<br/>
1&#160;+&#160;D2<br/>
D2&#160;−&#160;D1<br/>
(D<br/>
0<br/>
0<br/>
D<br/>
1,&#160;D2&#160;positive&#160;diagonal)<br/>
2&#160;−&#160;D1<br/>
D1&#160;+&#160;D2<br/>
•&#160;very&#160;expensive&#160;(O(n3))&#160;for&#160;large&#160;n<br/>
Interior-point&#160;methods<br/>
97<br/>
<hr/>
<a name=100></a>Customized&#160;implementation<br/>
•&#160;can&#160;reduce&#160;Newton&#160;equation&#160;to&#160;solution&#160;of&#160;a&#160;system<br/>
(AD−1AT&#160;+&#160;I)∆u&#160;=&#160;r<br/>
•&#160;cost&#160;per&#160;iteration&#160;is&#160;O(m2n)<br/>
Comparison&#160;(seconds&#160;on&#160;3.2Ghz&#160;machine)<br/>
m<br/>
n<br/>
custom<br/>
general-purpose<br/>
50<br/>
100<br/>
0.02<br/>
0.05<br/>
50<br/>
200<br/>
0.03<br/>
0.17<br/>
100<br/>
1000<br/>
0.32<br/>
10.6<br/>
100<br/>
2000<br/>
0.71<br/>
76.9<br/>
500<br/>
1000<br/>
2.5<br/>
11.2<br/>
500<br/>
2000<br/>
5.5<br/>
79.8<br/>
general-purpose&#160;solver&#160;is&#160;MOSEK<br/>
Interior-point&#160;methods<br/>
98<br/>
<hr/>
<a name=101></a>Convex&#160;optimization&#160;—&#160;MLSS&#160;2009<br/>
First-order&#160;methods<br/>
•&#160;gradient&#160;method<br/>
•&#160;Nesterov’s&#160;gradient&#160;methods<br/>
•&#160;extensions<br/>
99<br/>
<hr/>
<a name=102></a>Gradient&#160;method<br/>
to&#160;minimize&#160;a&#160;convex&#160;differentiable&#160;function&#160;f&#160;:&#160;choose&#160;x(0)&#160;and&#160;repeat<br/>
x(k)&#160;=&#160;x(k−1)&#160;−&#160;tk∇f(x(k−1)),<br/>
k&#160;=&#160;1,&#160;2,&#160;.&#160;.&#160;.<br/>
tk&#160;is&#160;step&#160;size&#160;(fixed&#160;or&#160;determined&#160;by&#160;backtracking&#160;line&#160;search)<br/>
Classical&#160;convergence&#160;result<br/>
•&#160;assume&#160;∇f&#160;Lipschitz&#160;continuous&#160;(k∇f(x)&#160;−&#160;∇f(y)k2&#160;≤&#160;Lkx&#160;−&#160;yk2)<br/>
•&#160;error&#160;decreases&#160;as&#160;1/k,&#160;hence<br/>
&#160;1<br/>
O<br/>
iterations<br/>
ǫ<br/>
needed&#160;to&#160;reach&#160;accuracy&#160;f&#160;(x(k))&#160;−&#160;f⋆&#160;≤&#160;ǫ<br/>
First-order&#160;methods<br/>
100<br/>
<hr/>
<a name=103></a>Nesterov’s&#160;gradient&#160;method<br/>
choose&#160;x(0);&#160;take&#160;x(1)&#160;=&#160;x(0)&#160;−&#160;t1∇f(x(0))&#160;and&#160;for&#160;k&#160;≥&#160;2<br/>
k&#160;−&#160;2<br/>
y(k)&#160;=&#160;x(k−1)&#160;+<br/>
(x(k−1)&#160;−&#160;x(k−2))<br/>
k&#160;+&#160;1<br/>
x(k)&#160;=&#160;y(k)&#160;−&#160;tk∇f(y(k))<br/>
•&#160;gradient&#160;method&#160;with&#160;‘extrapolation’<br/>
•&#160;if&#160;f&#160;has&#160;Lipschitz&#160;continuous&#160;gradient,&#160;error&#160;decreases&#160;as&#160;1/k2;&#160;hence<br/>
&#160;1&#160;<br/>
O&#160;√<br/>
iterations<br/>
ǫ<br/>
needed&#160;to&#160;reach&#160;accuracy&#160;f&#160;(x(k))&#160;−&#160;f⋆&#160;≤&#160;ǫ<br/>
•&#160;many&#160;variations;&#160;first&#160;one&#160;published&#160;in&#160;1983<br/>
First-order&#160;methods<br/>
101<br/>
<hr/>
<a name=104></a>Example<br/>
m<br/>
X<br/>
minimize&#160;log<br/>
exp(aTi&#160;x&#160;+&#160;bi)<br/>
i=1<br/>
randomly&#160;generated&#160;data&#160;with&#160;m&#160;=&#160;2000,&#160;n&#160;=&#160;1000,&#160;fixed&#160;step&#160;size<br/>
100<br/>
gradient<br/>Nesterov<br/>
10-1<br/>
|⋆<br/>|f&#160;10-2<br/>
)/⋆<br/>f<br/>−&#160;10-3<br/>
))<br/>
(k<br/>
10-4<br/>
(x<br/>(f&#160;10-5<br/>
10-6&#160;0<br/>
50<br/>
100<br/>
150<br/>
200<br/>
k<br/>
First-order&#160;methods<br/>
102<br/>
<hr/>
<a name=105></a>Interpretation&#160;of&#160;gradient&#160;update<br/>
x(k)&#160;=&#160;x(k−1)&#160;−&#160;tk∇f(x(k−1))<br/>
<br/>
1<br/>
<br/>
=&#160;argmin&#160;∇f(x(k−1))T&#160;z&#160;+<br/>
kz&#160;−&#160;x(k−1)k22<br/>
z<br/>
tk<br/>
Interpretation<br/>
x(k)&#160;minimizes<br/>
1<br/>
f&#160;(x(k−1))&#160;+&#160;∇f(x(k−1))T&#160;(z&#160;−&#160;x(k−1))&#160;+<br/>
kz&#160;−&#160;x(k−1)k2<br/>
t<br/>
2<br/>
k<br/>
a&#160;simple&#160;quadratic&#160;model&#160;of&#160;f&#160;at&#160;x(k−1)<br/>
First-order&#160;methods<br/>
103<br/>
<hr/>
<a name=106></a>Projected&#160;gradient&#160;method<br/>
minimize<br/>
f&#160;(x)<br/>
subject&#160;to&#160;x&#160;∈&#160;C<br/>
f&#160;convex,&#160;C&#160;a&#160;closed&#160;convex&#160;set<br/>
<br/>
1<br/>
<br/>
x(k)&#160;=&#160;argmin&#160;∇f(x(k−1))T&#160;z&#160;+<br/>
kz&#160;−&#160;x(k−1)k22<br/>
z∈C<br/>
tk<br/>
<br/>
<br/>
=&#160;PC&#160;x(k−1)&#160;−&#160;tk∇f(x(k−1))<br/>
•&#160;useful&#160;if&#160;projection&#160;PC&#160;on&#160;C&#160;is&#160;inexpensive&#160;(e.g.,&#160;box&#160;constraints)<br/>
•&#160;similar&#160;convergence&#160;result&#160;as&#160;for&#160;basic&#160;gradient&#160;algorithm<br/>
•&#160;can&#160;be&#160;used&#160;in&#160;fast&#160;Nesterov-type&#160;gradient&#160;methods<br/>
First-order&#160;methods<br/>
104<br/>
<hr/>
<a name=107></a>Nonsmooth&#160;components<br/>
minimize&#160;f&#160;(x)&#160;+&#160;g(x)<br/>
f&#160;,&#160;g&#160;convex,&#160;with&#160;f&#160;differentiable,&#160;g&#160;nondifferentiable<br/>
<br/>
1<br/>
<br/>
x(k)&#160;=&#160;argmin&#160;∇f(x(k−1))T&#160;z&#160;+&#160;g(x)&#160;+<br/>
kz&#160;−&#160;x(k−1)k22<br/>
z<br/>
tk<br/>
<br/>
1<br/>
<br/>
<br/>
2<br/>
=&#160;argmin<br/>
z&#160;−&#160;x(k−1)&#160;+&#160;tk∇f&#160;(x(k−1))&#160;+&#160;g(z)<br/>
<br/>
<br/>
z<br/>
2tk<br/>
2<br/>
∆<br/>
<br/>
<br/>
=&#160;St<br/>
x(k−1)&#160;−&#160;t<br/>
k<br/>
k∇f&#160;(x(k−1))<br/>
•&#160;gradient&#160;step&#160;for&#160;f&#160;followed&#160;by&#160;‘thresholding’&#160;operation&#160;St<br/>•&#160;useful&#160;if&#160;thresholding&#160;is&#160;inexpensive&#160;(e.g.,&#160;because&#160;g&#160;is&#160;separable)<br/>•&#160;similar&#160;convergence&#160;result&#160;as&#160;basic&#160;gradient&#160;method<br/>
First-order&#160;methods<br/>
105<br/>
<hr/>
<a name=108></a>Example:&#160;ℓ1-norm&#160;regularization<br/>
minimize&#160;f&#160;(x)&#160;+&#160;kxk1<br/>
f&#160;convex&#160;and&#160;differentiable<br/>
Thresholding&#160;operator<br/>
&#160;1<br/>
<br/>
St(y)&#160;=&#160;argmin<br/>
kz&#160;−&#160;yk2&#160;+<br/>
2<br/>
kzk1<br/>
z<br/>
2t<br/>
St(y)k<br/>
<br/>
yk&#160;−&#160;t&#160;yk&#160;≥&#160;t<br/>
<br/>
−t<br/>
S<br/>
t<br/>
y<br/>
t(y)k&#160;=<br/>
0<br/>
−t&#160;≤&#160;yk&#160;≤&#160;t<br/>
k<br/>
<br/>
yk&#160;+&#160;t&#160;yk&#160;≤&#160;−t<br/>
First-order&#160;methods<br/>
106<br/>
<hr/>
<a name=109></a>ℓ1-Norm&#160;regularized&#160;least-squares<br/>
1<br/>
minimize<br/>
kAx&#160;−&#160;bk2<br/>
2<br/>
2&#160;+&#160;kxk1<br/>
100<br/>
gradient<br/>Nesterov<br/>
10-1<br/>
⋆&#160;f&#160;10-2<br/>
)/⋆<br/>f&#160;10-3<br/>
−<br/>))&#160;10-4<br/>
(k<br/>
(x&#160;10-5<br/>
(f<br/>
10-6<br/>
10-7&#160;0<br/>
20<br/>
40<br/>
60<br/>
80<br/>
100<br/>
k<br/>
randomly&#160;generated&#160;A&#160;∈&#160;R2000×1000;&#160;fixed&#160;step<br/>
First-order&#160;methods<br/>
107<br/>
<hr/>
<a name=110></a>Summary:&#160;Advances&#160;in&#160;convex&#160;optimization<br/>
Theory<br/>
new&#160;problem&#160;classes,&#160;robust&#160;optimization,&#160;convex&#160;relaxations,&#160;.&#160;.&#160;.<br/>
Applications<br/>
new&#160;applications&#160;in&#160;different&#160;fields;&#160;surprisingly&#160;many&#160;discovered&#160;recently<br/>
Algorithms&#160;and&#160;software<br/>
•&#160;high-quality&#160;general-purpose&#160;implementations&#160;of&#160;interior-point&#160;methods<br/>
•&#160;software&#160;packages&#160;for&#160;convex&#160;modeling<br/>
•&#160;new&#160;first-order&#160;methods<br/>
108<br/>
<hr/>
</body>
</html>
