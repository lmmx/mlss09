<!DOCTYPE html><html>
<head>
<title></title>
<style type="text/css">
<!--
.xflip {
    -moz-transform: scaleX(-1);
    -webkit-transform: scaleX(-1);
    -o-transform: scaleX(-1);
    transform: scaleX(-1);
    filter: fliph;
}
.yflip {
    -moz-transform: scaleY(-1);
    -webkit-transform: scaleY(-1);
    -o-transform: scaleY(-1);
    transform: scaleY(-1);
    filter: flipv;
}
.xyflip {
    -moz-transform: scaleX(-1) scaleY(-1);
    -webkit-transform: scaleX(-1) scaleY(-1);
    -o-transform: scaleX(-1) scaleY(-1);
    transform: scaleX(-1) scaleY(-1);
    filter: fliph + flipv;
}
-->
</style>
</head>
<body>
<a name=1></a><b>Learning&#160;Theory</b><br/>
John&#160;Shawe-Taylor<br/>
Centre&#160;for&#160;Computational&#160;Statistics<br/>
and&#160;Machine&#160;Learning<br/>
University&#160;College&#160;London<br/>
jst@cs.ucl.ac.uk<br/>
September,&#160;2009<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
<hr/>
<a name=2></a><b>STRUCTURE</b><br/>
PART&#160;A<br/>
1.&#160;General&#160;Statistical&#160;Considerations<br/>2.&#160;Basic&#160;PAC&#160;Ideas&#160;and&#160;proofs<br/>3.&#160;Real-valued&#160;Function&#160;Classes&#160;and&#160;the&#160;Margin<br/>
PART&#160;B<br/>
1.&#160;Rademacher&#160;complexity&#160;and&#160;Main&#160;Theory<br/>2.&#160;Applications&#160;to&#160;classification<br/>3.&#160;Conclusions<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
1<br/>
<hr/>
<a name=3></a><b>Aim:</b><br/>
<i>•&#160;</i>Some&#160;thoughts&#160;on&#160;why&#160;theory<br/>
<i>•&#160;</i>Basic&#160;Techniques&#160;with&#160;some&#160;deference&#160;to&#160;history<br/>
<i>•&#160;</i>Insights&#160;into&#160;proof&#160;techniques&#160;and&#160;statistical<br/>
learning&#160;approaches<br/>
<i>•&#160;</i>Concentration&#160;inequalities&#160;and&#160;relation&#160;to&#160;Rademacher<br/>
approach<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
2<br/>
<hr/>
<a name=4></a><b>What&#160;won’t&#160;be&#160;included:</b><br/>
<i>•&#160;</i>The&#160;most&#160;general&#160;results<br/>
<i>•&#160;</i>Complete&#160;History<br/>
<i>•&#160;</i>Analysis&#160;of&#160;Bayesian&#160;inference<br/>
<i>•&#160;</i>Most&#160;recent&#160;developments,&#160;eg&#160;PAC-Bayes,&#160;local<br/>
Rademacher&#160;complexity,&#160;etc.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
3<br/>
<hr/>
<a name=5></a><b>PART&#160;A</b><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
4<br/>
<hr/>
<a name=6></a><b>Theories&#160;of&#160;learning</b><br/>
<i>•&#160;</i>Basic&#160;approach&#160;of&#160;SLT&#160;is&#160;to&#160;view&#160;learning&#160;from&#160;a<br/>
statistical&#160;viewpoint.<br/>
<i>•&#160;</i>Aim&#160;of&#160;any&#160;theory&#160;is&#160;to&#160;model&#160;real/&#160;artificial<br/>
phenomena&#160;so&#160;that&#160;we&#160;can&#160;better&#160;understand/<br/>predict/&#160;exploit&#160;them.<br/>
<i>•&#160;</i>SLT&#160;is&#160;just&#160;one&#160;approach&#160;to&#160;understanding/<br/>
predicting/&#160;exploiting&#160;learning&#160;systems,&#160;others<br/>include&#160;Bayesian&#160;inference,&#160;inductive&#160;inference,<br/>statistical&#160;physics,&#160;traditional&#160;statistical&#160;analysis.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
5<br/>
<hr/>
<a name=7></a><b>Theories&#160;of&#160;learning&#160;cont.</b><br/>
<i>•&#160;</i>Each&#160;theory&#160;makes&#160;assumptions&#160;about&#160;the<br/>
phenomenon&#160;of&#160;learning&#160;and&#160;based&#160;on&#160;these<br/>derives&#160;predictions&#160;of&#160;behaviour<br/>
<b>–&#160;</b>every&#160;predictive&#160;theory&#160;automatically&#160;implies&#160;a<br/>
possible&#160;algorithm<br/>
<b>–&#160;</b>simply&#160;optimise&#160;the&#160;quantities&#160;that&#160;improve&#160;the<br/>
predictions<br/>
<i>•&#160;</i>Each&#160;theory&#160;has&#160;strengths&#160;and&#160;weaknesses<br/>
<b>–&#160;</b>generally&#160;speaking&#160;more&#160;assumptions,&#160;potentially<br/>
more&#160;accurate&#160;predictions<br/>
<b>–&#160;</b>BUT&#160;depends&#160;on&#160;capturing&#160;the&#160;right&#160;aspects&#160;of<br/>
the&#160;phenomenon<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
6<br/>
<hr/>
<a name=8></a><b>General&#160;statistical&#160;considerations</b><br/>
<i>•&#160;</i>Statistical&#160;models&#160;(not&#160;including&#160;Bayesian)&#160;begin<br/>
with&#160;an&#160;assumption&#160;that&#160;the&#160;data&#160;is&#160;generated<br/>by&#160;an&#160;underlying&#160;distribution&#160;<i>P&#160;</i>typically&#160;not&#160;given<br/>explicitly&#160;to&#160;the&#160;learner.<br/>
<i>•&#160;</i>If&#160;we&#160;are&#160;trying&#160;to&#160;classify&#160;cancerous&#160;tissue&#160;from<br/>
healthy&#160;tissue,&#160;there&#160;are&#160;two&#160;distributions,&#160;one&#160;for<br/>cancerous&#160;cells&#160;and&#160;one&#160;for&#160;healthy&#160;ones.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
7<br/>
<hr/>
<a name=9></a><b>General&#160;statistical&#160;considerations&#160;cont.</b><br/>
<i>•&#160;</i>Usually&#160;the&#160;distribution&#160;subsumes&#160;the&#160;processes<br/>
of&#160;the&#160;natural/artificial&#160;world&#160;that&#160;we&#160;are&#160;studying.<br/>
<i>•&#160;</i>Rather&#160;than&#160;accessing&#160;the&#160;distribution&#160;directly,<br/>
statistical&#160;learning&#160;typically&#160;assumes&#160;that&#160;we&#160;are<br/>given&#160;a&#160;‘training&#160;sample’&#160;or&#160;‘training&#160;set’<br/>
<i>S&#160;</i>=&#160;<i>{</i>(x1<i>,&#160;y</i>1)<i>,&#160;.&#160;.&#160;.&#160;,&#160;</i>(x<i>m,&#160;ym</i>)<i>}</i><br/>
generated&#160;identically&#160;and&#160;independently&#160;(i.i.d.)<br/>
according&#160;to&#160;the&#160;distribution&#160;<i>P&#160;</i>.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
8<br/>
<hr/>
<a name=10></a><b>Generalisation&#160;of&#160;a&#160;learner</b><br/>
<i>•&#160;</i>Assume&#160;that&#160;we&#160;have&#160;a&#160;learning&#160;algorithm&#160;A&#160;that<br/>
chooses&#160;a&#160;function&#160;AF(<i>S</i>)&#160;from&#160;a&#160;function&#160;space<br/>F&#160;in&#160;response&#160;to&#160;the&#160;training&#160;set&#160;<i>S</i>.<br/>
<i>•&#160;</i>From&#160;a&#160;statistical&#160;point&#160;of&#160;view&#160;the&#160;quantity&#160;of<br/>
interest&#160;is&#160;the&#160;random&#160;variable:<br/>
<i>²</i>(<i>S,&#160;</i>A<i>,&#160;</i>F)&#160;=&#160;E(x<i>,y</i>)&#160;[<i>`</i>(AF(<i>S</i>)<i>,&#160;</i>x<i>,&#160;y</i>)]&#160;<i>,</i><br/>
where&#160;<i>`&#160;</i>is&#160;a&#160;‘loss’&#160;function&#160;that&#160;measures&#160;the<br/>
discrepancy&#160;between&#160;AF(<i>S</i>)(x)&#160;and&#160;<i>y</i>.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
9<br/>
<hr/>
<a name=11></a><b>Generalisation&#160;of&#160;a&#160;learner</b><br/>
<i>•&#160;</i>For&#160;example,&#160;in&#160;the&#160;case&#160;of&#160;classification&#160;<i>`&#160;</i>is&#160;1<br/>
if&#160;the&#160;two&#160;disagree&#160;and&#160;0&#160;otherwise,&#160;while&#160;for<br/>regression&#160;it&#160;could&#160;be&#160;the&#160;square&#160;of&#160;the&#160;difference<br/>between&#160;AF(<i>S</i>)(x)&#160;and&#160;<i>y</i>.<br/>
<i>•&#160;</i>We&#160;refer&#160;to&#160;the&#160;random&#160;variable&#160;<i>²</i>(<i>S,&#160;</i>A<i>,&#160;</i>F)&#160;as&#160;the<br/>
generalisation&#160;of&#160;the&#160;learner.<br/>
<i>•&#160;</i>Note&#160;is&#160;random&#160;because&#160;of&#160;the&#160;dependence&#160;on<br/>
the&#160;training&#160;set&#160;<i>S</i>.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
10<br/>
<hr/>
<a name=12></a><b>Example&#160;of&#160;Generalisation&#160;I</b><br/>
<i>•&#160;</i>We&#160;consider&#160;the&#160;Breast&#160;Cancer&#160;dataset&#160;from&#160;the<br/>
UCI&#160;repository.<br/>
<i>•&#160;</i>Use&#160;the&#160;simple&#160;Parzen&#160;window&#160;classifier:&#160;weight<br/>
vector&#160;is<br/>
w+&#160;<i>−&#160;</i>w<i>−</i><br/>
where&#160;w+&#160;is&#160;the&#160;average&#160;of&#160;the&#160;positive&#160;training<br/>
examples&#160;and&#160;w<i>−&#160;</i>is&#160;average&#160;of&#160;negative&#160;training<br/>examples.<br/>
Threshold&#160;is&#160;set&#160;so&#160;hyperplane<br/>
bisects&#160;the&#160;line&#160;joining&#160;these&#160;two&#160;points.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
11<br/>
<hr/>
<a name=13></a><b>Example&#160;of&#160;Generalisation&#160;II</b><br/>
<i>•&#160;</i>Given&#160;a&#160;size&#160;<i>m&#160;</i>of&#160;the&#160;training&#160;set,&#160;by&#160;repeatedly<br/>
drawing&#160;random&#160;training&#160;sets&#160;<i>S&#160;</i>we&#160;estimate&#160;the<br/>distribution&#160;of<br/>
<i>²</i>(<i>S,&#160;</i>A<i>,&#160;</i>F)&#160;=&#160;E(x<i>,y</i>)&#160;[<i>`</i>(AF(<i>S</i>)<i>,&#160;</i>x<i>,&#160;y</i>)]&#160;<i>,</i><br/>
by&#160;using&#160;the&#160;test&#160;set&#160;error&#160;as&#160;a&#160;proxy&#160;for&#160;the&#160;true<br/>
generalisation.<br/>
<i>•&#160;</i>We&#160;plot&#160;the&#160;histogram&#160;and&#160;the&#160;average&#160;of&#160;the<br/>
distribution&#160;for&#160;various&#160;sizes&#160;of&#160;training&#160;set&#160;–<br/>initially&#160;the&#160;whole&#160;dataset&#160;gives&#160;a&#160;single&#160;value&#160;if<br/>we&#160;use&#160;training&#160;and&#160;test&#160;as&#160;the&#160;all&#160;the&#160;examples,<br/>but&#160;then&#160;we&#160;plot&#160;for&#160;training&#160;set&#160;sizes:<br/>
342<i>,&#160;</i>273<i>,&#160;</i>205<i>,&#160;</i>137<i>,&#160;</i>68<i>,&#160;</i>34<i>,&#160;</i>27<i>,&#160;</i>20<i>,&#160;</i>14<i>,&#160;</i>7<i>.</i><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
12<br/>
<hr/>
<a name=14></a><b>Example&#160;of&#160;Generalisation&#160;III</b><br/>
<i>•&#160;</i>Since&#160;the&#160;expected&#160;classifier&#160;is&#160;in&#160;all&#160;cases&#160;the<br/>
same:<br/>
£<br/>
¤<br/>
E&#160;[AF(<i>S</i>)]&#160;=&#160;E<i>S&#160;</i>w+&#160;<i>−&#160;</i>w<i>−</i><br/>
<i>S</i><br/>
<i>S</i><br/>
£<br/>
¤<br/>
£<br/>
¤<br/>
=&#160;E<i>S&#160;</i>w+&#160;<i>−&#160;</i>E&#160;w<i>−</i><br/>
<i>S</i><br/>
<i>S</i><br/>
<i>S</i><br/>
=&#160;E<i>y</i>=+1&#160;[x]&#160;<i>−&#160;</i>E<i>y</i>=<i>−</i>1&#160;[x]&#160;<i>,</i><br/>
we&#160;do&#160;not&#160;expect&#160;large&#160;differences&#160;in&#160;the&#160;average<br/>
of&#160;the&#160;distribution,&#160;though&#160;the&#160;non-linearity&#160;of<br/>the&#160;loss&#160;function&#160;means&#160;they&#160;won’t&#160;be&#160;the&#160;same<br/>exactly.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
13<br/>
<hr/>
<a name=15></a><b>Error&#160;distribution:&#160;full&#160;dataset</b><br/>
100<br/>
90<br/>
80<br/>
70<br/>
60<br/>
50<br/>
40<br/>
30<br/>
20<br/>
10<br/>
00<br/>
0.1<br/>
0.2<br/>
0.3<br/>
0.4<br/>
0.5<br/>
0.6<br/>
0.7<br/>
0.8<br/>
0.9<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
14<br/>
<hr/>
<a name=16></a><b>Error&#160;distribution:&#160;dataset&#160;size:&#160;342</b><br/>
30<br/>
25<br/>
20<br/>
15<br/>
10<br/>
5<br/>
00<br/>
0.1<br/>
0.2<br/>
0.3<br/>
0.4<br/>
0.5<br/>
0.6<br/>
0.7<br/>
0.8<br/>
0.9<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
15<br/>
<hr/>
<a name=17></a><b>Error&#160;distribution:&#160;dataset&#160;size:&#160;273</b><br/>
30<br/>
25<br/>
20<br/>
15<br/>
10<br/>
5<br/>
00<br/>
0.1<br/>
0.2<br/>
0.3<br/>
0.4<br/>
0.5<br/>
0.6<br/>
0.7<br/>
0.8<br/>
0.9<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
16<br/>
<hr/>
<a name=18></a><b>Error&#160;distribution:&#160;dataset&#160;size:&#160;205</b><br/>
35<br/>
30<br/>
25<br/>
20<br/>
15<br/>
10<br/>
5<br/>
00<br/>
0.1<br/>
0.2<br/>
0.3<br/>
0.4<br/>
0.5<br/>
0.6<br/>
0.7<br/>
0.8<br/>
0.9<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
17<br/>
<hr/>
<a name=19></a><b>Error&#160;distribution:&#160;dataset&#160;size:&#160;137</b><br/>
30<br/>
25<br/>
20<br/>
15<br/>
10<br/>
5<br/>
00<br/>
0.1<br/>
0.2<br/>
0.3<br/>
0.4<br/>
0.5<br/>
0.6<br/>
0.7<br/>
0.8<br/>
0.9<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
18<br/>
<hr/>
<a name=20></a><b>Error&#160;distribution:&#160;dataset&#160;size:&#160;68</b><br/>
25<br/>
20<br/>
15<br/>
10<br/>
5<br/>
00<br/>
0.1<br/>
0.2<br/>
0.3<br/>
0.4<br/>
0.5<br/>
0.6<br/>
0.7<br/>
0.8<br/>
0.9<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
19<br/>
<hr/>
<a name=21></a><b>Error&#160;distribution:&#160;dataset&#160;size:&#160;34</b><br/>
20<br/>
18<br/>
16<br/>
14<br/>
12<br/>
10<br/>
8<br/>
6<br/>
4<br/>
2<br/>
00<br/>
0.1<br/>
0.2<br/>
0.3<br/>
0.4<br/>
0.5<br/>
0.6<br/>
0.7<br/>
0.8<br/>
0.9<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
20<br/>
<hr/>
<a name=22></a><b>Error&#160;distribution:&#160;dataset&#160;size:&#160;27</b><br/>
16<br/>
14<br/>
12<br/>
10<br/>
8<br/>
6<br/>
4<br/>
2<br/>
00<br/>
0.1<br/>
0.2<br/>
0.3<br/>
0.4<br/>
0.5<br/>
0.6<br/>
0.7<br/>
0.8<br/>
0.9<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
21<br/>
<hr/>
<a name=23></a><b>Error&#160;distribution:&#160;dataset&#160;size:&#160;20</b><br/>
12<br/>
10<br/>
8<br/>
6<br/>
4<br/>
2<br/>
00<br/>
0.1<br/>
0.2<br/>
0.3<br/>
0.4<br/>
0.5<br/>
0.6<br/>
0.7<br/>
0.8<br/>
0.9<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
22<br/>
<hr/>
<a name=24></a><b>Error&#160;distribution:&#160;dataset&#160;size:&#160;14</b><br/>
12<br/>
10<br/>
8<br/>
6<br/>
4<br/>
2<br/>
00<br/>
0.1<br/>
0.2<br/>
0.3<br/>
0.4<br/>
0.5<br/>
0.6<br/>
0.7<br/>
0.8<br/>
0.9<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
23<br/>
<hr/>
<a name=25></a><b>Error&#160;distribution:&#160;dataset&#160;size:&#160;7</b><br/>
6<br/>
5<br/>
4<br/>
3<br/>
2<br/>
1<br/>
00<br/>
0.1<br/>
0.2<br/>
0.3<br/>
0.4<br/>
0.5<br/>
0.6<br/>
0.7<br/>
0.8<br/>
0.9<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
24<br/>
<hr/>
<a name=26></a><b>Bayes&#160;risk&#160;and&#160;consistency</b><br/>
<i>•&#160;</i>Traditional&#160;statistics&#160;has&#160;concentrated&#160;on&#160;analysing<br/>
E<i>S&#160;</i>[<i>²</i>(<i>S,&#160;</i>A<i>,&#160;</i>F)]&#160;<i>.</i><br/>
<i>•&#160;</i>For&#160;example&#160;consistency&#160;of&#160;a&#160;classification<br/>
algorithm&#160;A&#160;and&#160;function&#160;class&#160;F&#160;means<br/>
lim&#160;E<i>S&#160;</i>[<i>²</i>(<i>S,&#160;</i>A<i>,&#160;</i>F)]&#160;=&#160;<i>f</i>Bayes<i>,</i><br/>
<i>m→∞</i><br/>
where<br/>
½&#160;1&#160;if<i>P</i>(x<i>,</i>1)&#160;<i>&gt;&#160;P</i>(x<i>,</i>0)<i>,</i><br/>
<i>f</i>Bayes(x)&#160;=<br/>
0&#160;otherwise<i>.</i><br/>
is&#160;the&#160;function&#160;with&#160;the&#160;lowest&#160;possible&#160;risk,&#160;often<br/>
referred&#160;to&#160;as&#160;the&#160;Bayes&#160;risk.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
25<br/>
<hr/>
<a name=27></a><b>Expected&#160;versus&#160;confident&#160;bounds</b><br/>
<i>•&#160;</i>For&#160;a&#160;finite&#160;sample&#160;the&#160;generalisation&#160;<i>²</i>(<i>S,&#160;</i>A<i>,&#160;</i>F)<br/>
has&#160;a&#160;distribution&#160;depending&#160;on&#160;the&#160;algorithm,<br/>function&#160;class&#160;and&#160;sample&#160;size&#160;<i>m</i>.<br/>
<i>•&#160;</i>Traditional&#160;statistics&#160;as&#160;indicated&#160;above&#160;has<br/>
concentrated&#160;on&#160;the&#160;mean&#160;of&#160;this&#160;distribution&#160;–<br/>but&#160;this&#160;quantity&#160;can&#160;be&#160;misleading,&#160;eg&#160;for&#160;low<br/>fold&#160;cross-validation.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
26<br/>
<hr/>
<a name=28></a><b>Expected&#160;versus&#160;confident&#160;bounds</b><br/>
<b>cont.</b><br/>
<i>•&#160;</i>Statistical&#160;learning&#160;theory&#160;has&#160;preferred&#160;to<br/>
analyse&#160;the&#160;tail&#160;of&#160;the&#160;distribution,&#160;finding&#160;a&#160;bound<br/>which&#160;holds&#160;with&#160;high&#160;probability.<br/>
<i>•&#160;</i>This&#160;looks&#160;like&#160;a&#160;statistical&#160;test&#160;–&#160;significant&#160;at&#160;a<br/>
1%&#160;confidence&#160;means&#160;that&#160;the&#160;chances&#160;of&#160;the<br/>conclusion&#160;not&#160;being&#160;true&#160;are&#160;less&#160;than&#160;1%&#160;over<br/>random&#160;samples&#160;of&#160;that&#160;size.<br/>
<i>•&#160;</i>This&#160;is&#160;also&#160;the&#160;source&#160;of&#160;the&#160;acronym&#160;PAC:<br/>
probably&#160;approximately&#160;correct,&#160;the&#160;‘confidence’<br/>parameter&#160;<i>δ&#160;</i>is&#160;the&#160;probability&#160;that&#160;we&#160;have&#160;been<br/>misled&#160;by&#160;the&#160;training&#160;set.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
27<br/>
<hr/>
<a name=29></a><b>Probability&#160;of&#160;being&#160;misled&#160;in</b><br/>
<b>classification</b><br/>
<i>•&#160;</i>Aim&#160;to&#160;cover&#160;a&#160;number&#160;of&#160;key&#160;techniques&#160;of<br/>
SLT.&#160;Basic&#160;approach&#160;is&#160;usually&#160;to&#160;bound&#160;the<br/>probability&#160;of&#160;being&#160;misled&#160;and&#160;set&#160;this&#160;equal&#160;to<br/><i>δ</i>.<br/>
<i>•&#160;</i>What&#160;is&#160;the&#160;chance&#160;of&#160;being&#160;misled&#160;by&#160;a&#160;single<br/>
bad&#160;function&#160;<i>f&#160;</i>,&#160;i.e.&#160;training&#160;error&#160;err<i>S</i>(<i>f</i>)&#160;=&#160;0,<br/>while&#160;true&#160;error&#160;is&#160;bad&#160;err(<i>f&#160;</i>)&#160;<i>&gt;&#160;²</i>?<br/>
<i>PS&#160;{</i>err<i>S</i>(<i>f</i>)&#160;=&#160;0<i>,&#160;</i>err(<i>f</i>)&#160;<i>&gt;&#160;²}&#160;</i>=&#160;(1&#160;<i>−&#160;</i>err(<i>f</i>))<i>m</i><br/>
<i>≤&#160;</i>(1&#160;<i>−&#160;²</i>)<i>m</i><br/>
<i>≤&#160;</i>exp(<i>−²m</i>)<i>.</i><br/>
so&#160;that&#160;choosing&#160;<i>²&#160;</i>=&#160;ln(1<i>/t</i>)<i>/m&#160;</i>ensures<br/>
probability&#160;less&#160;than&#160;<i>t</i>.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
28<br/>
<hr/>
<a name=30></a><b>Finite&#160;or&#160;Countable&#160;function&#160;classes</b><br/>
If&#160;we&#160;now&#160;consider&#160;a&#160;function&#160;class<br/>
F&#160;=&#160;<i>{f</i>1<i>,&#160;f</i>2<i>,&#160;.&#160;.&#160;.&#160;,&#160;fn,&#160;.&#160;.&#160;.}</i><br/>
and&#160;make&#160;the&#160;probability&#160;of&#160;being&#160;misled&#160;by&#160;<i>fn&#160;</i>less<br/>
than&#160;<i>qnδ&#160;</i>with<br/>
<i>∞</i><br/>
X&#160;<i>qn&#160;≤&#160;</i>1<i>,</i><br/>
<i>n</i>=1<br/>
then&#160;the&#160;probability&#160;of&#160;being&#160;misled&#160;by&#160;one&#160;of&#160;the<br/>
functions&#160;is&#160;bounded&#160;by<br/>
½<br/>
µ<br/>
¶¾<br/>
1<br/>
1<br/>
<i>PS&#160;∃fn</i>:&#160;err<i>S</i>(<i>fn</i>)&#160;=&#160;0<i>,&#160;</i>err(<i>fn</i>)&#160;<i>&gt;</i><br/>
ln<br/>
<i>≤&#160;δ.</i><br/>
<i>m</i><br/>
<i>qnδ</i><br/>
This&#160;uses&#160;the&#160;so-called&#160;union&#160;bound&#160;–&#160;the<br/>
probability&#160;of&#160;the&#160;union&#160;of&#160;a&#160;set&#160;of&#160;events&#160;is&#160;at&#160;most<br/>the&#160;sum&#160;of&#160;the&#160;individual&#160;probabilities.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
29<br/>
<hr/>
<a name=31></a><b>Finite&#160;or&#160;Countable&#160;function&#160;classes</b><br/>
<b>result</b><br/>
<i>•&#160;</i>The&#160;bound&#160;translates&#160;into&#160;a&#160;theorem:&#160;given&#160;F<br/>
and&#160;<i>q</i>,&#160;with&#160;probability&#160;at&#160;least&#160;1&#160;<i>−&#160;δ&#160;</i>over&#160;random<br/><i>m&#160;</i>samples&#160;the&#160;generalisation&#160;error&#160;of&#160;a&#160;function<br/><i>fn&#160;∈&#160;</i>F&#160;with&#160;zero&#160;training&#160;error&#160;is&#160;bounded&#160;by<br/>
µ&#160;µ<br/>
¶<br/>
µ&#160;¶¶<br/>
1<br/>
1<br/>
1<br/>
err(<i>fn</i>)&#160;<i>≤</i><br/>
ln<br/>
+&#160;ln<br/>
<i>m</i><br/>
<i>qn</i><br/>
<i>δ</i><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
30<br/>
<hr/>
<a name=32></a><b>Some&#160;comments&#160;on&#160;the&#160;result</b><br/>
³&#160;´<br/>
<i>•&#160;</i>We&#160;can&#160;think&#160;of&#160;the&#160;term&#160;ln&#160;1<br/>
as&#160;the<br/>
<i>qn</i><br/>
complexity&#160;/&#160;description&#160;length&#160;of&#160;the&#160;function&#160;<i>fn</i>.<br/>
<i>•&#160;</i>Note&#160;that&#160;we&#160;must&#160;put&#160;a&#160;prior&#160;weight&#160;on&#160;the<br/>
functions.&#160;If&#160;the&#160;functions&#160;are&#160;drawn&#160;at&#160;random<br/>according&#160;to&#160;a&#160;distribution&#160;<i>pn</i>,&#160;the&#160;expected<br/>generalisation&#160;will&#160;be&#160;minimal&#160;if&#160;we&#160;choose&#160;our<br/>prior&#160;<i>q&#160;</i>=&#160;<i>p</i>.<br/>
<i>•&#160;</i>This&#160;is&#160;the&#160;starting&#160;point&#160;of&#160;the&#160;PAC-Bayes<br/>
analysis.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
31<br/>
<hr/>
<a name=33></a><b>What&#160;if&#160;uncountably&#160;many&#160;functions?</b><br/>
<i>•&#160;</i>We&#160;need&#160;a&#160;way&#160;to&#160;convert&#160;an&#160;infinite&#160;set&#160;to&#160;a&#160;finite<br/>
one.<br/>
<i>•&#160;</i>Key&#160;idea&#160;is&#160;to&#160;replace&#160;measuring&#160;performance&#160;on<br/>
a&#160;random&#160;test&#160;point&#160;with&#160;measuring&#160;on&#160;a&#160;second<br/>‘ghost’&#160;sample<br/>
<i>•&#160;</i>In&#160;this&#160;way&#160;the&#160;analysis&#160;is&#160;reduced&#160;to&#160;a&#160;finite<br/>
set&#160;of&#160;examples&#160;and&#160;hence&#160;a&#160;finite&#160;set&#160;of<br/>classification&#160;functions.<br/>
<i>•&#160;</i>This&#160;step&#160;is&#160;often&#160;referred&#160;to&#160;as&#160;the&#160;‘double<br/>
sample&#160;trick’<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
32<br/>
<hr/>
<a name=34></a><b>Double&#160;sample&#160;trick</b><br/>
The&#160;result&#160;has&#160;the&#160;following&#160;form:<br/>
<i>P&#160;m{</i>X&#160;<i>∈&#160;Xm&#160;</i>:&#160;<i>∃h&#160;∈&#160;H&#160;</i>:&#160;errX(<i>h</i>)&#160;=&#160;0<i>,&#160;</i>err(<i>h</i>)&#160;<i>≥&#160;²}</i><br/>
<i>≤&#160;</i>2<i>P&#160;</i>2<i>m{</i>XY&#160;<i>∈&#160;X</i>2<i>m&#160;</i>:&#160;<i>∃h&#160;∈&#160;H&#160;</i>:<br/>
errX(<i>h</i>)&#160;=&#160;0<i>,&#160;</i>errY(<i>h</i>)&#160;<i>≥&#160;²/</i>2<i>}</i><br/>
If&#160;we&#160;think&#160;of&#160;the&#160;first&#160;probability&#160;as&#160;being&#160;over&#160;XY<br/>
the&#160;result&#160;concerns&#160;three&#160;events:<br/>
<i>A</i>(<i>h</i>)&#160;:=&#160;<i>{</i>errX(<i>h</i>)&#160;=&#160;0<i>}</i><br/>
<i>B</i>(<i>h</i>)&#160;:=&#160;<i>{</i>err(<i>h</i>)&#160;<i>≥&#160;²}</i><br/>
<i>C</i>(<i>h</i>)&#160;:=&#160;<i>{</i>errY(<i>h</i>)&#160;<i>≥&#160;²/</i>2<i>}</i><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
33<br/>
<hr/>
<a name=35></a><b>Double&#160;sample&#160;trick&#160;II</b><br/>
It&#160;is&#160;clear&#160;that<br/>
<i>P&#160;</i>2<i>m</i>(<i>C</i>(<i>h</i>)<i>|A</i>(<i>h</i>)&amp;<i>B</i>(<i>h</i>))&#160;=&#160;<i>P&#160;</i>2<i>m</i>(<i>C</i>(<i>h</i>)<i>|B</i>(<i>h</i>))<br/>
<i>&gt;&#160;</i>0<i>.</i>5<br/>
for&#160;reasonable&#160;<i>m&#160;</i>by&#160;a&#160;binomial&#160;tail&#160;bound.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
34<br/>
<hr/>
<a name=36></a><b>Double&#160;sample&#160;trick&#160;II</b><br/>
Hence,&#160;we&#160;have<br/>
<i>P&#160;</i>2<i>m{</i>XY&#160;<i>∈&#160;X</i>2<i>m&#160;</i>:&#160;<i>∃h&#160;∈&#160;H&#160;</i>:&#160;<i>A</i>(<i>h</i>)&amp;<i>C</i>(<i>h</i>)<i>}&#160;≥</i><br/>
<i>P&#160;</i>2<i>m{</i>XY&#160;<i>∈&#160;X</i>2<i>m&#160;</i>:&#160;<i>∃h&#160;∈&#160;H&#160;</i>:&#160;<i>A</i>(<i>h</i>)&amp;<i>B</i>(<i>h</i>)&amp;<i>C</i>(<i>h</i>)<i>}&#160;</i>=<br/>
<i>P&#160;</i>2<i>m{</i>XY&#160;<i>∈&#160;X</i>2<i>m&#160;</i>:&#160;<i>∃h&#160;∈&#160;H&#160;</i>:&#160;<i>A</i>(<i>h</i>)&amp;<i>B</i>(<i>h</i>)<i>}</i><br/>
<i>P&#160;</i>(<i>C</i>(<i>h</i>)<i>|A</i>(<i>h</i>)&amp;<i>B</i>(<i>h</i>))<br/>
It&#160;follows&#160;that<br/>
<i>P&#160;m{</i>X&#160;<i>∈&#160;Xm&#160;</i>:&#160;<i>∃h&#160;∈&#160;H&#160;</i>:&#160;<i>A</i>(<i>h</i>)&amp;<i>B</i>(<i>h</i>)<i>}&#160;≤</i><br/>
2<i>P&#160;</i>2<i>m{</i>XY&#160;<i>∈&#160;X</i>2<i>m&#160;</i>:&#160;<i>∃h&#160;∈&#160;H&#160;</i>:&#160;<i>A</i>(<i>h</i>)&amp;<i>C</i>(<i>h</i>)<i>}</i><br/>
the&#160;required&#160;result.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
35<br/>
<hr/>
<a name=37></a><b>How&#160;many&#160;functions&#160;on&#160;a&#160;finite</b><br/>
<b>sample?</b><br/>
<i>•&#160;</i>Let&#160;<i>H&#160;</i>be&#160;a&#160;set&#160;of&#160;<i>{−</i>1<i>,&#160;</i>1<i>}&#160;</i>valued&#160;functions.<br/>
<i>•&#160;</i>The&#160;growth&#160;function&#160;<i>BH</i>(<i>m</i>)&#160;is&#160;the&#160;maximum<br/>
cardinality&#160;of&#160;the&#160;set&#160;of&#160;functions&#160;<i>H&#160;</i>when<br/>restricted&#160;to&#160;<i>m&#160;</i>points&#160;–&#160;note&#160;that&#160;this&#160;cannot&#160;be<br/>larger&#160;than&#160;2<i>m</i>,&#160;i.e.&#160;log2(<i>BH</i>(<i>m</i>))&#160;<i>≤&#160;m</i><br/>
<i>•&#160;</i>For&#160;the&#160;statistics&#160;to&#160;work&#160;we&#160;want&#160;the&#160;number&#160;of<br/>
functions&#160;to&#160;be&#160;much&#160;smaller&#160;than&#160;this&#160;as&#160;we&#160;will<br/>perform&#160;a&#160;union&#160;bound&#160;over&#160;this&#160;number.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
36<br/>
<hr/>
<a name=38></a><b>Examining&#160;the&#160;growth&#160;function</b><br/>
Consider&#160;a&#160;plot&#160;of&#160;the&#160;ratio&#160;of&#160;the&#160;growth&#160;function<br/><i>BH</i>(<i>m</i>)&#160;to&#160;2<i>m&#160;</i>for&#160;linear&#160;functions&#160;in&#160;a&#160;20&#160;dimensional<br/>space:<br/>
1<br/>
0.9<br/>
0.8<br/>
0.7<br/>
0.6<br/>
0.5<br/>
0.4<br/>
0.3<br/>
0.2<br/>
0.1<br/>
00<br/>
10<br/>
20<br/>
30<br/>
40<br/>
50<br/>
60<br/>
70<br/>
80<br/>
90<br/>
100<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
37<br/>
<hr/>
<a name=39></a><b>Vapnik&#160;Chervonenkis&#160;dimension</b><br/>
<i>•&#160;</i>The&#160;Vapnik-Chervonenkis&#160;dimension&#160;is&#160;the&#160;point<br/>
at&#160;which&#160;the&#160;ratio&#160;stops&#160;being&#160;equal&#160;to&#160;1:<br/>
VCdim(<i>H</i>)&#160;=&#160;max<i>{m&#160;</i>:&#160;for&#160;some&#160;x1<i>,&#160;.&#160;.&#160;.&#160;,&#160;</i>x<i>m,</i><br/>
for&#160;all&#160;<i>b&#160;∈&#160;{−</i>1<i>,&#160;</i>1<i>}m,</i><br/>
<i>∃hb&#160;∈&#160;H,&#160;hb</i>(x<i>i</i>)&#160;=&#160;<i>bi}</i><br/>
<i>•&#160;</i>For&#160;linear&#160;functions&#160;L&#160;in&#160;R<i>n</i>,&#160;VCdim(L)&#160;=&#160;<i>n&#160;</i>+&#160;1.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
38<br/>
<hr/>
<a name=40></a><b>Sauer’s&#160;Lemma</b><br/>
<i>•&#160;</i>Sauer’s&#160;Lemma&#160;(also&#160;due&#160;to&#160;VC&#160;and&#160;Shelah):<br/>
<i>d</i><br/>
X&#160;µ&#160;¶<br/>
<i>m</i><br/>
³<i>em</i>´<i>d</i><br/>
<i>BH</i>(<i>m</i>)&#160;<i>≤</i><br/>
<i>≤</i><br/>
<i>,</i><br/>
<i>i</i><br/>
<i>d</i><br/>
<i>i</i>=0<br/>
where&#160;<i>m&#160;≥&#160;d&#160;</i>=&#160;VCdim(<i>H</i>).<br/>
<i>•&#160;</i>This&#160;is&#160;surprising&#160;as&#160;the&#160;rate&#160;of&#160;growth&#160;of&#160;<i>BH</i>(<i>m</i>)<br/>
up&#160;to&#160;<i>m&#160;</i>=&#160;<i>d&#160;</i>is&#160;exponential&#160;–&#160;but&#160;thereafter&#160;it&#160;is<br/>polynomial&#160;with&#160;exponent&#160;<i>d</i>.<br/>
<i>•&#160;</i>Furthermore,&#160;there&#160;is&#160;no&#160;assumption&#160;made&#160;about<br/>
the&#160;functions&#160;being&#160;linear&#160;–&#160;this&#160;is&#160;a&#160;completely<br/>general&#160;result.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
39<br/>
<hr/>
<a name=41></a><b>Basic&#160;Theorem&#160;of&#160;SLT</b><br/>
We&#160;want&#160;to&#160;bound&#160;the&#160;probability&#160;that&#160;the&#160;training<br/>examples&#160;can&#160;mislead&#160;us&#160;about&#160;one&#160;of&#160;the&#160;functions<br/>we&#160;are&#160;considering&#160;using:<br/>
<i>P&#160;m{</i>X&#160;<i>∈&#160;Xm&#160;</i>:&#160;<i>∃h&#160;∈&#160;H&#160;</i>:&#160;errX(<i>h</i>)&#160;=&#160;0<i>,&#160;</i>err(<i>h</i>)&#160;<i>≥&#160;²}</i><br/>
<i>→&#160;</i>double&#160;sample&#160;trick&#160;<i>→</i><br/>
<i>≤&#160;</i>2<i>P&#160;</i>2<i>m{</i>XY&#160;<i>∈&#160;X</i>2<i>m&#160;</i>:&#160;<i>∃h&#160;∈&#160;H&#160;</i>:<br/>
errX(<i>h</i>)&#160;=&#160;0<i>,&#160;</i>errY(<i>h</i>)&#160;<i>≥&#160;²/</i>2<i>}</i><br/>
<i>→&#160;</i>union&#160;bound&#160;<i>→</i><br/>
<i>≤&#160;</i>2<i>BH</i>(2<i>m</i>)<i>P&#160;</i>2<i>m{</i>XY&#160;<i>∈&#160;X</i>2<i>m&#160;</i>:<br/>
errX(<i>h</i>)&#160;=&#160;0<i>,&#160;</i>errY(<i>h</i>)&#160;<i>≥&#160;²/</i>2<i>}</i><br/>
Final&#160;ingredient&#160;is&#160;known&#160;as&#160;symmetrisation.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
40<br/>
<hr/>
<a name=42></a><b>Symmetrisation</b><br/>
<i>•&#160;</i>Consider&#160;generating&#160;a&#160;2<i>m&#160;</i>sample&#160;<i>S</i>.<br/>
Since<br/>
the&#160;points&#160;are&#160;generated&#160;independently&#160;the<br/>probability&#160;of&#160;generating&#160;the&#160;same&#160;set&#160;of&#160;points<br/>in&#160;a&#160;different&#160;order&#160;is&#160;the&#160;same.<br/>
<i>•&#160;</i>Consider&#160;a&#160;fixed&#160;set&#160;Σ&#160;of&#160;permutations&#160;and&#160;each<br/>
time&#160;we&#160;generate&#160;a&#160;sample&#160;we&#160;randomly&#160;permute<br/>it&#160;with&#160;a&#160;uniformly&#160;chosen&#160;element&#160;of&#160;Σ&#160;–&#160;gives<br/>probability&#160;distribution&#160;<i>P&#160;</i>2<i>m</i><br/>
Σ<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
41<br/>
<hr/>
<a name=43></a><b>Symmetrisation&#160;cont.</b><br/>
<i>•&#160;</i>Any&#160;event&#160;has&#160;equal&#160;probability&#160;under&#160;<i>P&#160;</i>2<i>m&#160;</i>and<br/>
<i>P&#160;</i>2<i>m</i>,&#160;so&#160;that<br/>
Σ<br/>
<i>P&#160;</i>2<i>m</i>(<i>A</i>)&#160;=&#160;<i>P&#160;</i>2<i>m</i><br/>
Σ&#160;(<i>A</i>)&#160;=&#160;E2<i>m&#160;</i>[<i>Pσ∼</i>Σ(<i>A</i>)]<br/>
<i>•&#160;</i>Consider&#160;particular&#160;choice&#160;of&#160;Σ&#160;the&#160;permutations<br/>
that&#160;swap/leave&#160;unchanged&#160;corresponding&#160;elements<br/>of&#160;the&#160;two&#160;samples&#160;X&#160;and&#160;Y&#160;–&#160;2<i>m&#160;</i>such<br/>permutations.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
42<br/>
<hr/>
<a name=44></a><b>Completion&#160;of&#160;the&#160;proof</b><br/>
<i>P&#160;</i>2<i>m{</i>XY&#160;<i>∈&#160;X</i>2<i>m&#160;</i>:&#160;errX(<i>h</i>)&#160;=&#160;0<i>,&#160;</i>errY(<i>h</i>)&#160;<i>≥&#160;²/</i>2<i>}</i><br/>
<i>≤&#160;</i>E2<i>m&#160;</i>[<i>Pσ∼</i>Σ<i>{</i>errX(<i>h</i>)&#160;=&#160;0<i>,&#160;</i>errY(<i>h</i>)&#160;<i>≥&#160;²/</i>2&#160;for&#160;<i>σ</i>(XY)<i>}</i>]<br/>
h<br/>
i<br/>
<i>≤&#160;</i>E2<i>m&#160;</i>2<i>−²m/</i>2<br/>
=&#160;2<i>−²m/</i>2<br/>
<i>•&#160;</i>Setting&#160;the&#160;right&#160;hand&#160;side&#160;equal&#160;to&#160;<i>δ/</i>(2<i>BH</i>(2<i>m</i>))<br/>
and&#160;inverting&#160;gives&#160;the&#160;bound&#160;on&#160;<i>²</i>.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
43<br/>
<hr/>
<a name=45></a><b>Final&#160;result</b><br/>
<i>•&#160;</i>Assembling&#160;the&#160;ingredients&#160;gives&#160;the&#160;result:&#160;with<br/>
probability&#160;at&#160;least&#160;1<i>−δ&#160;</i>of&#160;random&#160;<i>m&#160;</i>samples&#160;the<br/>generalisation&#160;error&#160;of&#160;a&#160;function&#160;<i>h&#160;∈&#160;H&#160;</i>chosen<br/>from&#160;a&#160;class&#160;<i>H&#160;</i>with&#160;VC&#160;dimension&#160;<i>d&#160;</i>with&#160;zero<br/>training&#160;error&#160;is&#160;bounded&#160;by<br/>
µ<br/>
¶<br/>
2<br/>
2<i>em</i><br/>
2<br/>
<i>²&#160;</i>=&#160;<i>²</i>(<i>m,&#160;H,&#160;δ</i>)&#160;=<br/>
<i>d&#160;</i>log<br/>
+&#160;log<br/>
<i>m</i><br/>
<i>d</i><br/>
<i>δ</i><br/>
<i>•&#160;</i>Note&#160;that&#160;we&#160;can&#160;think&#160;of&#160;<i>d&#160;</i>as&#160;the&#160;complexity&#160;/<br/>
capacity&#160;of&#160;the&#160;function&#160;class&#160;<i>H</i>.<br/>
<i>•&#160;</i>The&#160;bound&#160;does&#160;not&#160;distinguish&#160;between<br/>
functions&#160;in&#160;<i>H</i>.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
44<br/>
<hr/>
<a name=46></a><b>Lower&#160;bounds</b><br/>
<i>•&#160;</i>VCdim&#160;<i>Characterises&#160;</i>Learnability&#160;in&#160;PAC&#160;setting:<br/>
there&#160;exist&#160;distributions&#160;such&#160;that&#160;with&#160;probability<br/>at&#160;least&#160;<i>δ&#160;</i>over&#160;<i>m&#160;</i>random&#160;examples,&#160;the&#160;error&#160;of<br/><i>h&#160;</i>is&#160;at&#160;least<br/>
µ<br/>
µ&#160;¶¶<br/>
<i>d&#160;−&#160;</i>1&#160;1<br/>
1<br/>
max<br/>
<i>,</i><br/>
log<br/>
<i>.</i><br/>
32<i>m&#160;m</i><br/>
<i>δ</i><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
45<br/>
<hr/>
<a name=47></a><b>Non-zero&#160;training&#160;error</b><br/>
<i>•&#160;</i>Very&#160;similar&#160;results&#160;can&#160;be&#160;obtained&#160;for&#160;non-zero<br/>
training&#160;error.<br/>
<i>•&#160;</i>The&#160;main&#160;difference&#160;is&#160;the&#160;introduction&#160;of&#160;a<br/>
square&#160;root&#160;to&#160;give&#160;a&#160;bound&#160;of&#160;the&#160;form<br/>
Ãr<br/>
r<br/>
!<br/>
<i>d</i><br/>
2<i>em</i><br/>
1<br/>
2<br/>
<i>²</i>(<i>m,&#160;H,&#160;k,&#160;δ</i>)&#160;=&#160;<i>k&#160;</i>+&#160;<i>O</i><br/>
log<br/>
+<br/>
log<br/>
<i>m</i><br/>
<i>d</i><br/>
<i>m</i><br/>
<i>δ</i><br/>
for&#160;<i>k&#160;</i>training&#160;errors,&#160;which&#160;is&#160;significantly&#160;worse<br/>than&#160;in&#160;the&#160;zero&#160;training&#160;error&#160;case.<br/>
<i>•&#160;</i>PAC-Bayes&#160;bounds&#160;now&#160;interpolate&#160;between<br/>
these&#160;two.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
46<br/>
<hr/>
<a name=48></a><b>Structural&#160;Risk&#160;Minimisation</b><br/>
<i>•&#160;</i>The&#160;way&#160;to&#160;differentiate&#160;between&#160;functions&#160;using<br/>
the&#160;VC&#160;result&#160;is&#160;to&#160;create&#160;a&#160;hierarchy&#160;of&#160;classes:<br/>
<i>H</i>1&#160;<i>⊆&#160;H</i>2&#160;<i>⊆&#160;·&#160;·&#160;·&#160;⊆&#160;Hd&#160;⊆&#160;·&#160;·&#160;·&#160;⊆&#160;HK.</i><br/>
of&#160;increasing&#160;complexity/VC&#160;dimension.<br/>
<i>•&#160;</i>Can&#160;now&#160;find&#160;the&#160;function&#160;in&#160;each&#160;class&#160;with<br/>
minimum&#160;empirical&#160;error&#160;<i>kd&#160;</i>and&#160;choose&#160;between<br/>the&#160;classes&#160;by&#160;minimising&#160;over&#160;the&#160;choice&#160;of&#160;<i>d</i>:<br/>
Ãr<br/>
r<br/>
!<br/>
<i>d</i><br/>
2<i>em</i><br/>
1<br/>
2<i>K</i><br/>
<i>²</i>(<i>m,&#160;d,&#160;δ</i>)&#160;=&#160;<i>kd&#160;</i>+&#160;<i>O</i><br/>
log<br/>
+<br/>
log<br/>
<i>m</i><br/>
<i>d</i><br/>
<i>m</i><br/>
<i>δ</i><br/>
which&#160;bounds&#160;the&#160;generalisation&#160;by&#160;a&#160;further<br/>application&#160;of&#160;the&#160;union&#160;bound&#160;over&#160;the&#160;<i>K<br/></i>classes.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
47<br/>
<hr/>
<a name=49></a><b>Criticisms&#160;of&#160;PAC&#160;Theory</b><br/>
<i>•&#160;</i>The&#160;theory&#160;is&#160;certainly&#160;valid&#160;and&#160;the&#160;lower<br/>
bounds&#160;indicate&#160;that&#160;it&#160;is&#160;not&#160;too&#160;far&#160;out&#160;–&#160;so&#160;can’t<br/>criticise&#160;as&#160;stands<br/>
<i>•&#160;</i>Criticism&#160;is&#160;that&#160;it&#160;doesn’t&#160;accord&#160;with&#160;experience<br/>
of&#160;those&#160;applying&#160;learning.<br/>
<i>•&#160;</i>Mismatch&#160;between&#160;theory&#160;and&#160;practice.<br/>
<i>•&#160;</i>For&#160;example<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
48<br/>
<hr/>
<a name=50></a><b>Support&#160;Vector&#160;Machines&#160;(SVM)</b><br/>
One&#160;example&#160;of&#160;PAC&#160;failure&#160;is&#160;in&#160;analysing&#160;SVMs:<br/>linear&#160;functions&#160;in&#160;very&#160;high&#160;dimensional&#160;feature<br/>spaces.<br/>
1.&#160;kernel&#160;trick&#160;means&#160;we&#160;can&#160;work&#160;in&#160;an&#160;infinite<br/>
dimensional&#160;feature&#160;space&#160;(<i>⇒&#160;</i>infinite&#160;VC<br/>dimension)&#160;so&#160;that&#160;VC&#160;result&#160;does&#160;not&#160;apply:<br/>
2.&#160;and&#160;YET&#160;very&#160;impressive&#160;performance<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
49<br/>
<hr/>
<a name=51></a><b>Support&#160;Vector&#160;Machines&#160;cont.</b><br/>
<i>•&#160;</i>SVM&#160;seeks&#160;linear&#160;function&#160;in&#160;a&#160;feature&#160;space<br/>
defined&#160;implicitly&#160;via&#160;a&#160;kernel&#160;<i>κ</i>:<br/>
<i>κ</i>(x<i>,&#160;</i>z)&#160;=&#160;<i>hφ</i>(x)<i>,&#160;φ</i>(z)<i>i</i><br/>
<i>•&#160;</i>For&#160;example&#160;the&#160;1-norm&#160;SVM&#160;seeks&#160;w&#160;to&#160;solve<br/>
P<br/>
min<br/>
<i>m</i><br/>
w<i>,b,γ,ξ</i><br/>
<i>k</i>w<i>k</i>2&#160;+&#160;<i>C</i><br/>
<i>ξ</i><br/>
<i>i</i>=1&#160;<i>i</i><br/>
subject&#160;to<br/>
<i>yi&#160;</i>(<i>h</i>w<i>,&#160;φ&#160;</i>(x<i>i</i>)<i>i&#160;</i>+&#160;<i>b</i>)&#160;<i>≥&#160;</i>1&#160;<i>−&#160;ξi</i>,&#160;<i>ξi&#160;≥&#160;</i>0,<br/><i>i&#160;</i>=&#160;1<i>,&#160;.&#160;.&#160;.&#160;,&#160;m</i>.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
50<br/>
<hr/>
<a name=52></a><b>Margin&#160;in&#160;SVMs</b><br/>
<i>•&#160;</i>Intuition&#160;behind&#160;SVMs&#160;is&#160;that&#160;maximising&#160;the<br/>
margin&#160;makes&#160;it&#160;possible&#160;to&#160;obtain&#160;good<br/>generalisation&#160;despite&#160;the&#160;high&#160;VC&#160;dimension<br/>
<i>•&#160;</i>The&#160;lower&#160;bound&#160;implies&#160;that&#160;we&#160;must&#160;be&#160;taking<br/>
advantage&#160;of&#160;a&#160;benign&#160;distribution,&#160;since&#160;we<br/>know&#160;that&#160;in&#160;the&#160;worst&#160;case&#160;generalisation&#160;will&#160;be<br/>bad.<br/>
<i>•&#160;</i>Structural&#160;Risk&#160;Minimisation&#160;cannot&#160;be&#160;applied&#160;to<br/>
a&#160;hierarchy&#160;determined&#160;by&#160;the&#160;margin&#160;of&#160;different<br/>classifiers,&#160;since&#160;the&#160;margin&#160;is&#160;not&#160;known&#160;until&#160;we<br/>see&#160;the&#160;data,&#160;while&#160;the&#160;SRM&#160;hierarchy&#160;must&#160;be<br/>chosen&#160;in&#160;advance.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
51<br/>
<hr/>
<a name=53></a><b>Margin&#160;in&#160;SVMs&#160;cont</b><br/>
<i>•&#160;</i>Hence,&#160;we&#160;require&#160;a&#160;theory&#160;that&#160;can&#160;give&#160;bounds<br/>
that&#160;are&#160;sensitive&#160;to&#160;serendipitous&#160;distributions&#160;–<br/>in&#160;particular&#160;we&#160;conjecture&#160;that&#160;the&#160;margin&#160;is&#160;an<br/>indication&#160;of&#160;such&#160;‘luckiness’.<br/>
<i>•&#160;</i>The&#160;proof&#160;approach&#160;will&#160;rely&#160;on&#160;using&#160;real-<br/>
valued&#160;function&#160;classes.&#160;The&#160;margin&#160;gives&#160;an<br/>indication&#160;of&#160;the&#160;accuracy&#160;with&#160;which&#160;we&#160;need<br/>to&#160;approximate&#160;the&#160;functions&#160;when&#160;applying&#160;the<br/>statistics.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
52<br/>
<hr/>
<a name=54></a><b>Covering&#160;Numbers</b><br/>
F&#160;a&#160;class&#160;of&#160;real&#160;functions&#160;defined&#160;on&#160;<i>X&#160;</i>and&#160;<i>k&#160;·&#160;kd&#160;</i>a<br/>norm&#160;on&#160;F,&#160;then<br/>
N(<i>γ,&#160;</i>F<i>,&#160;k&#160;·&#160;kd</i>)<br/>
is&#160;the&#160;smallest&#160;size&#160;set&#160;<i>Uγ&#160;</i>such&#160;that<br/>
for&#160;any&#160;<i>f&#160;∈&#160;</i>F&#160;there&#160;is&#160;a&#160;<i>u&#160;∈&#160;Uγ&#160;</i>such&#160;that&#160;<i>kf&#160;−&#160;ukd&#160;&lt;<br/>γ</i>.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
53<br/>
<hr/>
<a name=55></a><b>Covering&#160;Numbers&#160;cont.</b><br/>
For&#160;generalization&#160;bounds&#160;we&#160;need&#160;the&#160;<i>γ-growth<br/>function</i>,<br/>
N<i>m</i>(<i>γ,&#160;</i>F)&#160;:=&#160;sup&#160;N(<i>γ,&#160;</i>F<i>,&#160;`</i>X<br/>
<i>∞</i>)<i>.</i><br/>
X<i>∈Xm</i><br/>
where&#160;<i>`</i>X<br/>
<i>∞&#160;</i>gives&#160;the&#160;distance&#160;between&#160;two&#160;functions<br/>
as&#160;the&#160;maximum&#160;difference&#160;between&#160;their&#160;outputs<br/>on&#160;the&#160;sample.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
54<br/>
<hr/>
<a name=56></a><b>Covering&#160;numbers&#160;for&#160;linear&#160;functions</b><br/>
<i>•&#160;</i>Consider&#160;the&#160;set&#160;of&#160;functions:<br/>
(<br/>
1&#160;<i>m</i><br/>
X<br/>
x&#160;<i>7→&#160;h</i>w<i>,&#160;φ</i>(x)<i>i&#160;</i>:&#160;w&#160;=<br/>
<i>z</i><br/>
<i>Z</i><br/>
<i>iφ</i>(x<i>i</i>)<i>,&#160;zi&#160;∈&#160;</i>Z<i>,</i><br/>
<i>i</i>=1<br/>
)<br/>
<i>m</i><br/>
X<br/>
<i>m</i><br/>
X<br/>
8<i>R</i>2<br/>
<i>Z&#160;</i>=<br/>
<i>zi&#160;6</i>=&#160;0<i>,</i><br/>
<i>|zi|&#160;≤&#160;γ</i>2<br/>
<i>i</i>=1<br/>
<i>i</i>=1<br/>
<i>•&#160;</i>This&#160;is&#160;an&#160;explicit&#160;cover&#160;that&#160;approximates&#160;the<br/>
output&#160;of&#160;any&#160;norm&#160;1&#160;linear&#160;function&#160;on&#160;the<br/>sample<br/>
<i>{</i>x1<i>,&#160;.&#160;.&#160;.&#160;,&#160;</i>x<i>m}</i><br/>
to&#160;within&#160;<i>γ/</i>2&#160;on&#160;the&#160;sample.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
55<br/>
<hr/>
<a name=57></a><b>Covering&#160;numbers&#160;for&#160;linear&#160;functions</b><br/>
<i>•&#160;</i>We&#160;convert&#160;the&#160;<i>γ/</i>2&#160;approximation&#160;on&#160;the&#160;sample<br/>
problem&#160;into&#160;a&#160;classification&#160;problem,&#160;which&#160;is<br/>solvable&#160;with&#160;a&#160;margin&#160;of&#160;<i>γ/</i>2.<br/>
<i>•&#160;</i>It&#160;follows&#160;that&#160;if&#160;we&#160;use&#160;the&#160;perceptron&#160;algorithm<br/>
to&#160;find&#160;a&#160;classifier,&#160;we&#160;will&#160;find&#160;a&#160;function<br/>satisfying&#160;the&#160;<i>γ/</i>2&#160;approximation&#160;with&#160;just&#160;8<i>R</i>2<i>/γ</i>2<br/>updates.<br/>
<i>•&#160;</i>This&#160;gives&#160;a&#160;sparse&#160;dual&#160;representation&#160;of&#160;the<br/>
function.&#160;The&#160;covering&#160;is&#160;chosen&#160;as&#160;the&#160;set&#160;of<br/>functions&#160;with&#160;small&#160;sparse&#160;dual&#160;representations.<br/>
<i>•&#160;</i>Gives&#160;a&#160;bound&#160;on&#160;the&#160;size&#160;of&#160;the&#160;cover<br/>
<i>e</i>(2<i>m&#160;</i>+&#160;<i>k&#160;−&#160;</i>1)<br/>
8<i>R</i>2<br/>
log2&#160;N2<i>m</i>(<i>γ/</i>2<i>,&#160;</i>F)&#160;<i>≤&#160;k&#160;</i>log2<br/>
<i>,&#160;</i>where&#160;<i>k&#160;</i>=<br/>
<i>.</i><br/>
<i>k</i><br/>
<i>γ</i>2<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
56<br/>
<hr/>
<a name=58></a><b>Second&#160;statistical&#160;result</b><br/>
<i>•&#160;</i>We&#160;want&#160;to&#160;bound&#160;the&#160;probability&#160;that&#160;the&#160;training<br/>
examples&#160;can&#160;mislead&#160;us&#160;about&#160;one&#160;of&#160;the<br/>functions&#160;with&#160;margin&#160;bigger&#160;than&#160;fixed&#160;<i>γ</i>:<br/>
<i>P&#160;m{</i>X&#160;<i>∈&#160;Xm&#160;</i>:&#160;<i>∃f&#160;∈&#160;</i>F&#160;:&#160;errX(<i>f</i>)&#160;=&#160;0<i>,&#160;m</i>X(<i>f</i>)&#160;<i>≥&#160;γ,&#160;</i>err<i>P&#160;</i>(<i>f</i>)&#160;<i>≥&#160;²}</i><br/>
<i>≤&#160;</i>2<i>P&#160;</i>2<i>m{</i>XY&#160;<i>∈&#160;X</i>2<i>m&#160;</i>:&#160;<i>∃f&#160;∈&#160;</i>F&#160;such&#160;that<br/>
errX(<i>f</i>)&#160;=&#160;0<i>,&#160;m</i>X(<i>f</i>)&#160;<i>≥&#160;γ,&#160;</i>errY(<i>f</i>)&#160;<i>≥&#160;²/</i>2<i>}</i><br/>
<i>≤&#160;</i>2N2<i>m</i>(<i>γ/</i>2<i>,&#160;</i>F)<i>P&#160;</i>2<i>m{</i>XY&#160;<i>∈&#160;X</i>2<i>m&#160;</i>:&#160;for&#160;fixed&#160;<i>f&#160;0</i><br/>
<i>m</i>X(<i>f0</i>)&#160;<i>&gt;&#160;γ/</i>2<i>,&#160;m</i>Y(<i>²m/</i>2)(<i>f0</i>)&#160;<i>&lt;&#160;γ/</i>2<i>}</i><br/>
<i>≤&#160;</i>2N2<i>m</i>(<i>γ/</i>2<i>,&#160;</i>F)2<i>−²m/</i>2&#160;<i>≤&#160;δ</i><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
57<br/>
<hr/>
<a name=59></a><b>Second&#160;statistical&#160;result&#160;cont.</b><br/>
<i>•&#160;</i>inverting&#160;gives<br/>
µ<br/>
¶<br/>
2<br/>
2<br/>
<i>²&#160;</i>=&#160;<i>²</i>(<i>m,&#160;</i>F<i>,&#160;δ,&#160;γ</i>)&#160;=<br/>
log<br/>
<i>m</i><br/>
2&#160;N2<i>m</i>(<i>γ/</i>2<i>,&#160;</i>F)&#160;+&#160;log2&#160;<i>δ</i><br/>
i.e.&#160;with&#160;probability&#160;1&#160;<i>−&#160;δ&#160;</i>over&#160;<i>m&#160;</i>random<br/>
examples&#160;a&#160;margin&#160;<i>γ&#160;</i>hypothesis&#160;has&#160;error&#160;less<br/>than&#160;<i>²</i>.&#160;Must&#160;apply&#160;for&#160;finite&#160;set&#160;of&#160;<i>γ&#160;</i>(‘do&#160;SRM<br/>over&#160;<i>γ</i>’).<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
58<br/>
<hr/>
<a name=60></a><b>Bounding&#160;the&#160;covering&#160;numbers</b><br/>
Have&#160;the&#160;following&#160;correspondences&#160;with&#160;the<br/>standard&#160;VC&#160;case&#160;(easy&#160;slogans):<br/>
Growth&#160;function&#160;–&#160;<i>γ</i>-growth&#160;function<br/>
Vapnik&#160;Chervonenkis&#160;dim&#160;–&#160;Fat&#160;shattering&#160;dim<br/>
Sauer’s&#160;Lemma&#160;–&#160;Alon&#160;<i>et&#160;al.</i><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
59<br/>
<hr/>
<a name=61></a><b>Generalization&#160;of&#160;SVMs</b><br/>
<i>•&#160;</i>Since&#160;SVMs&#160;are&#160;linear&#160;functions,&#160;can&#160;apply&#160;linear<br/>
function&#160;bound.<br/>
<i>•&#160;</i>Hence&#160;for&#160;distribution&#160;with&#160;support&#160;in&#160;ball&#160;of<br/>
radius&#160;<i>R</i>,&#160;(eg&#160;Gaussian&#160;Kernels&#160;<i>R&#160;</i>=&#160;1)&#160;and<br/>margin&#160;<i>γ</i>,&#160;have&#160;bound:<br/>
µ<br/>
¶<br/>
2<br/>
<i>e</i>(2<i>m&#160;</i>+&#160;<i>k&#160;−&#160;</i>1)<br/>
<i>m</i><br/>
<i>²</i>(<i>m,&#160;</i>L<i>,&#160;δ,&#160;γ</i>)&#160;=<br/>
<i>k&#160;</i>log<br/>
+&#160;log<br/>
<i>m</i><br/>
2<br/>
<i>k</i><br/>
2&#160;<i>δ</i><br/>
where&#160;<i>k&#160;</i>=&#160;8<i>R</i>2.<br/>
<i>γ</i>2<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
60<br/>
<hr/>
<a name=62></a><b>Error&#160;distribution:&#160;dataset&#160;size:&#160;205</b><br/>
35<br/>
30<br/>
25<br/>
20<br/>
15<br/>
10<br/>
5<br/>
00<br/>
0.2<br/>
0.4<br/>
0.6<br/>
0.8<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
61<br/>
<hr/>
<a name=63></a><b>Error&#160;distribution:&#160;dataset&#160;size:&#160;137</b><br/>
35<br/>
30<br/>
25<br/>
20<br/>
15<br/>
10<br/>
5<br/>
00<br/>
0.2<br/>
0.4<br/>
0.6<br/>
0.8<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
62<br/>
<hr/>
<a name=64></a><b>Error&#160;distribution:&#160;dataset&#160;size:&#160;68</b><br/>
30<br/>
25<br/>
20<br/>
15<br/>
10<br/>
5<br/>
00<br/>
0.2<br/>
0.4<br/>
0.6<br/>
0.8<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
63<br/>
<hr/>
<a name=65></a><b>Error&#160;distribution:&#160;dataset&#160;size:&#160;34</b><br/>
20<br/>
18<br/>
16<br/>
14<br/>
12<br/>
10<br/>
8<br/>
6<br/>
4<br/>
2<br/>
00<br/>
0.2<br/>
0.4<br/>
0.6<br/>
0.8<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
64<br/>
<hr/>
<a name=66></a><b>Error&#160;distribution:&#160;dataset&#160;size:&#160;27</b><br/>
20<br/>
18<br/>
16<br/>
14<br/>
12<br/>
10<br/>
8<br/>
6<br/>
4<br/>
2<br/>
00<br/>
0.2<br/>
0.4<br/>
0.6<br/>
0.8<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
65<br/>
<hr/>
<a name=67></a><b>Error&#160;distribution:&#160;dataset&#160;size:&#160;20</b><br/>
14<br/>
12<br/>
10<br/>
8<br/>
6<br/>
4<br/>
2<br/>
00<br/>
0.2<br/>
0.4<br/>
0.6<br/>
0.8<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
66<br/>
<hr/>
<a name=68></a><b>Error&#160;distribution:&#160;dataset&#160;size:&#160;14</b><br/>
15<br/>
10<br/>
5<br/>
00<br/>
0.2<br/>
0.4<br/>
0.6<br/>
0.8<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
67<br/>
<hr/>
<a name=69></a><b>Error&#160;distribution:&#160;dataset&#160;size:&#160;7</b><br/>
12<br/>
10<br/>
8<br/>
6<br/>
4<br/>
2<br/>
00<br/>
0.2<br/>
0.4<br/>
0.6<br/>
0.8<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
68<br/>
<hr/>
<a name=70></a><b>Using&#160;a&#160;kernel</b><br/>
<i>•&#160;</i>Can&#160;consider&#160;much&#160;higher&#160;dimensional&#160;spaces<br/>
using&#160;the&#160;kernel&#160;trick<br/>
<i>•&#160;</i>Can&#160;even&#160;work&#160;in&#160;infinite&#160;dimensional&#160;spaces,&#160;eg<br/>
using&#160;the&#160;Gaussian&#160;kernel:<br/>
µ<br/>
¶<br/>
<i>k</i>x&#160;<i>−&#160;</i>z<i>k</i>2<br/>
<i>κ</i>(x<i>,&#160;</i>z)&#160;=&#160;exp&#160;<i>−</i><br/>
2<i>σ</i>2<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
69<br/>
<hr/>
<a name=71></a><b>Error&#160;distribution:&#160;dataset&#160;size:&#160;342</b><br/>
40<br/>
35<br/>
30<br/>
25<br/>
20<br/>
15<br/>
10<br/>
5<br/>
00<br/>
0.2<br/>
0.4<br/>
0.6<br/>
0.8<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
70<br/>
<hr/>
<a name=72></a><b>Error&#160;distribution:&#160;dataset&#160;size:&#160;273</b><br/>
40<br/>
35<br/>
30<br/>
25<br/>
20<br/>
15<br/>
10<br/>
5<br/>
00<br/>
0.2<br/>
0.4<br/>
0.6<br/>
0.8<br/>
1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
71<br/>
<hr/>
<a name=73></a><b>Conclusions</b><br/>
<i>•&#160;</i>Outline&#160;of&#160;philosophy&#160;and&#160;approach&#160;of&#160;SLT<br/>
<i>•&#160;</i>Central&#160;result&#160;of&#160;SLT<br/>
<i>•&#160;</i>Touched&#160;on&#160;covering&#160;number&#160;bounds&#160;for&#160;margin<br/>
based&#160;analysis<br/>
<i>•&#160;</i>Application&#160;to&#160;analyse&#160;SVM&#160;learning<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
72<br/>
<hr/>
<a name=74></a><b>STRUCTURE</b><br/>
PART&#160;A<br/>
1.&#160;General&#160;Statistical&#160;Considerations<br/>2.&#160;Basic&#160;PAC&#160;Ideas&#160;and&#160;proofs<br/>3.&#160;Real-valued&#160;Function&#160;Classes&#160;and&#160;the&#160;Margin<br/>
PART&#160;B<br/>
1.&#160;Rademacher&#160;complexity&#160;and&#160;Main&#160;Theory<br/>2.&#160;Applications&#160;to&#160;classification<br/>3.&#160;Conclusions<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
73<br/>
<hr/>
<a name=75></a><b>PART&#160;B</b><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
74<br/>
<hr/>
<a name=76></a><b>Concentration&#160;inequalities</b><br/>
<i>•&#160;</i>Statistical&#160;Learning&#160;is&#160;concerned&#160;with&#160;the<br/>
reliability&#160;or&#160;stability&#160;of&#160;inferences&#160;made&#160;from&#160;a<br/>random&#160;sample.<br/>
<i>•&#160;</i>Random&#160;variables&#160;with&#160;this&#160;property&#160;have&#160;been<br/>
a&#160;subject&#160;of&#160;ongoing&#160;interest&#160;to&#160;probabilists&#160;and<br/>statisticians.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
75<br/>
<hr/>
<a name=77></a><b>Concentration&#160;inequalities&#160;cont.</b><br/>
<i>•&#160;</i>As&#160;an&#160;example&#160;consider&#160;the&#160;mean&#160;of&#160;a&#160;sample&#160;of<br/>
<i>m&#160;</i>1-dimensional&#160;random&#160;variables&#160;<i>X</i>1<i>,&#160;.&#160;.&#160;.&#160;,&#160;Xm</i>:<br/>
1&#160;<i>m</i><br/>
X<br/>
<i>Sm&#160;</i>=<br/>
<i>X</i><br/>
<i>m</i><br/>
<i>i.</i><br/>
<i>i</i>=1<br/>
<i>•&#160;</i>Hoeffding’s&#160;inequality&#160;states&#160;that&#160;if&#160;<i>Xi&#160;∈&#160;</i>[<i>ai,&#160;bi</i>]<br/>
µ<br/>
¶<br/>
2<i>m</i>2<i>²</i>2<br/>
<i>P&#160;{|Sm&#160;−&#160;</i>E[<i>Sm</i>]<i>|&#160;≥&#160;²}&#160;≤&#160;</i>2&#160;exp&#160;<i>−</i>P<i>m&#160;</i>(<i>b</i><br/>
<i>i</i>=1<br/>
<i>i&#160;−&#160;ai</i>)2<br/>
Note&#160;how&#160;the&#160;probability&#160;falls&#160;off&#160;exponentially<br/>with&#160;the&#160;distance&#160;from&#160;the&#160;mean&#160;and&#160;with&#160;the<br/>number&#160;of&#160;variables.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
76<br/>
<hr/>
<a name=78></a><b>Concentration&#160;for&#160;SLT</b><br/>
<i>•&#160;</i>We&#160;are&#160;now&#160;going&#160;to&#160;look&#160;at&#160;deriving&#160;SLT&#160;results<br/>
from&#160;concentration&#160;inequalities.<br/>
<i>•&#160;</i>Perhaps&#160;the&#160;best&#160;known&#160;form&#160;is&#160;due&#160;to<br/>
McDiarmid&#160;(although&#160;he&#160;was&#160;actually&#160;re-<br/>presenting&#160;previously&#160;derived&#160;results):<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
77<br/>
<hr/>
<a name=79></a><b>McDiarmid’s&#160;inequality</b><br/>
<b>Theorem&#160;1.&#160;</b><i>Let&#160;X</i>1<i>,&#160;.&#160;.&#160;.&#160;,&#160;Xn&#160;be&#160;independent&#160;random<br/>variables&#160;taking&#160;values&#160;in&#160;a&#160;set&#160;A,&#160;and&#160;assume&#160;that<br/>f&#160;</i>:&#160;<i>An&#160;→&#160;</i>R&#160;<i>satisfies</i><br/>
sup<br/>
<i>|f&#160;</i>(<i>x</i>1<i>,&#160;.&#160;.&#160;.&#160;,&#160;xn</i>)&#160;<i>−&#160;f</i>(<i>x</i>1<i>,&#160;.&#160;.&#160;.&#160;,&#160;</i>ˆ<i>xi,&#160;xi</i>+1<i>,&#160;.&#160;.&#160;.&#160;,&#160;xn</i>)<i>|&#160;≤&#160;ci,</i><br/>
<i>x</i>1<i>,...,xn,</i>ˆ<br/>
<i>xi∈A</i><br/>
<i>for&#160;</i>1&#160;<i>≤&#160;i&#160;≤&#160;n.&#160;Then&#160;for&#160;all&#160;²&#160;&gt;&#160;</i>0<i>,</i><br/>
µ<br/>
¶<br/>
<i>−</i>2<i>²</i>2<br/>
<i>P&#160;{f&#160;</i>(<i>X</i>1<i>,&#160;.&#160;.&#160;.&#160;,&#160;Xn</i>)&#160;<i>−&#160;</i>E<i>f&#160;</i>(<i>X</i>1<i>,&#160;.&#160;.&#160;.&#160;,&#160;Xn</i>)&#160;<i>≥&#160;²}&#160;≤&#160;</i>exp&#160;P<i>n&#160;c</i>2<br/>
<i>i</i>=1&#160;<i>i</i><br/>
<i>•&#160;</i>Hoeffding&#160;is&#160;a&#160;special&#160;case&#160;when&#160;<i>f&#160;</i>(<i>x</i>1<i>,&#160;.&#160;.&#160;.&#160;,&#160;xn</i>)&#160;=<br/>
<i>Sn</i><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
78<br/>
<hr/>
<a name=80></a><b>Using&#160;McDiarmid</b><br/>
<i>•&#160;</i>By&#160;setting&#160;the&#160;right&#160;hand&#160;side&#160;equal&#160;to&#160;<i>δ</i>,&#160;we&#160;can<br/>
always&#160;invert&#160;McDiarmid&#160;to&#160;get&#160;a&#160;high&#160;confidence<br/>bound:&#160;with&#160;probability&#160;at&#160;least&#160;1&#160;<i>−&#160;δ</i><br/>
rP<i>n&#160;c</i>2&#160;1<br/>
<i>f&#160;</i>(<i>X</i><br/>
<i>i</i>=1&#160;<i>i</i><br/>
1<i>,&#160;.&#160;.&#160;.&#160;,&#160;Xn</i>)&#160;<i>&lt;&#160;</i>E<i>f&#160;</i>(<i>X</i>1<i>,&#160;.&#160;.&#160;.&#160;,&#160;Xn</i>)&#160;+<br/>
log<br/>
2<br/>
<i>δ</i><br/>
<i>•&#160;</i>If&#160;<i>ci&#160;</i>=&#160;<i>c/n&#160;</i>for&#160;each&#160;<i>i&#160;</i>this&#160;reduces&#160;to<br/>
r&#160;<i>c</i>2&#160;1<br/>
<i>f&#160;</i>(<i>X</i>1<i>,&#160;.&#160;.&#160;.&#160;,&#160;Xn</i>)&#160;<i>&lt;&#160;</i>E<i>f&#160;</i>(<i>X</i>1<i>,&#160;.&#160;.&#160;.&#160;,&#160;Xn</i>)&#160;+<br/>
log<br/>
2<i>n</i><br/>
<i>δ</i><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
79<br/>
<hr/>
<a name=81></a><b>Rademacher&#160;complexity</b><br/>
<i>•&#160;</i>Rademacher&#160;complexity&#160;is&#160;a&#160;new&#160;way&#160;of<br/>
measuring&#160;the&#160;complexity&#160;of&#160;a&#160;function&#160;class.&#160;It<br/>arises&#160;naturally&#160;if&#160;we&#160;rerun&#160;the&#160;proof&#160;using&#160;the<br/>double&#160;sample&#160;trick&#160;and&#160;symmetrisation&#160;but&#160;look<br/>at&#160;what&#160;is&#160;actually&#160;needed&#160;to&#160;continue&#160;the&#160;proof:<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
80<br/>
<hr/>
<a name=82></a><b>Rademacher&#160;proof&#160;beginnings</b><br/>
For&#160;a&#160;fixed&#160;<i>f&#160;∈&#160;</i>F&#160;we&#160;have<br/>
³<br/>
´<br/>
E&#160;[<i>f&#160;</i>(z)]&#160;<i>≤&#160;</i>ˆ<br/>
E&#160;[<i>f&#160;</i>(z)]&#160;+&#160;sup&#160;E[<i>h</i>]&#160;<i>−&#160;</i>ˆ<br/>
E[<i>h</i>]&#160;<i>.</i><br/>
<i>h∈</i>F<br/>
where&#160;F&#160;is&#160;a&#160;class&#160;of&#160;functions&#160;mapping&#160;from&#160;<i>Z&#160;</i>to<br/>[0<i>,&#160;</i>1]&#160;and&#160;ˆ<br/>
E&#160;denotes&#160;the&#160;sample&#160;average.<br/>
We&#160;must&#160;bound&#160;the&#160;size&#160;of&#160;the&#160;second&#160;term.&#160;First<br/>apply&#160;McDiarmid’s&#160;inequality&#160;to&#160;obtain&#160;(<i>ci&#160;</i>=&#160;1<i>/m&#160;</i>for<br/>all&#160;<i>i</i>)&#160;with&#160;probability&#160;at&#160;least&#160;1&#160;<i>−&#160;δ</i>:<br/>
³<br/>
´<br/>
·<br/>
³<br/>
´¸<br/>
rln(1<i>/δ</i>)<br/>
sup&#160;E[<i>h</i>]&#160;<i>−&#160;</i>ˆ<br/>
E[<i>h</i>]&#160;<i>≤&#160;</i>E<i>S&#160;</i>sup&#160;E[<i>h</i>]&#160;<i>−&#160;</i>ˆ<br/>
E[<i>h</i>]<br/>
+<br/>
<i>.</i><br/>
<i>h∈</i>F<br/>
<i>h∈</i>F<br/>
2<i>m</i><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
81<br/>
<hr/>
<a name=83></a><b>Deriving&#160;double&#160;sample&#160;result</b><br/>
<i>•&#160;</i>We&#160;can&#160;now&#160;move&#160;to&#160;the&#160;ghost&#160;sample&#160;by&#160;simply<br/>
h<br/>
i<br/>
observing&#160;that&#160;E[<i>h</i>]&#160;=&#160;E<br/>
ˆ<br/>
˜&#160;E[<i>h</i>]&#160;:<br/>
<i>S</i><br/>
·<br/>
³<br/>
´¸<br/>
E<i>S&#160;</i>sup&#160;E[<i>h</i>]&#160;<i>−&#160;</i>ˆ<br/>
E[<i>h</i>]<br/>
=<br/>
<i>h∈</i>F<br/>
&#34;<br/>
&#34;<br/>
¯&#160;##<br/>
1&#160;<i>m</i><br/>
X<br/>
1&#160;<i>m</i><br/>
X<br/>
¯<br/>¯<br/>
E<i>S&#160;</i>sup&#160;E&#160;˜<br/>
<i>h</i>(˜<br/>
z<br/>
<i>h</i>(z<br/>
<i>S</i><br/>
<i>S</i><br/>
<i>i</i>)&#160;<i>−</i><br/>
<i>i</i>)¯<br/>
<i>h∈</i>F<br/>
<i>m</i><br/>
<i>m</i><br/>
¯<br/>
<i>i</i>=1<br/>
<i>i</i>=1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
82<br/>
<hr/>
<a name=84></a><b>Deriving&#160;double&#160;sample&#160;result&#160;cont.</b><br/>
Since&#160;the&#160;sup&#160;of&#160;an&#160;expectation&#160;is&#160;less&#160;than&#160;or<br/>equal&#160;to&#160;the&#160;expectation&#160;of&#160;the&#160;sup&#160;(we&#160;can&#160;make<br/>the&#160;choice&#160;to&#160;optimise&#160;for&#160;each&#160;˜<br/>
<i>S</i>)&#160;we&#160;have<br/>
·<br/>
³<br/>
´¸<br/>
E<i>S&#160;</i>sup&#160;E[<i>h</i>]&#160;<i>−&#160;</i>ˆ<br/>
E[<i>h</i>]<br/>
<i>≤</i><br/>
<i>h∈</i>F<br/>
&#34;<br/>
#<br/>
1&#160;<i>m</i><br/>
X<br/>
E<i>S</i>E&#160;˜&#160;sup<br/>
(<i>h</i>(˜<br/>
z<br/>
<i>S</i><br/>
<i>i</i>)&#160;<i>−&#160;h</i>(z<i>i</i>))<br/>
<i>h∈</i>F&#160;<i>m&#160;i</i>=1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
83<br/>
<hr/>
<a name=85></a><b>Adding&#160;symmetrisation</b><br/>
Here&#160;symmetrisation&#160;is&#160;again&#160;just&#160;swapping<br/>corresponding&#160;elements&#160;–&#160;but&#160;we&#160;can&#160;write&#160;this&#160;as<br/>multiplication&#160;by&#160;a&#160;variable&#160;<i>σi&#160;</i>which&#160;takes&#160;values&#160;<i>±</i>1<br/>with&#160;equal&#160;probability:<br/>
·<br/>
³<br/>
´¸<br/>
E<i>S&#160;</i>sup&#160;E[<i>h</i>]&#160;<i>−&#160;</i>ˆ<br/>
E[<i>h</i>]<br/>
<i>≤</i><br/>
<i>h∈</i>F<br/>
&#34;<br/>
#<br/>
1&#160;<i>m</i><br/>
X<br/>
<i>≤&#160;</i>E<br/>
sup<br/>
<i>σ</i><br/>
<i>σS&#160;</i>˜<br/>
<i>S</i><br/>
<i>i&#160;</i>(<i>h</i>(˜<br/>
z<i>i</i>)&#160;<i>−&#160;h</i>(z<i>i</i>))<br/>
<i>h∈</i>F&#160;<i>m&#160;i</i>=1<br/>
&#34;<br/>
¯<br/>
¯#<br/>
¯<br/>
<i>m</i><br/>
¯<br/>
¯&#160;1&#160;X<br/>
¯<br/>
<i>≤&#160;</i>2E<i>Sσ&#160;</i>sup&#160;¯<br/>
<i>σih</i>(z<i>i</i>)¯<br/>
<i>h∈</i>F&#160;¯<i>m</i><br/>
¯<br/>
<i>i</i>=1<br/>
=&#160;<i>Rm&#160;</i>(F)&#160;<i>,</i><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
84<br/>
<hr/>
<a name=86></a><b>Rademacher&#160;complexity</b><br/>
where<br/>
&#34;<br/>
¯<br/>
¯#<br/>
¯<br/>
<i>m</i><br/>
¯<br/>
¯&#160;2&#160;X<br/>
¯<br/>
<i>Rm</i>(F)&#160;=&#160;E<i>Sσ&#160;</i>sup&#160;¯<br/>
<i>σif&#160;</i>(z<i>i</i>)¯&#160;<i>.</i><br/>
<i>f&#160;∈</i>F&#160;¯<i>m</i><br/>
¯<br/>
<i>i</i>=1<br/>
is&#160;known&#160;as&#160;the&#160;Rademacher&#160;complexity&#160;of&#160;the<br/>
function&#160;class&#160;F.<br/>
<i>•&#160;</i>Rademacher&#160;complexity&#160;is&#160;the&#160;expected&#160;value&#160;of<br/>
the&#160;maximal&#160;correlation&#160;with&#160;random&#160;noise&#160;–&#160;a<br/>very&#160;natural&#160;measure&#160;of&#160;capacity.<br/>
<i>•&#160;</i>Note&#160;that&#160;the&#160;Rademacher&#160;complexity&#160;is&#160;distribution<br/>
dependent&#160;since&#160;it&#160;involves&#160;an&#160;expectation&#160;over<br/>the&#160;choice&#160;of&#160;sample&#160;–&#160;this&#160;might&#160;seem&#160;hard&#160;to<br/>compute.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
85<br/>
<hr/>
<a name=87></a><b>Main&#160;Rademacher&#160;theorem</b><br/>
Putting&#160;the&#160;pieces&#160;together&#160;gives&#160;the&#160;main&#160;theorem<br/>of&#160;Rademacher&#160;complexity:&#160;with&#160;probability&#160;at&#160;least<br/>1&#160;<i>−&#160;δ&#160;</i>over&#160;random&#160;samples&#160;<i>S&#160;</i>of&#160;size&#160;<i>m</i>,&#160;every&#160;<i>f&#160;∈&#160;</i>F<br/>satisfies<br/>
rln(1<i>/δ</i>)<br/>
E&#160;[<i>f&#160;</i>(z)]&#160;<i>≤&#160;</i>ˆ<br/>
E&#160;[<i>f&#160;</i>(z)]&#160;+&#160;<i>Rm</i>(F)&#160;+<br/>
2<i>m</i><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
86<br/>
<hr/>
<a name=88></a><b>Empirical&#160;Rademacher&#160;theorem</b><br/>
<i>•&#160;</i>Since&#160;the&#160;empirical&#160;Rademacher&#160;complexity<br/>
&#34;<br/>
¯<br/>
¯¯<br/>
#<br/>
¯<br/>
<i>m</i><br/>
X<br/>
¯¯<br/>
ˆ<br/>
¯&#160;2<br/>
¯¯<br/>
<i>Rm</i>(F)&#160;=&#160;E<i>σ&#160;</i>sup&#160;¯<br/>
<i>σif&#160;</i>(z<i>i</i>)¯¯&#160;z1<i>,&#160;.&#160;.&#160;.&#160;,&#160;</i>z<i>m</i><br/>
<i>f&#160;∈</i>F&#160;¯<i>m</i><br/>
¯¯<br/>
<i>i</i>=1<br/>
is&#160;concentrated,<br/>
we&#160;can&#160;make&#160;a&#160;further<br/>
application&#160;of&#160;McDiarmid&#160;to&#160;obtain&#160;with&#160;probability<br/>at&#160;least&#160;1&#160;<i>−&#160;δ</i><br/>
rln(2<i>/δ</i>)<br/>
ED&#160;[<i>f</i>(z)]&#160;<i>≤&#160;</i>ˆ<br/>
E&#160;[<i>f&#160;</i>(z)]&#160;+&#160;ˆ<br/>
<i>Rm</i>(F)&#160;+&#160;3<br/>
<i>.</i><br/>
2<i>m</i><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
87<br/>
<hr/>
<a name=89></a><b>Relation&#160;to&#160;VC&#160;theorem</b><br/>
<i>•&#160;</i>For&#160;<i>H&#160;</i>a&#160;class&#160;of&#160;<i>±</i>1&#160;valued&#160;functions&#160;with&#160;VC<br/>
dimension&#160;<i>d</i>,&#160;we&#160;can&#160;upper&#160;bound&#160;ˆ<br/>
<i>Rm</i>(<i>H</i>)&#160;using<br/>
Hoeffding’s&#160;inequality&#160;to&#160;upper&#160;bound<br/>
(¯<br/>
¯<br/>
)<br/>
¯&#160;<i>m</i><br/>
¯<br/>
µ<br/>
¶<br/>
¯X<br/>
¯<br/>
<i>²</i>2<br/>
<i>P</i><br/>
¯<br/>
<i>σ</i><br/>
¯&#160;<i>≥&#160;²</i><br/>
<i>≤&#160;</i>2&#160;exp&#160;<i>−</i><br/>
¯<br/>
<i>if&#160;</i>(x<i>i</i>)¯<br/>
2<i>m</i><br/>
<i>i</i>=1<br/>
for&#160;a&#160;fixed&#160;function&#160;<i>f&#160;</i>,&#160;since&#160;the&#160;expected&#160;value<br/>of&#160;the&#160;sum&#160;is&#160;0,&#160;and&#160;the&#160;maximum&#160;change&#160;by<br/>replacing&#160;a&#160;<i>σi&#160;</i>is&#160;2.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
88<br/>
<hr/>
<a name=90></a><b>Relation&#160;to&#160;VC&#160;theorem&#160;cont.</b><br/>
<i>•&#160;</i>By&#160;Sauer’s&#160;lemma&#160;there&#160;are&#160;at&#160;most&#160;(<i>em/d</i>)<i>d</i><br/>
we&#160;can&#160;bound&#160;the&#160;probability&#160;that&#160;the&#160;sum&#160;is<br/>bounded&#160;by&#160;<i>²&#160;</i>for&#160;all&#160;functions&#160;by<br/>
³<br/>
µ<br/>
¶<br/>
<i>em</i>´<i>d</i><br/>
<i>²</i>2<br/>
2&#160;exp&#160;<i>−</i><br/>
=:&#160;<i>δ.</i><br/>
<i>d</i><br/>
2<i>m</i><br/>
<i>•&#160;</i>Taking&#160;<i>δ&#160;</i>=&#160;<i>m−</i>1&#160;and&#160;solving&#160;for&#160;<i>²&#160;</i>gives<br/>
r<br/>
<i>em</i><br/>
<i>²&#160;</i>=<br/>
2<i>md&#160;</i>ln<br/>
+&#160;2<i>m&#160;</i>ln(2<i>m</i>)<br/>
<i>d</i><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
89<br/>
<hr/>
<a name=91></a><b>Rademacher&#160;bound&#160;for&#160;VC&#160;class</b><br/>
<i>•&#160;</i>Hence&#160;we&#160;can&#160;bound<br/>
ˆ<br/>
2<br/>
<i>Rm</i>(<i>H</i>)&#160;<i>≤</i><br/>
(<i>δm&#160;</i>+&#160;<i>²</i>(1&#160;<i>−&#160;δ</i>))<br/>
<i>m</i><br/>
r<br/>
2<br/>
8(<i>d&#160;</i>ln(<i>em/d</i>)&#160;+&#160;ln(2<i>m</i>))<br/>
<i>≤</i><br/>
+<br/>
<i>m</i><br/>
<i>m</i><br/>
<i>•&#160;</i>This&#160;is&#160;equivalent&#160;to&#160;the&#160;PAC&#160;bound&#160;with&#160;non-<br/>
zero&#160;loss,&#160;except&#160;that&#160;we&#160;could&#160;have&#160;used&#160;the<br/>growth&#160;function&#160;or&#160;VC&#160;dimension&#160;measured&#160;on<br/>the&#160;sample&#160;rather&#160;than&#160;the&#160;sup&#160;over&#160;the&#160;whole<br/>input&#160;space.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
90<br/>
<hr/>
<a name=92></a><b>Application&#160;to&#160;large&#160;margin</b><br/>
<b>classification</b><br/>
<i>•&#160;</i>Rademacher&#160;complexity&#160;comes&#160;into&#160;its&#160;own&#160;for<br/>
Boosting&#160;and&#160;SVMs.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
91<br/>
<hr/>
<a name=93></a><b>Application&#160;to&#160;Boosting</b><br/>
<i>•&#160;</i>We&#160;can&#160;view&#160;Boosting&#160;as&#160;seeking&#160;a&#160;function&#160;from<br/>
the&#160;class&#160;(<i>H&#160;</i>is&#160;the&#160;set&#160;of&#160;weak&#160;learners)<br/>
(<br/>
)<br/>
X<br/>
X<br/>
<i>ahh</i>(x)&#160;:<br/>
<i>ah&#160;≤&#160;B&#160;</i>=&#160;conv<i>B</i>(<i>H</i>)<br/>
<i>h∈H</i><br/>
<i>h∈H</i><br/>
by&#160;minimising&#160;some&#160;function&#160;of&#160;the&#160;margin<br/>
distribution.<br/>
<i>•&#160;</i>Adaboost&#160;corresponds&#160;to&#160;optimising&#160;an&#160;exponential<br/>
function&#160;of&#160;the&#160;margin&#160;over&#160;this&#160;set&#160;of&#160;functions.<br/>
<i>•&#160;</i>We&#160;will&#160;see&#160;how&#160;to&#160;include&#160;the&#160;margin&#160;in&#160;the<br/>
analysis&#160;later,&#160;but&#160;concentrate&#160;on&#160;computing&#160;the<br/>Rademacher&#160;complexity&#160;for&#160;now.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
92<br/>
<hr/>
<a name=94></a><b>Rademacher&#160;complexity&#160;of&#160;convex</b><br/>
<b>hulls</b><br/>
Rademacher&#160;complexity&#160;has&#160;a&#160;very&#160;nice&#160;property<br/>for&#160;convex&#160;hull&#160;classes:<br/>
<br/>
¯<br/>
¯<br/>
¯<br/>
¯<br/>
¯&#160;<i>m</i><br/>
X<br/>
X<br/>
¯<br/>
ˆ<br/>
2<br/>
<i>R</i><br/>
<br/>
¯<br/>
¯<br/>
<i>m</i>(conv<i>B</i>(<i>H&#160;</i>))<br/>
=<br/>
E<br/>
sup<br/>
<i>σ</i><br/>
<i>a</i><br/>
<i>m&#160;σ</i><br/>
P<br/>
¯<br/>
<i>i</i><br/>
<i>jhj</i>(x<i>i</i>)¯<br/>
<i>hj∈H,&#160;j&#160;aj≤B&#160;</i>¯<i>i</i>=1<br/>
<i>j</i><br/>
¯<br/>
<br/>
¯<br/>
¯<br/>
2<br/>
X<br/>
¯&#160;<i>m</i><br/>
¯<br/>
¯X<br/>
¯<br/>
<i>≤</i><br/>
E&#160;<br/>
sup<br/>
<i>a&#160;</i>¯<br/>
<i>σ</i><br/>
¯<br/>
<i>m&#160;σ</i><br/>
P<br/>
<i>j</i><br/>
<i>ihj</i>(x<i>i</i>)<br/>
<i>h</i><br/>
¯<br/>
¯<br/>
<i>j∈H,</i><br/>
<i>j&#160;aj≤B</i><br/>
<i>j</i><br/>
<i>i</i>=1<br/>
&#34;<br/>
¯<br/>
¯#<br/>
2<br/>
¯&#160;<i>m</i><br/>
¯<br/>
¯X<br/>
¯<br/>
<i>≤</i><br/>
E<br/>
sup&#160;<i>B&#160;</i>¯<br/>
<i>σ</i><br/>
¯<br/>
<i>m&#160;σ</i><br/>
<i>ihj</i>(x<i>i</i>)<br/>
<i>h</i><br/>
¯<br/>
¯<br/>
<i>j∈H</i><br/>
<i>i</i>=1<br/>
<i>≤&#160;B&#160;</i>ˆ<br/>
<i>Rm</i>(<i>H</i>)<i>.</i><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
93<br/>
<hr/>
<a name=95></a><b>Rademacher&#160;complexity&#160;of&#160;convex</b><br/>
<b>hulls&#160;cont.</b><br/>
<i>•&#160;</i>Hence,&#160;we&#160;can&#160;move&#160;to&#160;the&#160;convex&#160;hull&#160;without<br/>
incurring&#160;any&#160;complexity&#160;penalty&#160;for&#160;<i>B&#160;</i>=&#160;1!<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
94<br/>
<hr/>
<a name=96></a><b>Rademacher&#160;complexity&#160;for&#160;SVMs</b><br/>
<i>•&#160;</i>The&#160;Rademacher&#160;complexity&#160;of&#160;a&#160;class&#160;of&#160;linear<br/>
functions&#160;with&#160;bounded&#160;2-norm:<br/>(<br/>
)<br/>
<i>m</i><br/>
X<br/>
x&#160;<i>→</i><br/>
<i>αiκ</i>(x<i>i,&#160;</i>x):&#160;<i>α0</i>K<i>α&#160;≤&#160;B</i>2&#160;<i>⊆</i><br/>
<i>i</i>=1<br/>
<i>⊆&#160;{</i>x&#160;<i>→&#160;h</i>w<i>,&#160;φ&#160;</i>(x)<i>i&#160;</i>:&#160;<i>k</i>w<i>k&#160;≤&#160;B}</i><br/>
=&#160;F<i>B,</i><br/>
where&#160;we&#160;assume&#160;a&#160;kernel&#160;defined&#160;feature<br/>
space&#160;with<br/>
<i>hφ</i>(x)<i>,&#160;φ</i>(z)<i>i&#160;</i>=&#160;<i>κ</i>(x<i>,&#160;</i>z)<i>.</i><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
95<br/>
<hr/>
<a name=97></a><b>Rademacher&#160;complexity&#160;of&#160;</b>F<i>B</i><br/>
&#34;<br/>
¯<br/>
¯#<br/>
¯<br/>
<i>m</i><br/>
X<br/>
¯<br/>
ˆ<br/>
¯&#160;2<br/>
¯<br/>
<i>Rm</i>(F<i>B</i>)&#160;=&#160;E<i>σ&#160;</i>sup&#160;¯<br/>
<i>σif&#160;</i>(x<i>i</i>)¯<br/>
<i>f&#160;∈</i>F&#160;¯<i>m</i><br/>
¯<br/>
<i>B</i><br/>
<i>i</i>=1<br/>
&#34;<br/>
¯*<br/>
+¯#<br/>
¯<br/>
<i>m</i><br/>
¯<br/>
¯<br/>
2&#160;X<br/>
¯<br/>
=&#160;E<i>σ</i><br/>
sup&#160;¯&#160;w<i>,</i><br/>
<i>σiφ&#160;</i>(x<i>i</i>)&#160;¯<br/>
<i>k</i>w<i>k≤B&#160;</i>¯<br/>
<i>m</i><br/>
¯<br/>
<i>i</i>=1<br/>
&#34;°<br/>
°#<br/>
2<i>B</i><br/>
°&#160;<i>m</i><br/>
°<br/>
°X<br/>
°<br/>
<i>≤</i><br/>
E<br/>
°<br/>
<i>σ</i><br/>
°<br/>
<i>m&#160;σ&#160;</i>°<br/>
<i>iφ</i>(x<i>i</i>)°<br/>
<i>i</i>=1<br/>
<br/>
<br/>
*<br/>
+1<i>/</i>2<br/>
2<i>B</i><br/>
<i>m</i><br/>
<i>m</i><br/>
<br/>
X<br/>
X<br/>
<br/>
=<br/>
E&#160;<br/>
<i>σ</i><br/>
<i>σ</i><br/>
<br/>
<br/>
<i>m&#160;σ</i><br/>
<i>iφ</i>(x<i>i</i>)<i>,</i><br/>
<i>jφ</i>(x<i>j</i>)<br/>
<i>i</i>=1<br/>
<i>j</i>=1<br/>
<br/>
<br/>
1<i>/</i>2<br/>
v<br/>
u<br/>
2<i>B</i><br/>
<i>m</i><br/>
X<br/>
2<i>B&#160;</i>u&#160;<i>m</i><br/>
X<br/>
<i>≤</i><br/>
E&#160;<br/>
<i>σ</i><br/>
<br/>
=<br/>
t<br/>
<i>κ</i>(x<br/>
<i>m</i><br/>
<i>σ</i><br/>
<i>iσjκ</i>(x<i>i,&#160;</i>x<i>j</i>)<br/>
<i>m</i><br/>
<i>i,&#160;</i>x<i>i</i>)<br/>
<i>i,j</i>=1<br/>
<i>i</i>=1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
96<br/>
<hr/>
<a name=98></a>Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
97<br/>
<hr/>
<a name=99></a><b>Applying&#160;to&#160;1-norm&#160;SVMs</b><br/>
We&#160;take&#160;the&#160;following&#160;formulation&#160;of&#160;the&#160;1-norm<br/>SVM:<br/>
P<br/>
min<br/>
<i>m</i><br/>
w<i>,b,γ,ξ</i><br/>
<i>−γ&#160;</i>+&#160;<i>C</i><br/>
<i>ξ</i><br/>
<i>i</i>=1&#160;<i>i</i><br/>
subject&#160;to<br/>
<i>yi&#160;</i>(<i>h</i>w<i>,&#160;φ&#160;</i>(x<i>i</i>)<i>i&#160;</i>+&#160;<i>b</i>)&#160;<i>≥&#160;γ&#160;−&#160;ξi</i>,&#160;<i>ξi&#160;≥&#160;</i>0,<br/><i>i&#160;</i>=&#160;1<i>,&#160;.&#160;.&#160;.&#160;,&#160;m</i>,&#160;and&#160;<i>k</i>w<i>k</i>2&#160;=&#160;1.<br/>
(1)<br/>
Note&#160;that<br/>
<i>ξi&#160;</i>=&#160;(<i>γ&#160;−&#160;yig</i>(x<i>i</i>))&#160;<i>,</i><br/>
+<br/>
where&#160;<i>g</i>(<i>·</i>)&#160;=&#160;<i>h</i>w<i>,&#160;φ</i>(<i>·</i>)<i>i&#160;</i>+&#160;<i>b</i>.<br/>
<i>•&#160;</i>The&#160;first&#160;step&#160;is&#160;to&#160;introduce&#160;a&#160;loss&#160;function&#160;which<br/>
upper&#160;bounds&#160;the&#160;discrete&#160;loss<br/>
<i>P&#160;</i>(<i>y&#160;6</i>=&#160;sgn(<i>g</i>(x)))&#160;=&#160;E&#160;[H(<i>−yg</i>(x))]<i>,</i><br/>
where&#160;H&#160;is&#160;the&#160;Heaviside&#160;function.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
98<br/>
<hr/>
<a name=100></a><b>Applying&#160;the&#160;Rademacher&#160;theorem</b><br/>
<i>•&#160;</i>Consider&#160;the&#160;loss&#160;function&#160;A&#160;:&#160;R&#160;<i>→&#160;</i>[0<i>,&#160;</i>1],&#160;given<br/>
by<br/>
<br/>
&#160;1,<br/>
if&#160;<i>a&#160;&gt;&#160;</i>0;<br/>
A(<i>a</i>)&#160;=<br/>
1&#160;+&#160;<i>a/γ</i>,<br/>
if&#160;<i>−γ&#160;≤&#160;a&#160;≤&#160;</i>0;<br/>
&#160;0,<br/>
otherwise.<br/>
<i>•&#160;</i>By&#160;the&#160;Rademacher&#160;Theorem&#160;and&#160;since&#160;the&#160;loss<br/>
function&#160;A&#160;<i>−&#160;</i>1&#160;dominates&#160;H&#160;<i>−&#160;</i>1,&#160;we&#160;have&#160;that<br/>
E&#160;[H(<i>−yg</i>(x))&#160;<i>−&#160;</i>1]&#160;<i>≤&#160;</i>E&#160;[A(<i>−yg</i>(x))&#160;<i>−&#160;</i>1]<br/>
<i>≤&#160;</i>ˆ<br/>
E&#160;[A(<i>−yg</i>(x))&#160;<i>−&#160;</i>1]&#160;+r<br/>
ˆ<br/>
ln(2<i>/δ</i>)<br/>
<i>Rm</i>((A&#160;<i>−&#160;</i>1)&#160;<i>◦&#160;</i>F)&#160;+&#160;3<br/>
<i>.</i><br/>
2<i>m</i><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
99<br/>
<hr/>
<a name=101></a><b>Empirical&#160;loss&#160;and&#160;slack&#160;variables</b><br/>
<i>•&#160;</i>But&#160;the&#160;function&#160;A(<i>−yig</i>(x<i>i</i>))&#160;<i>≤&#160;ξi/γ</i>,&#160;for&#160;<i>i&#160;</i>=<br/>
1<i>,&#160;.&#160;.&#160;.&#160;,&#160;`</i>,&#160;and&#160;so<br/>
r<br/>
1<br/>
<i>m</i><br/>
X<br/>
ln(2<i>/δ</i>)<br/>
E&#160;[H(<i>−yg</i>(x))]&#160;<i>≤</i><br/>
<i>ξ</i><br/>
<i>.</i><br/>
<i>mγ</i><br/>
<i>i&#160;</i>+&#160;ˆ<br/>
<i>Rm</i>((A&#160;<i>−&#160;</i>1)&#160;<i>◦&#160;</i>F)&#160;+&#160;3<br/>
2<i>m</i><br/>
<i>i</i>=1<br/>
<i>•&#160;</i>The&#160;final&#160;missing&#160;ingredient&#160;to&#160;complete&#160;the<br/>
bound&#160;is&#160;to&#160;bound&#160;ˆ<br/>
<i>Rm</i>((A&#160;<i>−&#160;</i>1)&#160;<i>◦&#160;</i>F)&#160;in&#160;terms&#160;of<br/>
ˆ<br/>
<i>Rm</i>(F).<br/>
<i>•&#160;</i>This&#160;will&#160;require&#160;a&#160;more&#160;detailed&#160;look&#160;at<br/>
Rademacher&#160;complexity.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
100<br/>
<hr/>
<a name=102></a><b>Rademacher&#160;complexity&#160;bounds</b><br/>
<i>•&#160;</i>First&#160;simple&#160;observation:<br/>
for&#160;<i>a&#160;∈&#160;</i>R<i>,</i><br/>
ˆ<br/>
<i>Rm</i>(<i>a</i>F)&#160;=&#160;<i>|a|&#160;</i>ˆ<br/>
<i>Rm</i>(F)<i>,</i><br/>
since&#160;<i>af&#160;</i>is&#160;the&#160;function&#160;achieving&#160;the&#160;sup&#160;for<br/>some&#160;<i>σ&#160;</i>for&#160;<i>a</i>F&#160;iff&#160;<i>f&#160;</i>achieves&#160;the&#160;sup&#160;for&#160;F.<br/>
<i>•&#160;</i>We&#160;are&#160;interested&#160;in&#160;bounding&#160;RC&#160;ˆ<br/>
<i>Rm</i>(L&#160;<i>◦</i><br/>
F)&#160;<i>≤&#160;</i>2<i>L&#160;</i>ˆ<br/>
<i>Rm</i>(F)&#160;for&#160;class&#160;L&#160;<i>◦&#160;</i>F&#160;=&#160;<i>{</i>L&#160;<i>◦&#160;f&#160;</i>:&#160;<i>f&#160;∈&#160;</i>F<i>},</i><br/>
where&#160;L&#160;satisfies,&#160;L(0)&#160;=&#160;0&#160;and<br/>
<i>|</i>L(<i>a</i>)&#160;<i>−&#160;</i>L(<i>b</i>)<i>|&#160;≤&#160;L|a&#160;−&#160;b|,</i><br/>
i.e.&#160;L&#160;is&#160;a&#160;Lipschitz&#160;function&#160;with&#160;constant&#160;<i>L&#160;&gt;&#160;</i>0.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
101<br/>
<hr/>
<a name=103></a><b>Rademacher&#160;complexity&#160;bounds&#160;cont.</b><br/>
<i>•&#160;</i>In&#160;our&#160;case&#160;L&#160;=&#160;A&#160;<i>−&#160;</i>1&#160;and&#160;<i>L&#160;</i>=&#160;1<i>/γ</i>.<br/>
<i>•&#160;</i>By&#160;above&#160;it&#160;is&#160;sufficient&#160;to&#160;prove&#160;for&#160;case&#160;<i>L&#160;</i>=&#160;1<br/>
only,&#160;since&#160;then<br/>
ˆ<br/>
<i>Rm</i>(L&#160;<i>◦&#160;</i>F)&#160;=&#160;<i>L&#160;</i>ˆ<br/>
<i>Rm</i>((L<i>/L</i>)&#160;<i>◦&#160;</i>F)&#160;<i>≤&#160;</i>2<i>L&#160;</i>ˆ<br/>
<i>Rm</i>(F)<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
102<br/>
<hr/>
<a name=104></a><b>Proof&#160;for&#160;contraction&#160;(</b><i>L&#160;</i>=&#160;1<b>)</b><br/>
Want&#160;to&#160;get&#160;rid&#160;of&#160;absolute&#160;value&#160;in&#160;RC:<br/>
&#34;<br/>
¯<br/>
¯#<br/>
¯<br/>
<i>m</i><br/>
X<br/>
¯<br/>
ˆ<br/>
¯&#160;2<br/>
¯<br/>
<i>Rm</i>(F)&#160;=&#160;E<i>σ&#160;</i>sup&#160;¯<br/>
<i>σif&#160;</i>(z<i>i</i>)¯<br/>
<i>f&#160;∈</i>F&#160;¯<i>m</i><br/>
¯<br/>
<i>i</i>=1<br/>
so&#160;consider&#160;defining<br/>
L+&#160;=&#160;L<i>,</i><br/>
L<i>−</i>(<i>a</i>)&#160;=&#160;<i>−</i>L(<i>−a</i>)<i>.</i><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
103<br/>
<hr/>
<a name=105></a><b>Proof&#160;for&#160;contraction&#160;(</b><i>L&#160;</i>=&#160;1<b>)</b><br/>
Assume&#160;F&#160;is&#160;closed&#160;under&#160;negation.&#160;Now&#160;if&#160;for<br/>some&#160;<i>σ&#160;</i>sup&#160;achieved&#160;with&#160;<i>f&#160;</i>such&#160;that<br/>
<i>m</i><br/>
X&#160;<i>σi</i>L(<i>f&#160;</i>(z<i>i</i>))&#160;<i>&lt;&#160;</i>0<i>,</i><br/>
<i>i</i>=1<br/>
then<br/>
¯<br/>
¯<br/>
¯&#160;<i>m</i><br/>
¯<br/>
<i>m</i><br/>
¯X<br/>
¯<br/>
X<br/>
¯<br/>
<i>σ</i><br/>
¯&#160;=<br/>
<i>σ</i><br/>
¯<br/>
<i>i</i>L&#160;(<i>f&#160;</i>(z<i>i</i>))¯<br/>
<i>i</i>L<i>−&#160;</i>(<i>−f&#160;</i>(z<i>i</i>))&#160;<i>,</i><br/>
<i>i</i>=1<br/>
<i>i</i>=1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
104<br/>
<hr/>
<a name=106></a><b>Contraction&#160;proof&#160;cont.</b><br/>
<i>•&#160;</i>and&#160;so<br/>
&#34;<br/>
#<br/>
<i>m</i><br/>
X<br/>
ˆ<br/>
2<br/>
<i>Rm</i>(L&#160;<i>◦&#160;</i>F)&#160;=&#160;E<i>σ</i><br/>
sup<br/>
<i>σi</i>N&#160;(<i>f&#160;</i>(z<i>i</i>))<br/>
<i>f&#160;∈</i>F<i>,</i>N<i>∈{</i>L+<i>,</i>L<i>−}&#160;m&#160;i</i>=1<br/>
<i>•&#160;</i>if&#160;we&#160;further&#160;assume&#160;0&#160;<i>∈&#160;</i>F&#160;we&#160;have<br/>
&#34;<br/>
#<br/>
<i>m</i><br/>
X<br/>
ˆ<br/>
2<br/>
<i>Rm</i>(L&#160;<i>◦&#160;</i>F)&#160;=&#160;E<i>σ&#160;</i>sup<br/>
<i>σi</i>L+&#160;(<i>f&#160;</i>(z<i>i</i>))<br/>
<i>f&#160;∈</i>F&#160;<i>m&#160;i</i>=1<br/>
&#34;<br/>
#<br/>
2&#160;<i>m</i><br/>
X<br/>
+E<i>σ&#160;</i>sup<br/>
<i>σi</i>L<i>−&#160;</i>(<i>f&#160;</i>(z<i>i</i>))<br/>
<i>f&#160;∈</i>F&#160;<i>m&#160;i</i>=1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
105<br/>
<hr/>
<a name=107></a><b>Contraction&#160;proof&#160;cont.</b><br/>
<i>•&#160;</i>Hence&#160;if&#160;we&#160;show&#160;the&#160;result&#160;without&#160;the&#160;factor&#160;of<br/>
2&#160;for&#160;the&#160;complexity&#160;without&#160;absolute&#160;values&#160;the<br/>desired&#160;result&#160;will&#160;follow,&#160;since&#160;for&#160;classes&#160;closed<br/>under&#160;negation&#160;we&#160;have<br/>
&#34;<br/>
#<br/>
<i>m</i><br/>
X<br/>
ˆ<br/>
2<br/>
<i>Rm</i>(F)&#160;=&#160;E<i>σ&#160;</i>sup<br/>
<i>σif&#160;</i>(z<i>i</i>)&#160;<i>.</i><br/>
<i>f&#160;∈</i>F&#160;<i>m&#160;i</i>=1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
106<br/>
<hr/>
<a name=108></a><b>Contraction&#160;proof&#160;cont.</b><br/>
<i>•&#160;</i>We&#160;show<br/>
&#34;<br/>
#<br/>
&#34;<br/>
#<br/>
<i>m</i><br/>
X<br/>
<i>m</i><br/>
X<br/>
E<i>σ&#160;</i>sup<br/>
<i>σi</i>L(<i>f</i>(x<i>i</i>))&#160;<i>≤&#160;</i>E<i>σ&#160;</i>sup&#160;<i>σ</i>1<i>f</i>(x1)&#160;+<br/>
<i>σi</i>L(<i>f</i>(x<i>i</i>))<br/>
<i>f&#160;∈</i>F&#160;<i>i</i>=1<br/>
<i>f&#160;∈</i>F<br/>
<i>i</i>=2<br/>
and&#160;apply&#160;induction&#160;to&#160;obtain&#160;the&#160;full&#160;result.<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
107<br/>
<hr/>
<a name=109></a><b>Contraction&#160;proof&#160;cont.</b><br/>
<i>•&#160;</i>We&#160;take&#160;the&#160;permutations&#160;in&#160;pairs:<br/>
(1<i>,&#160;σ</i>2<i>,&#160;.&#160;.&#160;.&#160;,&#160;σm</i>)&#160;and&#160;(<i>−</i>1<i>,&#160;σ</i>2<i>,&#160;.&#160;.&#160;.&#160;,&#160;σm</i>)<br/>
The&#160;result&#160;will&#160;follow&#160;if&#160;we&#160;show&#160;that&#160;for&#160;all&#160;<i>f,&#160;g&#160;∈<br/></i>F&#160;we&#160;can&#160;find&#160;<i>f&#160;0,&#160;g0&#160;∈&#160;</i>F&#160;such&#160;that&#160;(<i>fi&#160;</i>=&#160;<i>f</i>(x<i>i</i>)&#160;etc.)<br/>
<i>m</i><br/>
X<br/>
<i>m</i><br/>
X<br/>
L(<i>f</i>1)&#160;+<br/>
<i>σi</i>L(<i>fi</i>))&#160;<i>−&#160;</i>L(<i>g</i>1)&#160;+<br/>
<i>σi</i>L(<i>gi</i>)<br/>
<i>i</i>=2<br/>
<i>i</i>=2<br/>
<i>m</i><br/>
X<br/>
<i>m</i><br/>
X<br/>
<i>≤&#160;f&#160;0</i>1&#160;+<br/>
<i>σi</i>L(<i>f0i</i>))&#160;<i>−&#160;g0</i>1&#160;+<br/>
<i>σi</i>L(<i>g0i</i>)<i>.</i><br/>
<i>i</i>=2<br/>
<i>i</i>=2<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
108<br/>
<hr/>
<a name=110></a><b>Contraction&#160;proof&#160;end</b><br/>
<i>•&#160;</i>If&#160;<i>f</i>1&#160;<i>≥&#160;g</i>1&#160;take&#160;<i>f0&#160;</i>=&#160;<i>f&#160;</i>and&#160;<i>g0&#160;</i>=&#160;<i>g&#160;</i>to&#160;reduce&#160;to<br/>
showing<br/>
L(<i>f</i>1)&#160;<i>−&#160;</i>L(<i>g</i>1)&#160;<i>≤&#160;f</i>1&#160;<i>−&#160;g</i>1&#160;=&#160;<i>|f</i>1&#160;<i>−&#160;g</i>1<i>|</i><br/>
which&#160;follows&#160;since&#160;<i>|</i>L(<i>f</i>1)&#160;<i>−&#160;</i>L(<i>g</i>1)<i>|&#160;≤&#160;|f</i>1&#160;<i>−&#160;g</i>1<i>|</i>.<br/>
<i>•&#160;</i>Otherwise&#160;<i>f</i>1&#160;<i>&lt;&#160;g</i>1&#160;and&#160;we&#160;take&#160;<i>f0&#160;</i>=&#160;<i>g&#160;</i>and&#160;<i>g0&#160;</i>=&#160;<i>f</i><br/>
to&#160;reduce&#160;to&#160;showing<br/>
L(<i>f</i>1)&#160;<i>−&#160;</i>L(<i>g</i>1)&#160;<i>≤&#160;g</i>1&#160;<i>−&#160;f</i>1&#160;=&#160;<i>|f</i>1&#160;<i>−&#160;g</i>1<i>|</i><br/>
which&#160;again&#160;follows&#160;from<br/>
<i>|</i>L(<i>f</i>1)&#160;<i>−&#160;</i>L(<i>g</i>1)<i>|&#160;≤&#160;|f</i>1&#160;<i>−&#160;g</i>1<i>|.</i><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
109<br/>
<hr/>
<a name=111></a><b>Final&#160;SVM&#160;bound</b><br/>
<i>•&#160;</i>Assembling&#160;the&#160;result&#160;we&#160;obtain:<br/>
<i>P&#160;</i>(<i>y&#160;6</i>=&#160;sgn(<i>g</i>(x)))&#160;=&#160;E&#160;[H(<i>−yg</i>(x))]<br/>
v<br/>
u<br/>
r<br/>
1<br/>
<i>m</i><br/>
X<br/>
4&#160;u&#160;<i>m</i><br/>
X<br/>
ln(2<i>/δ</i>)<br/>
<i>≤</i><br/>
<i>ξ</i><br/>
t<br/>
<i>κ</i>(x<br/>
<i>mγ</i><br/>
<i>i&#160;</i>+&#160;<i>mγ</i><br/>
<i>i,&#160;</i>x<i>i</i>)&#160;+&#160;3<br/>
2<i>m</i><br/>
<i>i</i>=1<br/>
<i>i</i>=1<br/>
<i>•&#160;</i>Note&#160;that&#160;for&#160;the&#160;Gaussian&#160;kernel&#160;this&#160;reduces&#160;to<br/>
r<br/>
1<br/>
<i>m</i><br/>
X<br/>
4<br/>
ln(2<i>/δ</i>)<br/>
<i>P&#160;</i>(<i>y&#160;6</i>=&#160;sgn(<i>g</i>(x)))&#160;<i>≤</i><br/>
<i>ξ</i><br/>
<i>√</i><br/>
+&#160;3<br/>
<i>mγ</i><br/>
<i>i&#160;</i>+<br/>
<i>mγ</i><br/>
2<i>m</i><br/>
<i>i</i>=1<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
110<br/>
<hr/>
<a name=112></a><b>Final&#160;Boosting&#160;bound</b><br/>
<i>•&#160;</i>Applying&#160;a&#160;similar&#160;strategy&#160;for&#160;Boosting&#160;with&#160;the<br/>
1-norm&#160;of&#160;the&#160;slack&#160;variables&#160;we&#160;arrive&#160;at&#160;Linear<br/>programming&#160;boosting&#160;that&#160;minimises<br/>
X<br/>
<i>m</i><br/>
X<br/>
<i>ah&#160;</i>+&#160;<i>C</i><br/>
<i>ξi,</i><br/>
<i>h</i><br/>
<i>i</i>=1<br/>
P<br/>
where&#160;<i>ξi&#160;</i>=&#160;(1&#160;<i>−&#160;yi</i><br/>
<i>a</i><br/>
.<br/>
<i>h&#160;hh</i>(x<i>i</i>))+<br/>
<i>•&#160;</i>with&#160;corresponding&#160;bound:<br/>
<i>P&#160;</i>(<i>y&#160;6</i>=&#160;sgn(<i>g</i>(x)))&#160;=&#160;E&#160;[H(<i>−yg</i>(x))]<br/>
r<br/>
1&#160;<i>m</i><br/>
X<br/>
X<br/>
ln(2<i>/δ</i>)<br/>
<i>≤</i><br/>
<i>ξ</i><br/>
<i>a</i><br/>
<i>m</i><br/>
<i>i&#160;</i>+&#160;ˆ<br/>
<i>R</i>(<i>H</i>)<br/>
<i>h&#160;</i>+&#160;3<br/>
2<i>m</i><br/>
<i>i</i>=1<br/>
<i>h</i><br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
111<br/>
<hr/>
<a name=113></a><b>Conclusions</b><br/>
<i>•&#160;</i>Outline&#160;of&#160;philosophy&#160;and&#160;approach&#160;of&#160;SLT<br/>
<i>•&#160;</i>Central&#160;result&#160;of&#160;SLT<br/>
<i>•&#160;</i>Touched&#160;on&#160;covering&#160;number&#160;analysis&#160;for&#160;margin<br/>
based&#160;analysis<br/>
<i>•&#160;</i>Moved&#160;to&#160;consideration&#160;of&#160;Rademacher&#160;complexity.<br/>
<i>•&#160;</i>Case&#160;of&#160;RC&#160;for&#160;classification&#160;giving&#160;bounds<br/>
for&#160;two&#160;of&#160;the&#160;most&#160;effective&#160;classification<br/>algorithms:&#160;SVMs&#160;and&#160;Boosting<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
112<br/>
<hr/>
<a name=114></a><b>Where&#160;to&#160;find&#160;out&#160;more</b><br/>
<b>Web&#160;Sites:&#160;</b>www.support-vector.net&#160;(SV&#160;Machines)<br/>
www.kernel-methods.net&#160;(kernel&#160;methods)<br/>
www.kernel-machines.net&#160;(kernel&#160;Machines)<br/>
www.neurocolt.com<br/>
www.pascal-network.org<br/>
Machine&#160;Learning&#160;Summer&#160;School&#160;(MLSS’09)&#160;Tutorial,&#160;Sept&#160;2009<br/>
113<br/>
<hr/>
</body>
</html>
