<!DOCTYPE html><html>
<head>
<title></title>
<style type="text/css">
<!--
.xflip {
    -moz-transform: scaleX(-1);
    -webkit-transform: scaleX(-1);
    -o-transform: scaleX(-1);
    transform: scaleX(-1);
    filter: fliph;
}
.yflip {
    -moz-transform: scaleY(-1);
    -webkit-transform: scaleY(-1);
    -o-transform: scaleY(-1);
    transform: scaleY(-1);
    filter: flipv;
}
.xyflip {
    -moz-transform: scaleX(-1) scaleY(-1);
    -webkit-transform: scaleX(-1) scaleY(-1);
    -o-transform: scaleX(-1) scaleY(-1);
    transform: scaleX(-1) scaleY(-1);
    filter: fliph + flipv;
}
-->
</style>
</head>
<body>
<a name=1></a>Machine&#160;Learning&#160;Summer&#160;School<br/>
Lecture&#160;1:&#160;Introduction&#160;to&#160;Graphical&#160;Models<br/>
Zoubin&#160;Ghahramani<br/>
zoubin@eng.cam.ac.uk<br/>
http://learning.eng.cam.ac.uk/zoubin/<br/>
Department&#160;of&#160;Engineering<br/>
University&#160;of&#160;Cambridge,&#160;UK<br/>
Machine&#160;Learning&#160;Department<br/>
Carnegie&#160;Mellon&#160;University,&#160;USA<br/>
August&#160;2009<br/>
<hr/>
<a name=2></a>Three&#160;main&#160;kinds&#160;of&#160;graphical&#160;models<br/>
<i>A</i><br/>
<i>B</i><br/>
<i>A</i><br/>
<i>B</i><br/>
<i>A</i><br/>
<i>B</i><br/>
<i>C</i><br/>
<i>C</i><br/>
<i>C</i><br/>
<i>D</i><br/>
<i>D</i><br/>
<i>D</i><br/>
<i>E</i><br/>
<i>E</i><br/>
<i>E</i><br/>
factor&#160;graph<br/>
undirected&#160;graph<br/>
directed&#160;graph<br/>
•&#160;Nodes&#160;correspond&#160;to&#160;random&#160;variables<br/>
•&#160;Edges&#160;represent&#160;statistical&#160;dependencies&#160;between&#160;the&#160;variables<br/>
<hr/>
<a name=3></a>Why&#160;do&#160;we&#160;need&#160;graphical&#160;models?<br/>
•&#160;Graphs&#160;are&#160;an&#160;intuitive&#160;way&#160;of&#160;representing&#160;and&#160;visualising&#160;the&#160;relationships&#160;between<br/>
many&#160;variables.&#160;(Examples:&#160;family&#160;trees,&#160;electric&#160;circuit&#160;diagrams,&#160;neural&#160;networks)<br/>
•&#160;A&#160;graph&#160;allows&#160;us&#160;to&#160;abstract&#160;out&#160;the&#160;conditional&#160;independence&#160;relationships&#160;between<br/>
the&#160;variables&#160;from&#160;the&#160;details&#160;of&#160;their&#160;parametric&#160;forms.&#160;Thus&#160;we&#160;can&#160;answer&#160;questions<br/>like:&#160;“Is&#160;A&#160;dependent&#160;on&#160;B&#160;given&#160;that&#160;we&#160;know&#160;the&#160;value&#160;of&#160;C&#160;?”&#160;just&#160;by&#160;looking&#160;at<br/>the&#160;graph.<br/>
•&#160;Graphical&#160;models&#160;allow&#160;us&#160;to&#160;define&#160;general&#160;message-passing&#160;algorithms&#160;that<br/>
implement&#160;probabilistic&#160;inference&#160;efficiently.&#160;Thus&#160;we&#160;can&#160;answer&#160;queries&#160;like&#160;“What&#160;is<br/>p(A|C&#160;=&#160;c)?”&#160;without&#160;enumerating&#160;all&#160;settings&#160;of&#160;all&#160;variables&#160;in&#160;the&#160;model.<br/>
Graphical&#160;models&#160;=&#160;statistics&#160;×&#160;graph&#160;theory&#160;×&#160;computer&#160;science.<br/>
<hr/>
<a name=4></a>Conditional&#160;Independence<br/>
Conditional&#160;Independence:<br/>
X⊥<br/>
⊥Y&#160;|V<br/>
⇔&#160;p(X|Y,&#160;V&#160;)&#160;=&#160;p(X|V&#160;)<br/>
when&#160;p(Y,&#160;V&#160;)&#160;&gt;&#160;0.&#160;Also<br/>
X⊥<br/>
⊥Y&#160;|V<br/>
⇔&#160;p(X,&#160;Y&#160;|V&#160;)&#160;=&#160;p(X|V&#160;)&#160;p(Y&#160;|V&#160;)<br/>
In&#160;general&#160;we&#160;can&#160;think&#160;of&#160;conditional&#160;independence&#160;between&#160;sets&#160;of&#160;variables:<br/>
X&#160;⊥<br/>
⊥Y|V&#160;⇔&#160;p(X&#160;,&#160;Y|V)&#160;=&#160;p(X&#160;|V)&#160;p(Y|V)<br/>
Marginal&#160;Independence:<br/>
X⊥<br/>
⊥Y<br/>
⇔&#160;X⊥<br/>
⊥Y&#160;|∅&#160;⇔&#160;p(X,&#160;Y&#160;)&#160;=&#160;p(X)&#160;p(Y&#160;)<br/>
<hr/>
<a name=5></a>Conditional&#160;and&#160;Marginal&#160;Independence&#160;(Examples)<br/>
•&#160;Amount&#160;of&#160;Speeding&#160;Fine&#160;⊥<br/>
⊥&#160;Type&#160;of&#160;Car&#160;|&#160;Speed<br/>
•&#160;Lung&#160;Cancer&#160;⊥<br/>
⊥&#160;Yellow&#160;Teeth&#160;|&#160;Smoking<br/>
•&#160;(Position,&#160;Velocity)t+1&#160;⊥<br/>
⊥&#160;(Position,&#160;Velocity)t−1&#160;|&#160;(Position,&#160;Velocity)t,&#160;Accelerationt<br/>
•&#160;Child’s&#160;Genes&#160;⊥<br/>
⊥&#160;Grandparents’&#160;Genes&#160;|&#160;Parents’&#160;Genes<br/>
•&#160;Ability&#160;of&#160;Team&#160;A&#160;⊥<br/>
⊥&#160;Ability&#160;of&#160;Team&#160;B<br/>
•&#160;not&#160;(&#160;Ability&#160;of&#160;Team&#160;A&#160;⊥<br/>
⊥&#160;Ability&#160;of&#160;Team&#160;B&#160;|&#160;Outcome&#160;of&#160;A&#160;vs&#160;B&#160;Game&#160;)<br/>
<hr/>
<a name=6></a>Factor&#160;Graphs<br/>
<i>A</i><br/>
<i>B</i><br/>
<i>A</i><br/>
<i>B</i><br/>
Two&#160;types&#160;of&#160;nodes:<br/>
•&#160;The<br/>
circles<br/>
in<br/>
a<br/>
factor<br/>
graph<br/>
<i>C</i><br/>
<i>C</i><br/>
represent&#160;random&#160;variables&#160;(e.g.&#160;A).<br/>
<i>D</i><br/>
<i>D</i><br/>
•&#160;The&#160;filled&#160;dots&#160;represent&#160;factors&#160;in<br/>
<i>E</i><br/>
<i>E</i><br/>
the&#160;joint&#160;distribution&#160;(e.g.&#160;g1(·)).<br/>
(a)<br/>
(b)<br/>
(a)&#160;p(A,&#160;B,&#160;C,&#160;D,&#160;E)&#160;=&#160;1&#160;g<br/>
Z&#160;1(A,&#160;C&#160;)g2(B,&#160;C,&#160;D)g3(C,&#160;D,&#160;E)<br/>
(b)&#160;p(A,&#160;B,&#160;C,&#160;D,&#160;E)&#160;=&#160;1&#160;g<br/>
Z&#160;1(A,&#160;C&#160;)g2(B,&#160;C&#160;)g3(C,&#160;D)g4(B,&#160;D)g5(C,&#160;E)g6(D,&#160;E)<br/>
The&#160;gi&#160;are&#160;non-negative&#160;functions&#160;of&#160;their&#160;arguments,&#160;and&#160;Z&#160;is&#160;a&#160;normalization&#160;constant.<br/>E.g.&#160;in&#160;(a),&#160;if&#160;all&#160;variables&#160;are&#160;discrete&#160;and&#160;take&#160;values&#160;in&#160;A&#160;×&#160;B&#160;×&#160;C&#160;×&#160;D&#160;×&#160;E&#160;:<br/>
X&#160;X&#160;X&#160;X&#160;X<br/>
Z&#160;=<br/>
g1(A&#160;=&#160;a,&#160;C&#160;=&#160;c)g2(B&#160;=&#160;b,&#160;C&#160;=&#160;c,&#160;D&#160;=&#160;d)g3(C&#160;=&#160;c,&#160;D&#160;=&#160;d,&#160;E&#160;=&#160;e)<br/>
a∈A&#160;b∈B&#160;c∈C&#160;d∈D&#160;e∈E<br/>
Two&#160;nodes&#160;are&#160;neighbors&#160;if&#160;they&#160;share&#160;a&#160;common&#160;factor.<br/>
<hr/>
<a name=7></a>Factor&#160;Graphs<br/>
<i>A</i><br/>
<i>B</i><br/>
<i>A</i><br/>
<i>B</i><br/>
<i>C</i><br/>
<i>C</i><br/>
<i>D</i><br/>
<i>D</i><br/>
<i>E</i><br/>
<i>E</i><br/>
(a)<br/>
(b)<br/>
The&#160;circles&#160;in&#160;a&#160;factor&#160;graph&#160;represent&#160;random&#160;variables.<br/>The&#160;filled&#160;dots&#160;represent&#160;factors&#160;in&#160;the&#160;joint&#160;distribution.<br/>
(a)&#160;p(A,&#160;B,&#160;C,&#160;D,&#160;E)&#160;=&#160;1&#160;g<br/>
Z&#160;1(A,&#160;C&#160;)g2(B,&#160;C,&#160;D)g3(C,&#160;D,&#160;E)<br/>
(b)&#160;p(A,&#160;B,&#160;C,&#160;D,&#160;E)&#160;=&#160;1&#160;g<br/>
Z&#160;1(A,&#160;C&#160;)g2(B,&#160;C&#160;)g3(C,&#160;D)g4(B,&#160;D)g5(C,&#160;E)g6(D,&#160;E)<br/>
Two&#160;nodes&#160;are&#160;neighbors&#160;if&#160;they&#160;share&#160;a&#160;common&#160;factor.<br/>
Definition:&#160;A&#160;path&#160;is&#160;a&#160;sequence&#160;of&#160;neighboring&#160;nodes.<br/>
Fact:&#160;X⊥<br/>
⊥Y&#160;|V&#160;if&#160;every&#160;path&#160;between&#160;X&#160;and&#160;Y&#160;contains&#160;some&#160;node&#160;V&#160;∈&#160;V<br/>
Corollary:&#160;Given&#160;the&#160;neighbors&#160;of&#160;X,&#160;the&#160;variable&#160;X&#160;is&#160;conditionally&#160;independent&#160;of&#160;all<br/>other&#160;variables:&#160;X⊥<br/>
⊥Y&#160;|&#160;ne(X),<br/>
∀Y&#160;/<br/>
∈&#160;{X}&#160;∪&#160;ne(X)<br/>
<hr/>
<a name=8></a>Proving&#160;Conditional&#160;Independence<br/>
Assume:<br/>
<i>g</i><br/>
<i>g</i><br/>
<i>1</i><br/>
<i>2</i><br/>
1<br/>
<i>X</i><br/>
<i>V</i><br/>
<i>Y</i><br/>
p(X,&#160;Y,&#160;V&#160;)&#160;=<br/>
g1(X,&#160;V&#160;)g2(Y,&#160;V&#160;),<br/>
(1)<br/>
Z<br/>
We&#160;want&#160;to&#160;show&#160;conditional&#160;independence:<br/>
X⊥<br/>
⊥Y&#160;|V<br/>
⇔&#160;p(X|Y,&#160;V&#160;)&#160;=&#160;p(X|V&#160;)<br/>
(2)<br/>
Summing&#160;<a href="Ghahramani_1s.html#8">(1)&#160;</a>over&#160;X&#160;we&#160;get:<br/>
&#34;<br/>
#<br/>
1<br/>
X<br/>
p(Y,&#160;V&#160;)&#160;=<br/>
g1(X,&#160;V&#160;)&#160;g2(Y,&#160;V&#160;)<br/>
(3)<br/>
Z<br/>
X<br/>
Dividing&#160;<a href="Ghahramani_1s.html#8">(1)&#160;</a>by&#160;<a href="Ghahramani_1s.html#8">(3)&#160;</a>we&#160;get:<br/>
g1(X,&#160;V&#160;)<br/>
p(X|Y,&#160;V&#160;)&#160;=<br/>
(4)<br/>
P<br/>
g<br/>
X<br/>
1(X,&#160;V&#160;)<br/>
Since&#160;the&#160;rhs.&#160;of&#160;<a href="Ghahramani_1s.html#8">(4)&#160;</a>doesn’t&#160;depend&#160;on&#160;Y&#160;,&#160;it&#160;follows&#160;that&#160;X&#160;is&#160;independent&#160;of&#160;Y&#160;given&#160;V&#160;.<br/>Therefore&#160;factorizaton&#160;<a href="Ghahramani_1s.html#8">(1)&#160;</a>implies&#160;conditional&#160;independence&#160;<a href="Ghahramani_1s.html#8">(2).</a><br/>
<hr/>
<a name=9></a>Undirected&#160;Graphical&#160;Models<br/>
In&#160;an&#160;Undirected&#160;Graphical&#160;Model,&#160;the&#160;joint&#160;probability&#160;over&#160;all&#160;variables&#160;can&#160;be&#160;written&#160;in<br/>a&#160;factored&#160;form:<br/>
1&#160;Y<br/>
p(x)&#160;=<br/>
gj(xC&#160;)<br/>
Z<br/>
j<br/>
j<br/>
where&#160;x&#160;=&#160;(x1,&#160;.&#160;.&#160;.&#160;,&#160;xK),&#160;and<br/>
Cj&#160;⊆&#160;{1,&#160;.&#160;.&#160;.&#160;,&#160;K}<br/>
are&#160;subsets&#160;of&#160;the&#160;set&#160;of&#160;all&#160;variables,&#160;and&#160;xS&#160;≡&#160;(xk&#160;:&#160;k&#160;∈&#160;S).<br/>
Graph&#160;Specification:&#160;Create&#160;a&#160;node&#160;for&#160;each&#160;variable.&#160;Connect&#160;nodes&#160;i&#160;and&#160;k&#160;if&#160;there<br/>exists&#160;a&#160;set&#160;Cj&#160;such&#160;that&#160;both&#160;i&#160;∈&#160;Cj&#160;and&#160;k&#160;∈&#160;Cj.&#160;These&#160;sets&#160;form&#160;the&#160;cliques&#160;of&#160;the&#160;graph<br/>(fully&#160;connected&#160;subgraphs).<br/>
Note:&#160;Undirected&#160;Graphical&#160;Models&#160;are&#160;also&#160;called&#160;Markov&#160;Networks.<br/>
Very&#160;similar&#160;to&#160;factor&#160;graphs.<br/>
<hr/>
<a name=10></a>Undirected&#160;Graphical&#160;Models<br/>
<i>A</i><br/>
<i>B</i><br/>
<i>C</i><br/>
<i>D</i><br/>
<i>E</i><br/>
1<br/>
p(A,&#160;B,&#160;C,&#160;D,&#160;E)&#160;=<br/>
g1(A,&#160;C)g2(B,&#160;C,&#160;D)g3(C,&#160;D,&#160;E)<br/>
Z<br/>
Fact:&#160;X⊥<br/>
⊥Y&#160;|V&#160;if&#160;every&#160;path&#160;between&#160;X&#160;and&#160;Y&#160;contains&#160;some&#160;node&#160;V&#160;∈&#160;V<br/>
Corollary:&#160;Given&#160;the&#160;neighbors&#160;of&#160;X,&#160;the&#160;variable&#160;X&#160;is&#160;conditionally&#160;independent&#160;of&#160;all<br/>other&#160;variables:&#160;X⊥<br/>
⊥Y&#160;|&#160;ne(X),<br/>
∀Y&#160;/<br/>
∈&#160;{X}&#160;∪&#160;ne(X)<br/>
Markov&#160;Blanket:&#160;V&#160;is&#160;a&#160;Markov&#160;Blanket&#160;for&#160;X&#160;iff&#160;X⊥<br/>
⊥Y&#160;|V&#160;for&#160;all&#160;Y&#160;/<br/>
∈&#160;{X&#160;∪&#160;V}.<br/>
Markov&#160;Boundary:&#160;minimal&#160;Markov&#160;Blanket&#160;≡&#160;ne(X)&#160;for&#160;undirected&#160;and&#160;factor&#160;graphs<br/>
<hr/>
<a name=11></a>Comparing&#160;Undirected&#160;Graphs&#160;and&#160;Factor&#160;Graphs<br/>
<i>A</i><br/>
<i>B</i><br/>
<i>A</i><br/>
<i>B</i><br/>
<i>A</i><br/>
<i>B</i><br/>
<i>C</i><br/>
<i>C</i><br/>
<i>C</i><br/>
<i>D</i><br/>
<i>D</i><br/>
<i>D</i><br/>
<i>E</i><br/>
<i>E</i><br/>
<i>E</i><br/>
(a)<br/>
(b)<br/>
(c)<br/>
All&#160;nodes&#160;in&#160;(a),&#160;(b),&#160;and&#160;(c)&#160;have&#160;exactly&#160;the&#160;same&#160;neighbors&#160;and&#160;therefore&#160;these&#160;three<br/>graphs&#160;represent&#160;exactly&#160;the&#160;same&#160;conditional&#160;independence&#160;relationships.<br/>
(c)&#160;also&#160;represents&#160;the&#160;fact&#160;that&#160;the&#160;probability&#160;factors&#160;into&#160;a&#160;product&#160;of&#160;pairwise&#160;functions.<br/>
Consider&#160;the&#160;case&#160;where&#160;each&#160;variables&#160;is&#160;discrete&#160;and&#160;can&#160;take&#160;on&#160;K&#160;possible&#160;values.&#160;Then<br/>the&#160;functions&#160;in&#160;(a)&#160;and&#160;(b)&#160;are&#160;tables&#160;with&#160;O(K3)&#160;cells,&#160;whereas&#160;in&#160;(c)&#160;they&#160;are&#160;O(K2).<br/>
<hr/>
<a name=12></a>Problems&#160;with&#160;Undirected&#160;Graphs&#160;and&#160;Factor&#160;Graphs<br/>
In&#160;UGs&#160;and&#160;FGs,&#160;many&#160;useful&#160;independencies&#160;are&#160;not&#160;represented—two&#160;variables&#160;are<br/>connected&#160;merely&#160;because&#160;some&#160;other&#160;variable&#160;depends&#160;on&#160;them:<br/>
Rain<br/>
Sprinkler<br/>
Rain<br/>
Sprinkler<br/>
Ground wet<br/>
Ground wet<br/>
This&#160;highlights&#160;the&#160;difference&#160;between&#160;marginal<br/>
independence&#160;and&#160;conditional<br/>
independence.<br/>
R&#160;and&#160;S&#160;are&#160;marginally&#160;independent&#160;(i.e.&#160;given&#160;nothing),&#160;but&#160;they&#160;are&#160;conditionally<br/>dependent&#160;given&#160;G.<br/>
“Explaining&#160;Away”:&#160;Observing&#160;that&#160;the&#160;spinkler&#160;is&#160;on&#160;would&#160;explain&#160;away&#160;the&#160;observation<br/>that&#160;the&#160;ground&#160;was&#160;wet,&#160;making&#160;it&#160;less&#160;probable&#160;that&#160;it&#160;rained.<br/>
<hr/>
<a name=13></a>Directed&#160;Acyclic&#160;Graphical&#160;Models&#160;(Bayesian&#160;Networks)<br/>
<i>A</i><br/>
<i>B</i><br/>
<i>C</i><br/>
<i>D</i><br/>
<i>E</i><br/>
A&#160;DAG&#160;Model&#160;/&#160;Bayesian&#160;netwo<a href="Ghahramani_1s.html#13">rk1&#160;</a>corresponds&#160;to&#160;a&#160;factorization&#160;of&#160;the&#160;joint&#160;probability<br/>distribution:<br/>
p(A,&#160;B,&#160;C,&#160;D,&#160;E)&#160;=&#160;p(A)p(B)p(C|A,&#160;B)p(D|B,&#160;C)p(E|C,&#160;D)<br/>
In&#160;general:<br/>
n<br/>
Y<br/>
p(X1,&#160;.&#160;.&#160;.&#160;,&#160;Xn)&#160;=<br/>
p(Xi|Xpa(i))<br/>
i=1<br/>
where&#160;pa(i)&#160;are&#160;the&#160;parents&#160;of&#160;node&#160;i.<br/>
1“Bayesian&#160;networks”&#160;can&#160;and&#160;often&#160;are&#160;learned&#160;using&#160;non-Bayesian&#160;(i.e.&#160;frequentist)&#160;methods;&#160;Bayesian&#160;networks&#160;(i.e.&#160;DAGs)<br/>
do&#160;not&#160;require&#160;parameter&#160;or&#160;structure&#160;learning&#160;using&#160;Bayesian&#160;methods.&#160;Also&#160;called&#160;“belief&#160;networks”.<br/>
<hr/>
<a name=14></a>Directed&#160;Acyclic&#160;Graphical&#160;Models&#160;(Bayesian&#160;Networks)<br/>
<i>A</i><br/>
<i>B</i><br/>
<i>C</i><br/>
<i>D</i><br/>
<i>E</i><br/>
Semantics:&#160;X⊥<br/>
⊥Y&#160;|V&#160;if&#160;V&#160;d-separates&#160;X&#160;from&#160;Y&#160;<a href="Ghahramani_1s.html#14">2.</a><br/>
Definition:&#160;V&#160;d-separates&#160;X&#160;from&#160;Y&#160;if&#160;every&#160;undirected&#160;<a href="Ghahramani_1s.html#14">path3&#160;</a>between&#160;X&#160;and&#160;Y&#160;is<br/>blocked&#160;by&#160;V.&#160;A&#160;path&#160;is&#160;blocked&#160;by&#160;V&#160;if&#160;there&#160;is&#160;a&#160;node&#160;W&#160;on&#160;the&#160;path&#160;such&#160;that&#160;either:<br/>
1.&#160;W&#160;has&#160;converging&#160;arrows&#160;along&#160;the&#160;path&#160;(→&#160;W&#160;←<a href="Ghahramani_1s.html#14">)4&#160;</a>and&#160;neither&#160;W&#160;nor&#160;its&#160;descendants<br/>
are&#160;observed&#160;(in&#160;V),&#160;or<br/>
2.&#160;W&#160;does&#160;not&#160;have&#160;converging&#160;arrows&#160;along&#160;the&#160;path&#160;(→&#160;W&#160;→&#160;or&#160;←&#160;W&#160;→)&#160;and&#160;W&#160;is<br/>
observed&#160;(W&#160;∈&#160;V).<br/>
Corollary:&#160;Markov&#160;Boundary&#160;for&#160;X:&#160;{parents(X)∪children(X)∪parents-of-children(X)}.<br/>
2See&#160;also&#160;the&#160;“Bayes&#160;Ball”&#160;algorithm&#160;in&#160;the&#160;Appendix<br/>3An&#160;undirected&#160;path&#160;ignores&#160;the&#160;direction&#160;of&#160;the&#160;edges.<br/>4Note&#160;that&#160;converging&#160;arrows&#160;along&#160;the&#160;path&#160;only&#160;refers&#160;to&#160;what&#160;happens&#160;on&#160;that&#160;path.&#160;Also&#160;called&#160;a&#160;collider.<br/>
<hr/>
<a name=15></a>Examples&#160;of&#160;D-Separation&#160;in&#160;DAGs<br/>
<i>A</i><br/>
<i>B</i><br/>
<i>C</i><br/>
<i>D</i><br/>
<i>E</i><br/>
Examples:<br/>
•&#160;A⊥<br/>
⊥B&#160;since&#160;A&#160;→&#160;C&#160;←&#160;B&#160;is&#160;blocked1&#160;by&#160;C,&#160;A&#160;→&#160;C&#160;→&#160;D&#160;←&#160;B&#160;is&#160;blocked1&#160;by&#160;D,&#160;etc.<br/>
•&#160;not&#160;(A⊥<br/>
⊥B|C&#160;)&#160;since&#160;A&#160;→&#160;C&#160;←&#160;B&#160;is&#160;not&#160;blocked.<br/>
•&#160;A⊥<br/>
⊥D|{B,&#160;C}&#160;since&#160;A&#160;→&#160;C&#160;→&#160;D&#160;is&#160;blocked2&#160;by&#160;C,&#160;A&#160;→&#160;C&#160;←&#160;B&#160;→&#160;D&#160;is&#160;blocked2&#160;by<br/>
B,&#160;and&#160;A&#160;→&#160;C&#160;→&#160;E&#160;←&#160;D&#160;is&#160;blocked2&#160;by&#160;C.<br/>
•&#160;not&#160;(A⊥<br/>
⊥B|E)&#160;since&#160;A&#160;→&#160;C&#160;←&#160;B&#160;is&#160;not&#160;blocked.<br/>
Note&#160;that&#160;it&#160;is&#160;the&#160;absence&#160;of&#160;edges&#160;that&#160;conveys&#160;conditional&#160;independence.<br/>
<hr/>
<a name=16></a>From&#160;Directed&#160;Trees&#160;to&#160;Undirected&#160;Trees<br/>
1<br/>
5<br/>
3<br/>
4<br/>
6<br/>
2<br/>
7<br/>
p(x1,&#160;x2,&#160;.&#160;.&#160;.&#160;,&#160;x7)&#160;=&#160;p(x3)p(x1|x3)p(x2|x3)p(x4|x3)p(x5|x4)p(x6|x4)p(x7|x4)<br/>
p(x1,&#160;x3)p(x2,&#160;x3)p(x3,&#160;x4)p(x4,&#160;x5)p(x4,&#160;x6)p(x4,&#160;x7)<br/>
=<br/>
p(x3)p(x3)p(x4)p(x4)p(x4)<br/>
product&#160;of&#160;cliques<br/>
=&#160;product&#160;of&#160;clique&#160;intersections<br/>
=&#160;g1(x1,&#160;x3)g2(x2,&#160;x3)g3(x3,&#160;x4)g4(x4,&#160;x5)g5(x4,&#160;x6)g6(x4,&#160;x7)<br/>
Y<br/>
=<br/>
gi(Ci)<br/>
i<br/>
Any&#160;directed&#160;tree&#160;can&#160;be&#160;converted&#160;into&#160;an&#160;undirected&#160;tree&#160;representing&#160;the&#160;same&#160;conditional<br/>independence&#160;relationships,&#160;and&#160;viceversa.<br/>
<hr/>
<a name=17></a>Directed&#160;Graphs&#160;for&#160;Statistical&#160;Models:<br/>
Plate&#160;Notation<br/>
Consider&#160;the&#160;following&#160;simple&#160;model.&#160;A&#160;data&#160;set&#160;of&#160;N&#160;points&#160;is&#160;generated&#160;i.i.d.&#160;from&#160;a<br/>Gaussian&#160;with&#160;mean&#160;µ&#160;and&#160;standard&#160;deviation&#160;σ:<br/>
N<br/>
Y<br/>
p(x1,&#160;.&#160;.&#160;.&#160;,&#160;xN&#160;,&#160;µ,&#160;σ)&#160;=&#160;p(µ)p(σ)<br/>
p(xn|µ,&#160;σ)<br/>
n=1<br/>
This&#160;can&#160;be&#160;represented&#160;graphically&#160;as&#160;follows:<br/>
μ<br/>
σ<br/>
μ<br/>
σ<br/>
≡<br/>
<i>xn</i><br/>
<i>x</i><br/>
<i>x</i><br/>
<i>...</i><br/>
<i>x</i><br/>
<i>1</i><br/>
<i>2</i><br/>
<i>N</i><br/>
<i>N</i><br/>
<hr/>
<a name=18></a>Expressive&#160;Power&#160;of&#160;Directed&#160;and&#160;Undirected&#160;Graphs<br/>
No&#160;Directed&#160;Graph&#160;(Bayesian<br/>network)&#160;can&#160;represent&#160;these&#160;and<br/>only&#160;these&#160;independencies<br/>
No&#160;matter&#160;how&#160;we&#160;direct&#160;the&#160;arrows&#160;there&#160;will&#160;always&#160;be&#160;two&#160;non-adjacent&#160;parents&#160;sharing<br/>a&#160;common&#160;child&#160;=⇒&#160;dependence&#160;in&#160;Directed&#160;Graph&#160;but&#160;independence&#160;in&#160;Undirected&#160;Graph.<br/>
No&#160;Undirected&#160;Graph&#160;or&#160;Factor<br/>Graph&#160;can&#160;represent&#160;these&#160;and<br/>only&#160;these&#160;independencies<br/>
Directed&#160;graphs&#160;are&#160;better&#160;at&#160;expressing&#160;causal&#160;generative&#160;models,&#160;undirected&#160;graphs&#160;are<br/>better&#160;at&#160;representing&#160;soft&#160;constraints&#160;between&#160;variables.<br/>
<hr/>
<a name=19></a>Summary<br/>
•&#160;Three&#160;kinds&#160;of&#160;graphical&#160;models:&#160;directed,&#160;undirected,&#160;factor<br/>
(there&#160;are&#160;other&#160;important&#160;classes,&#160;e.g.&#160;directed&#160;mixed&#160;graphs)<br/>
•&#160;Marginal&#160;and&#160;conditional&#160;independence<br/>
•&#160;Markov&#160;boundaries&#160;and&#160;d-separation<br/>
•&#160;Differences&#160;between&#160;directed&#160;and&#160;undirected&#160;graphs.<br/>
•&#160;Next&#160;lectures:<br/>
–&#160;exact&#160;inference&#160;and&#160;propagation&#160;algorithm<br/>–&#160;parameter&#160;and&#160;structure&#160;learning&#160;in&#160;graphs<br/>–&#160;nonparametric&#160;approaches&#160;to&#160;graph&#160;learning<br/>
<hr/>
<a name=20></a>Appendix:&#160;Some&#160;Examples&#160;of&#160;Directed&#160;Graphical&#160;Models<br/>
<i>X</i><br/>
<i>X</i><br/>
<i>1</i><br/>
<i>K</i><br/>
Λ<br/>
factor&#160;analysis<br/>probabilistic&#160;PCA<br/>
<i>Y1</i><br/>
<i>Y2</i><br/>
<i>YD</i><br/>
<i>X</i><br/>
<br/>
1<br/>
<i>X</i><br/>
<i>X</i><br/>
<br/>
2<br/>
<i>X&#160;</i>3<br/>
T<br/>
hidden&#160;Markov&#160;models<br/>linear&#160;dynamical&#160;systems<br/>
<i>Y</i><br/>
<br/>
1<br/>
<i>Y</i><br/>
<i>Y</i><br/>
<br/>
2<br/>
<i>Y</i>3<br/>
T<br/>
<i>U&#160;1</i><br/>
<i>U&#160;2</i><br/>
<i>U&#160;3</i><br/>
<i>S&#160;1</i><br/>
<i>S&#160;2</i><br/>
<i>S&#160;3</i><br/>
<i>X&#160;1</i><br/>
<i>X&#160;2</i><br/>
<i>X&#160;3</i><br/>
switching&#160;state-space&#160;models<br/>
<i>Y1</i><br/>
<i>Y2</i><br/>
<i>Y3</i><br/>
<hr/>
<a name=21></a>Appendix:&#160;Examples&#160;of&#160;Undirected&#160;Graphical&#160;Models<br/>
•&#160;Markov&#160;Random&#160;Fields&#160;(used&#160;in&#160;Computer&#160;Vision)<br/>
•&#160;Exponential&#160;Language&#160;Models&#160;(used&#160;in&#160;Speech&#160;and&#160;Language&#160;Modelling)<br/>
(<br/>
)<br/>
1<br/>
X<br/>
p(s)&#160;=<br/>
p0(s)&#160;exp<br/>
λifi(s)<br/>
Z<br/>
i<br/>
•&#160;Products&#160;of&#160;Experts&#160;(widely&#160;applicable)<br/>
1&#160;Y<br/>
p(x)&#160;=<br/>
pj(x|θj)<br/>
Z&#160;j<br/>
•&#160;Boltzmann&#160;Machines&#160;(a&#160;kind&#160;of&#160;Neural&#160;Network/Ising&#160;Model)<br/>
<hr/>
<a name=22></a>Appendix:&#160;Clique&#160;Potentials&#160;and&#160;Undirected&#160;Graphs<br/>
Definition:&#160;a&#160;clique&#160;is&#160;a&#160;fully&#160;connected&#160;subgraph.&#160;By&#160;clique&#160;we&#160;usually&#160;mean&#160;maximal<br/>clique&#160;(i.e.&#160;not&#160;contained&#160;within&#160;another&#160;clique)<br/>
Ci&#160;denotes&#160;the&#160;set&#160;of&#160;variables&#160;in&#160;the&#160;ith&#160;clique.<br/>
A<br/>
B<br/>
1&#160;Y<br/>
p(x1,&#160;.&#160;.&#160;.&#160;,&#160;xK)&#160;=<br/>
gi(xC&#160;)<br/>
Z<br/>
i<br/>
i<br/>
C<br/>
D<br/>
where&#160;Z&#160;=&#160;P<br/>
Q<br/>
g<br/>
)&#160;is&#160;the&#160;normalization.<br/>
x<br/>
i(xC<br/>
1···xK<br/>
i<br/>
i<br/>
E<br/>
Associated&#160;with&#160;each&#160;clique&#160;Ci&#160;is&#160;a&#160;non-negative&#160;function<br/>g<br/>
A<br/>
C<br/>
g1(A,&#160;C)<br/>
i(xC&#160;)&#160;which&#160;measures&#160;“compatibility”&#160;between&#160;settings<br/>
i<br/>
of&#160;the&#160;variables.<br/>
0<br/>
0<br/>
0.2<br/>
0<br/>
1<br/>
0.6<br/>
Example:&#160;Let&#160;C<br/>
1<br/>
0<br/>
0.0<br/>
1&#160;=&#160;{A,&#160;C&#160;},&#160;A&#160;∈&#160;{0,&#160;1},&#160;C&#160;∈&#160;{0,&#160;1}<br/>
What&#160;does&#160;this&#160;mean?<br/>
1<br/>
1<br/>
1.2<br/>
<hr/>
<a name=23></a>Appendix:&#160;Hammersley–Clifford&#160;Theorem&#160;(1971)<br/>
Theorem:&#160;A&#160;probability&#160;function&#160;p&#160;formed&#160;by&#160;a&#160;normalized&#160;product&#160;of&#160;positive&#160;functions<br/>on&#160;cliques&#160;of&#160;G&#160;is&#160;a&#160;Markov&#160;Field&#160;relative&#160;to&#160;G.<br/>
Definition:&#160;The&#160;distribution&#160;p&#160;is&#160;a&#160;Markov&#160;Field&#160;relative&#160;to&#160;G&#160;if&#160;all&#160;conditional&#160;independence<br/>relations&#160;represented&#160;by&#160;G&#160;are&#160;true&#160;of&#160;p.<br/>
G&#160;represents&#160;the&#160;following&#160;CI&#160;relations:&#160;If&#160;V&#160;∈&#160;V&#160;lies&#160;on&#160;all&#160;paths&#160;between&#160;X&#160;and&#160;Y&#160;in&#160;G,<br/>then&#160;X⊥<br/>
⊥Y&#160;|V.<br/>
Proof:&#160;We&#160;need&#160;to&#160;show&#160;that&#160;if&#160;p&#160;is&#160;a&#160;product&#160;of&#160;functions&#160;on&#160;cliques&#160;of&#160;G&#160;then&#160;a&#160;variable<br/>is&#160;conditionally&#160;independent&#160;of&#160;its&#160;non-neighbors&#160;in&#160;G&#160;given&#160;its&#160;neighbors&#160;in&#160;G.&#160;That&#160;is:<br/>ne(x`)&#160;is&#160;a&#160;Markov&#160;Blanket&#160;for&#160;x`.&#160;Let&#160;xm&#160;/<br/>
∈&#160;{x`&#160;∪&#160;ne(x`)}<br/>
1&#160;Y<br/>
1&#160;Y<br/>
Y<br/>
p(x`,&#160;xm,&#160;.&#160;.&#160;.)&#160;=<br/>
gi(xC&#160;)&#160;=<br/>
gi(xC&#160;)<br/>
gj(xC&#160;)<br/>
Z<br/>
i<br/>
Z<br/>
i<br/>
j<br/>
i<br/>
i:`∈Ci<br/>
j:`&#160;/<br/>
∈Cj<br/>
1<br/>
1<br/>
=<br/>
f<br/>
<br/>
1&#160;x`,&#160;ne(x`)&#160;f2<br/>
ne(x`),&#160;xm&#160;=<br/>
p(x`|&#160;ne(x`))&#160;p(xm|&#160;ne(x`))<br/>
Z0<br/>
Z00<br/>
It&#160;follows&#160;that:<br/>
p(x`,&#160;xm|&#160;ne(x`))&#160;=&#160;p(x`|&#160;ne(x`))&#160;p(xm|&#160;ne(x`))&#160;⇔&#160;x`⊥<br/>
⊥xm|&#160;ne(x`).<br/>
<hr/>
<a name=24></a>Appendix:&#160;The&#160;“Bayes-ball”&#160;algorithm<br/>
<i>A</i><br/>
<i>B</i><br/>
<i>C</i><br/>
<i>D</i><br/>
<i>E</i><br/>
Game:&#160;can&#160;you&#160;get&#160;a&#160;ball&#160;from&#160;X&#160;to&#160;Y&#160;without&#160;being&#160;blocked&#160;by&#160;V?<br/>
Depending&#160;on&#160;the&#160;direction&#160;the&#160;ball&#160;came&#160;from&#160;and&#160;the&#160;type&#160;of&#160;node,&#160;the&#160;ball&#160;can&#160;pass<br/>through&#160;(from&#160;a&#160;parent&#160;to&#160;all&#160;children,&#160;from&#160;a&#160;child&#160;to&#160;all&#160;parents),&#160;bounce&#160;back&#160;(from<br/>any&#160;parent&#160;to&#160;all&#160;parents,&#160;or&#160;from&#160;any&#160;child&#160;to&#160;all&#160;children),&#160;or&#160;be&#160;blocked.<br/>
•&#160;An&#160;unobserved&#160;(hidden)&#160;node&#160;(W&#160;/<br/>
∈&#160;V)&#160;passes&#160;balls&#160;through&#160;but&#160;also&#160;bounces&#160;back<br/>
balls&#160;from&#160;children.<br/>
•&#160;An&#160;observed&#160;(given)&#160;node&#160;(W&#160;∈&#160;V)&#160;bounces&#160;back&#160;balls&#160;from&#160;parents&#160;but&#160;blocks&#160;balls<br/>
from&#160;children.<br/>
<hr/>
</body>
</html>
