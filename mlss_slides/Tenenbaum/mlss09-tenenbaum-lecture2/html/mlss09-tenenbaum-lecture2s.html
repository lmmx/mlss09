<!DOCTYPE html><html>
<head>
<title></title>
<style type="text/css">
<!--
.xflip {
    -moz-transform: scaleX(-1);
    -webkit-transform: scaleX(-1);
    -o-transform: scaleX(-1);
    transform: scaleX(-1);
    filter: fliph;
}
.yflip {
    -moz-transform: scaleY(-1);
    -webkit-transform: scaleY(-1);
    -o-transform: scaleY(-1);
    transform: scaleY(-1);
    filter: flipv;
}
.xyflip {
    -moz-transform: scaleX(-1) scaleY(-1);
    -webkit-transform: scaleX(-1) scaleY(-1);
    -o-transform: scaleX(-1) scaleY(-1);
    transform: scaleX(-1) scaleY(-1);
    filter: fliph + flipv;
}
-->
</style>
</head>
<body>
<a name=1></a>Outline for lectures<br/>
•&#160;&#160;Introduction<br/>
•&#160;&#160;Cognition as probabilistic inference<br/>
•&#160;&#160;Learning concepts from examples&#160;<br/>
(continued)<br/>
•&#160;&#160;Learning and using intuitive theories (more&#160;<br/>
structured systems of knowledge)<br/>
<hr/>
<a name=2></a><img src="./mlss09-tenenbaum-lecture2-2_1.png"/><br/>
“tufa”<br/>
“tufa”<br/>
“tufa”<br/>
Learning from just one or a few examples, and mostly&#160;<br/>unlabeled examples (“semi-supervised learning”).&#160;<br/>
<hr/>
<a name=3></a>Simple model of concept learning<br/>
“This is a blicket.”<br/>
“Can you show me the<br/>
other blickets?”<br/>
<hr/>
<a name=4></a><img src="./mlss09-tenenbaum-lecture2-4_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-4_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-4_3.png"/><br/>
Learning to learn: what object&#160;<br/>
features count for word learning? &#160;<br/>
This is a dax.<br/>
•&#160;&#160;24-month-olds show the shape&#160;<br/>
bias with simple novel objects.&#160;<br/><i>20-month-olds do not.&#160;</i>(Landau,&#160;<br/>
Show me the dax…<br/>
Smith, Jones 1988)<br/>
•&#160;&#160;Smith et al (2002) &#160;trained 17-&#160;<br/>
“lug”<br/>
month-olds on labels for 4&#160;<br/>
“wib”<br/>
artificial categories:<br/>
•&#160;&#160;After 8 weeks of training (20&#160;<br/>
min/week), 19-month-olds&#160;<br/>
“zup”<br/>
“div”<br/>
show the shape bias.&#160;<br/>
<hr/>
<a name=5></a><img src="./mlss09-tenenbaum-lecture2-5_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-5_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-5_3.png"/><br/>
Transfer to real-world vocabulary<br/>
The intuition: Learn that shape varies across categories but is&#160;<br/>
relatively constant within nameable categories.&#160;<br/>
The puzzle: The shape bias is&#160;a powerful inductive constraint,&#160;<br/>
yet can be learned from very little data.&#160;<br/>
<hr/>
<a name=6></a><img src="./mlss09-tenenbaum-lecture2-6_1.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-6_2.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-6_3.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-6_4.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-6_5.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-6_6.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-6_7.jpg"/><br/>
Learning about feature variability<br/>
(Kemp, Perfors&#160;&amp; Tenenbaum,&#160;<i>Dev. Science&#160;</i>2007)<br/>
?<br/>
<hr/>
<a name=7></a><img src="./mlss09-tenenbaum-lecture2-7_1.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-7_2.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-7_3.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-7_4.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-7_5.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-7_6.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-7_7.jpg"/><br/>
Learning about feature variability<br/>
(Kemp, Perfors&#160;&amp; Tenenbaum,&#160;<i>Dev. Science&#160;</i>2007)<br/>
?<br/>
<hr/>
<a name=8></a><img src="./mlss09-tenenbaum-lecture2-8_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-8_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-8_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-8_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-8_5.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-8_6.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-8_7.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-8_8.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-8_9.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-8_10.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-8_11.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-8_12.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-8_13.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-8_14.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-8_15.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-8_16.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-8_17.png"/><br/>
A hierarchical Bayesian model<br/>
Level 3: &#160; &#160; &#160; &#160; &#160; &#160;<br/>Prior expectations&#160;<br/>on bags in general<br/>
Level 2: &#160; &#160; &#160; &#160;<br/>Bags in general<br/>
Simultaneously&#160;<br/>infer &#160; &#160; &#160;<br/>
Level 1: &#160; &#160; &#160; &#160;<br/>
…<br/>
Bag proportions<br/>
Data<br/>
…<br/>
<hr/>
<a name=9></a><img src="./mlss09-tenenbaum-lecture2-9_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-9_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-9_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-9_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-9_5.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-9_6.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-9_7.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-9_8.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-9_9.png"/><br/>
A hierarchical Bayesian model<br/>
Level 3: &#160; &#160; &#160; &#160; &#160; &#160;<br/>Prior expectations&#160;<br/>on bags in general<br/>
α&#160;=<br/>
Level 2: &#160; &#160; &#160; &#160;<br/>
0.1&#160;(within-bag variability)<br/>
β<br/>
Bags in general<br/>
(overall population distribution)<br/>
Level 1: &#160; &#160; &#160; &#160;<br/>Bag proportions<br/>
…<br/>
…<br/>
Data<br/>
…<br/>
<hr/>
<a name=10></a><img src="./mlss09-tenenbaum-lecture2-10_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-10_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-10_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-10_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-10_5.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-10_6.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-10_7.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-10_8.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-10_9.png"/><br/>
A hierarchical Bayesian model<br/>
Level 3: &#160; &#160; &#160; &#160; &#160; &#160;<br/>Prior expectations&#160;<br/>on bags in general<br/>
α&#160;=<br/>
Level 2: &#160; &#160; &#160; &#160;<br/>
5<br/>
(within-bag variability)<br/>
β<br/>
Bags in general<br/>
(overall population distribution)<br/>
Level 1: &#160; &#160; &#160; &#160;<br/>Bag proportions<br/>
…<br/>
Data<br/>
…<br/>
<hr/>
<a name=11></a><img src="./mlss09-tenenbaum-lecture2-11_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-11_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-11_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-11_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-11_5.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-11_6.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-11_7.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-11_8.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-11_9.png"/><br/>
Learning the shape bias&#160;<br/>
(Kemp, Perfors&#160;&amp; Tenenbaum,&#160;<i>Dev. Science&#160;</i>2007)<br/>
Assuming independent Dirichlet-&#160;<br/>
multinomial models for each&#160;<br/>
“lug”<br/>
dimension …<br/>
“wib”<br/>
“zup”<br/>
“div”<br/>
Training<br/>
…&#160;we learn that:<br/>
–&#160;&#160;Shape varies across categories&#160;<br/>
but not within categories.&#160;<br/>
–&#160;&#160;Texture, color, size vary across&#160;<br/>
and within categories. &#160;<br/>
<hr/>
<a name=12></a><img src="./mlss09-tenenbaum-lecture2-12_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-12_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-12_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-12_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-12_5.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-12_6.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-12_7.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-12_8.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-12_9.png"/><br/>
Second-order generalization test&#160;<br/>
(Kemp, Perfors&#160;&amp; Tenenbaum,&#160;<i>Dev. Science&#160;</i>2007)<br/>
1st&#160;&#160;order gen<br/>2nd&#160;&#160;order gen<br/>
This is a dax.<br/>
Show me the&#160;<br/>dax…<br/>
Training<br/>
Test<br/>
<i>“blessing of&#160;<br/>abstraction”</i><br/>
<hr/>
<a name=13></a><img src="./mlss09-tenenbaum-lecture2-13_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-13_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-13_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-13_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-13_5.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-13_6.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-13_7.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-13_8.png"/><br/>
A more realistic model<br/>
Prior expectations on&#160;<br/>categories in general<br/>
Categories in general<br/>
Individual&#160;<br/>categories<br/>
…?<br/>
Data<br/>
?<br/>
?<br/>
?<br/>
?<br/>
42571507&#160;-&#160;1<br/>
50614667 -&#160;?<br/>
23648160 -&#160;?<br/>
30746502&#160;-&#160;4<br/>
56315442 -&#160;?<br/>
73046446 -&#160;?<br/>
78640370&#160;-&#160;2<br/>
31242541 -&#160;?<br/>
73616235 -&#160;?<br/>
11577707 -&#160;?<br/>
41502465 -&#160;?<br/>
16616311 -&#160;?<br/>
30252135 -&#160;?<br/>
30746502 -&#160;?<br/>
56643025 -&#160;?<br/>
41670016 -&#160;?<br/>
(Perfors&#160;&amp; Tenenbaum,&#160;<i>Proc Cog Sci&#160;2009</i>)<br/>
<hr/>
<a name=14></a><img src="./mlss09-tenenbaum-lecture2-14_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-14_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-14_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-14_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-14_5.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-14_6.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-14_7.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-14_8.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-14_9.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-14_10.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-14_11.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-14_12.png"/><br/>
A more realistic model<br/>
Prior expectations on&#160;<br/>categories in general<br/>
Categories in general<br/>
Individual&#160;<br/>categories<br/>
<i>y</i><br/>
<i>z</i><br/>
<i>y</i><br/>
<i>z</i><br/>
<i>y</i><br/>
<i>z</i><br/>
<i>y</i><br/>
<i>z</i><br/>
Data<br/>
42571507 -&#160;1<br/>
23648160 -&#160;2&#160; &#160;&#160;&#160;56643025 -&#160;3<br/>
30746502 -&#160;4<br/>
11577707 -&#160;1&#160; &#160;&#160;&#160;73046446 -&#160;2&#160; &#160;&#160;&#160;50614667 -&#160;3<br/>
31242541 -&#160;4<br/>
41670016 -&#160;1<br/>
78640370 -&#160;2&#160; &#160;<br/>
56315442 -&#160;3&#160; &#160;&#160;&#160;30252135 -&#160;4<br/>
41502465 -&#160;1<br/>
73616235 -&#160;2<br/>
16616311 -&#160;3<br/>
60240453 -&#160;4<br/>
(Perfors&#160;&amp; Tenenbaum,&#160;<i>Proc Cog Sci&#160;2009</i>)<br/>
<hr/>
<a name=15></a><img src="./mlss09-tenenbaum-lecture2-15_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-15_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-15_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-15_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-15_5.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-15_6.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-15_7.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-15_8.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-15_9.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-15_10.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-15_11.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-15_12.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-15_13.png"/><br/>
A more realistic model<br/>
Prior expectations on&#160;<br/>categories in general<br/>
Categories in general<br/>
Individual&#160;<br/>categories<br/>
<i>y</i><br/>
<i>z</i><br/>
<i>y</i><br/>
<i>z</i><br/>
<i>y</i><br/>
<i>z</i><br/>
<i>y</i><br/>
<i>z</i><br/>
<i>y</i><br/>
<i>z</i><br/>
Data<br/>
42571507 -&#160;1<br/>
23648160 -&#160;2&#160; &#160;&#160;&#160;56643025 -&#160;3<br/>
30746502 -&#160;4<br/>
88998899 -&#160;5<br/>
11577707 -&#160;1&#160; &#160;&#160;&#160;73046446 -&#160;2&#160; &#160;&#160;&#160;50614667 -&#160;3<br/>
31242541 -&#160;4<br/>
41670016 -&#160;1<br/>
78640370 -&#160;2&#160; &#160;<br/>
56315442 -&#160;3&#160; &#160;&#160;&#160;30252135 -&#160;4<br/>
41502465 -&#160;1<br/>
73616235 -&#160;2<br/>
16616311 -&#160;3<br/>
60240453 -&#160;4<br/>
2nd&#160;&#160;order generalization:&#160;<br/>88994271 -&#160;5? &#160;or &#160;42718899 -&#160;5?<br/>
(Perfors&#160;&amp; Tenenbaum,&#160;<i>Proc Cog Sci&#160;2009</i>)<br/>
<hr/>
<a name=16></a><img src="./mlss09-tenenbaum-lecture2-16_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-16_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-16_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-16_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-16_5.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-16_6.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-16_7.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-16_8.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-16_9.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-16_10.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-16_11.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-16_12.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-16_13.png"/><br/>
A more realistic model<br/>
Prior expectations on&#160;<br/>categories in generalLearning the base&#160;<br/>
distribution of a DP&#160;<br/>mixture<br/>
Categories in general<br/>
Individual&#160;<br/>categories<br/>
<i>y</i><br/>
<i>z</i><br/>
<i>y</i><br/>
<i>z</i><br/>
<i>y</i><br/>
<i>z</i><br/>
<i>y</i><br/>
<i>z</i><br/>
<i>y</i><br/>
<i>z</i><br/>
Data<br/>
42571507 -&#160;1<br/>
23648160 -&#160;2&#160; &#160;&#160;&#160;56643025 -&#160;3<br/>
30746502 -&#160;4<br/>
88998899 -&#160;5<br/>
11577707 -&#160;1&#160; &#160;&#160;&#160;73046446 -&#160;2&#160; &#160;&#160;&#160;50614667 -&#160;3<br/>
31242541 -&#160;4<br/>
41670016 -&#160;1<br/>
78640370 -&#160;2&#160; &#160;<br/>
56315442 -&#160;3&#160; &#160;&#160;&#160;30252135 -&#160;4<br/>
41502465 -&#160;1<br/>
73616235 -&#160;2<br/>
16616311 -&#160;3<br/>
60240453 -&#160;4<br/>
2nd&#160;&#160;order generalization:&#160;<br/>88994271 -&#160;5? &#160;or &#160;42718899 -&#160;5?<br/>
(Perfors&#160;&amp; Tenenbaum,&#160;<i>Proc Cog Sci&#160;2009</i>)<br/>
<hr/>
<a name=17></a><img src="./mlss09-tenenbaum-lecture2-17_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-17_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-17_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-17_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-17_5.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-17_6.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-17_7.png"/><br/>
Model &#160; &#160;vs. &#160; &#160;People<br/>
1st&#160;&#160;order generalization<br/>
1st&#160;&#160;order generalization<br/>
2nd&#160;&#160;order generalization<br/>
2nd&#160;&#160;order generalization<br/>
lization<br/>
lization<br/>
a<br/>
a<br/>
% correct gener<br/>
% correct gener<br/>
Category labels<br/>
Category labels<br/>
Category labels<br/>
Category labels<br/>
absent&#160;<br/>
present&#160;<br/>
absent&#160;<br/>
present&#160;<br/>
% coherence:<br/>
% coherence:<br/>
lization<br/>
lization<br/>
a<br/>
a<br/>
% correct gener<br/>
% correct gener<br/>
(Perfors&#160;&amp; Tenenbaum,&#160;<i>Proc Cog Sci 2009</i>)<br/>
<hr/>
<a name=18></a><img src="./mlss09-tenenbaum-lecture2-18_1.png"/><br/>
Towards more natural concepts<br/>
<hr/>
<a name=19></a><img src="./mlss09-tenenbaum-lecture2-19_1.png"/><br/>
Towards more natural concepts<br/>
CRP mixture:<br/>
<hr/>
<a name=20></a><img src="./mlss09-tenenbaum-lecture2-20_1.png"/><br/>
How many different ways to&#160;<br/>
structure a domain?<br/>
(Shafto, Kemp, Mansingka, Tenenbaum, 2006; submitted)<br/>
“CrossCat”: nonparametric clustering over features, with a&#160;<br/>
different clustering of objects for each feature-cluster. &#160;&#160;<br/>
<hr/>
<a name=21></a><img src="./mlss09-tenenbaum-lecture2-21_1.png"/><br/>
How many different ways to&#160;<br/>
structure a domain?<br/>
(Shafto, Kemp, Mansingka, Tenenbaum, 2006; submitted)<br/>
“CrossCat”: nonparametric clustering over features, with a&#160;<br/>
different clustering of objects for each feature-cluster. &#160;&#160;<br/>
Evidence for CrossCat-like learning&#160;<br/>in humans:<br/>
- Sorting natural categories<br/>-&#160;Sorting artificial categories<br/>- Predicting values for novel&#160;<br/>
features and novel objects<br/>
<hr/>
<a name=22></a>Learning relational concepts<br/>
<b>CONCEPTS</b><br/>
Professors<br/>Graduate students<br/>Undergraduates<br/>
<b>RELATIONSHIPS</b><br/>
Professors&#160;give advice to&#160;Grad students&#160;and&#160;Undergrads.<br/>Grad students&#160;give advice to&#160;Undergrads.&#160;<br/>Undergrads&#160;give advice to no one.<br/>
<hr/>
<a name=23></a>Learning relational concepts<br/>
<b>CONCEPTS</b><br/>
Magnets<br/>Magnetic objects<br/>Non-magnetic objects<br/>
<b>RELATIONSHIPS</b><br/>
Magnets&#160;interact with each other.<br/>Magnets&#160;and&#160;Magnetic objects&#160;interact.<br/>Magnetic objects&#160;do not interact with each other.<br/>Non-magnetic objects&#160;do not interact with anything.&#160;<br/>
<hr/>
<a name=24></a><img src="./mlss09-tenenbaum-lecture2-24_1.png"/><br/>
Learning relational concepts<br/>
person 1 &#160;<i>gives advice to&#160;&#160;</i>person 9<br/>
people<br/>
people<br/>
<hr/>
<a name=25></a><img src="./mlss09-tenenbaum-lecture2-25_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-25_2.png"/><br/>
Learning relational concepts<br/>
<i>gives advice to</i><br/>
<i>gives advice to</i><br/>
people<br/>
Prof&#160;Grads&#160;Ug<br/>
Profs<br/>
Grads<br/>
people<br/>
Ugrads<br/>
<hr/>
<a name=26></a><img src="./mlss09-tenenbaum-lecture2-26_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-26_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-26_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-26_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-26_5.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-26_6.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-26_7.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-26_8.png"/><br/>
Infinite Relational Model (IRM)&#160;<br/>
(Kemp, Griffiths, Tenenbaum, Yamada, &amp; Ueda, 2006)<br/>
1&#160;4<br/>
2&#160;6<br/>
7&#160;3<br/>
8<br/>
9&#160;5<br/>
0.1<br/>
0.9<br/>
0.9<br/>
0.1<br/>
0.1<br/>
0.9<br/>
0.1<br/>
0.1<br/>
0.1<br/>
<hr/>
<a name=27></a><img src="./mlss09-tenenbaum-lecture2-27_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-27_2.png"/><br/>
Learning algorithm<br/>
•&#160;&#160;Continuous parameters (weights/probabilities)&#160;<br/>
integrated out analytically.<br/>
•&#160;&#160;Gibbs sampling + split-merge moves:<br/>
Iteration 1<br/>
Iteration 2<br/>
Iteration 3<br/>
Iteration 4<br/>
Iteration 5<br/>
Iteration 6<br/>
<hr/>
<a name=28></a>The causal blocks world&#160;<br/>
(Tenenbaum and Niyogi, 2003)<br/>
<b>G</b><br/>
<b>F</b><br/>
<b>O</b><br/>
<b>W</b><br/>
<b>C</b><br/>
<b>L</b><br/>
<b>A</b><br/>
<b>C</b><br/>
<hr/>
<a name=29></a>The causal blocks world&#160;<br/>
(Tenenbaum and Niyogi, 2003)<br/>
<b>G</b><br/>
<b>F</b><br/>
<b>O</b><br/>
<b>W</b><br/>
<b>C</b><br/>
<b>L</b><br/>
<b>A</b><br/>
<b>C</b><br/>
<hr/>
<a name=30></a><img src="./mlss09-tenenbaum-lecture2-30_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-30_2.png"/><br/>
Training<br/>
Test<br/>
<b>F</b><br/>
<b>?</b><br/>
A<br/>
<b>O</b><br/>
<b>G</b><br/>
<b>W</b><br/>
<b>X</b><br/>
<b>?</b><br/>
<b>C</b><br/>
B<br/>
<b>A</b><br/>
<b>C</b><br/>
<b>L</b><br/>
Learning curves<br/>
9<br/>
8<br/>
7<br/>
<b>p</b><br/>
<b>g U&#160;</b>6<br/><b>tin</b><br/>
<b>igh&#160;</b>5<br/>
Pre<br/>
<b>f&#160;L</b><br/>
Post&#160;Same&#160;Group<br/>
<b>d o&#160;</b>4<br/>
Post&#160;Different&#160;Group<br/>
<b>oo<br/>lih&#160;</b>3<br/>
<b>e<br/>k<br/>Li&#160;</b>2<br/>
1<br/>
Probability of lighting up&#160;0<br/>
1<br/>
2<br/>
3<br/>
4<br/>
5<br/>
3 &#160; &#160; &#160; 6 &#160; &#160; &#160; &#160;9 &#160; &#160; &#160; &#160;12 &#160; &#160; &#160; 15<br/>
Model predictions<br/>
<b>Stage&#160;(#&#160;of&#160;objects&#160;/ 3)</b><br/>
# of objects observed<br/>
<hr/>
<a name=31></a><img src="./mlss09-tenenbaum-lecture2-31_1.jpg"/><br/>
Constructing semantic networks<br/>
(Collins &amp; Quillian, 1969)<br/>
<hr/>
<a name=32></a><img src="./mlss09-tenenbaum-lecture2-32_1.png"/><br/>
Upper level medical ontology<br/>
<hr/>
<a name=33></a><img src="./mlss09-tenenbaum-lecture2-33_1.png"/><br/>
Upper level medical ontology<br/>
Biomedical predicate data used to construct&#160;<br/>
ontology in UMLS (McCrae et al.):<br/>
–&#160;&#160;134 concepts: enzyme, hormone, organ, disease, cell&#160;<br/>
function ...<br/>
–&#160;&#160;49 predicates:&#160;<i>affects</i>(hormone, organ),&#160;<br/>
<i>complicates</i>(enzyme, cell function),&#160;<i>treats</i>(drug,&#160;<br/>disease),&#160;<i>diagnoses</i>(procedure, disease) …<br/>
<hr/>
<a name=34></a><img src="./mlss09-tenenbaum-lecture2-34_1.png"/><br/>
Learning semantic networks with IRM<br/>
concept<br/>
predicate<br/>
concept<br/>
Biomedical predicate data from UMLS (McCrae et al.):&#160;<br/>
–&#160;&#160;134 concepts: enzyme, hormone, organ, disease, cell function ...<br/>
–&#160;&#160;49 predicates:&#160;<i>affects</i>(hormone, organ),&#160;<i>complicates</i>(enzyme, cell&#160;<br/>
function),&#160;<i>treats</i>(drug, disease),&#160;<i>diagnoses</i>(procedure, disease) …<br/>
<hr/>
<a name=35></a><img src="./mlss09-tenenbaum-lecture2-35_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-35_2.png"/><br/>
Learning semantic networks with IRM<br/>
e.g., Diseases&#160;<i>affect</i><br/>
Chemicals&#160;<i>interact</i><br/>
Chemicals&#160;<i>cause</i><br/>
Organisms<br/>
<i>with&#160;</i>Chemicals<br/>
Diseases<br/>
<hr/>
<a name=36></a><img src="./mlss09-tenenbaum-lecture2-36_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-36_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-36_3.png"/><br/>
Learning semantic networks with IRM<br/>
Relations&#160;<br/>between&#160;<br/>clusters:<br/>
e.g., Diseases&#160;<i>affect</i><br/>
Chemicals&#160;<i>interact</i><br/>
Chemicals&#160;<i>cause</i><br/>
Organisms<br/>
<i>with&#160;</i>Chemicals<br/>
Diseases<br/>
<hr/>
<a name=37></a><img src="./mlss09-tenenbaum-lecture2-37_1.png"/><br/>
Extracting semantic networks from text via&#160;<br/>
relational clustering&#160;(Kok&#160;&amp; Domingos&#160;2008)<br/>
Tested several algorithms for relational clustering on TextRunner&#160;data:<br/>
~ 2 million triples of the form&#160;<i>R</i>(<i>x</i>,&#160;<i>y</i>): e.g.,&#160;upheld(Court, ruling),&#160;<br/>
named_after(Jupiter, Roman_god).<br/>
~ 10,214&#160;<i>R&#160;</i>symbols, 8942&#160;<i>x&#160;</i>symbols, 7995&#160;<i>y&#160;</i>symbols (each appears &gt;25 times).<br/>
<hr/>
<a name=38></a><img src="./mlss09-tenenbaum-lecture2-38_1.png"/><br/>
Annotated hierarchies model&#160;<br/>
(Roy, Kemp, Mansinghka&#160;&amp;&#160;Tenenbaum, 2007)<br/>
<hr/>
<a name=39></a><img src="./mlss09-tenenbaum-lecture2-39_1.png"/><br/>
Annotated hierarchies model&#160;<br/>
(Roy, Kemp, Mansinghka&#160;&amp;&#160;Tenenbaum, 2007)<br/>
<hr/>
<a name=40></a><img src="./mlss09-tenenbaum-lecture2-40_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-40_2.png"/><br/>
The Mondrian Process&#160;<br/>
(Roy &amp; Teh, 2008; in prep)<br/>
<hr/>
<a name=41></a><img src="./mlss09-tenenbaum-lecture2-41_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-41_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-41_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-41_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-41_5.png"/><br/>
The Mondrian Process&#160;<br/>
(Roy &amp; Teh, 2008; in prep)<br/>
<hr/>
<a name=42></a><img src="./mlss09-tenenbaum-lecture2-42_1.png"/><br/>
“tufa”<br/>
“tufa”<br/>
“tufa”<br/>
Learning from just one or a few examples, and mostly&#160;<br/>unlabeled examples (“semi-supervised learning”).&#160;<br/>
<hr/>
<a name=43></a><img src="./mlss09-tenenbaum-lecture2-43_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-43_2.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-43_3.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-43_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-43_5.png"/><br/>
Learning words for objects<br/>
“tufa”<br/>
“tufa”<br/>
“tufa”<br/>
(Collins &amp; Quillian, 1969)<br/>
(IT population responses<br/>
Hung et al., 2005; c.f.&#160;<br/>Kiani&#160;et al. 2007)<br/>
<hr/>
<a name=44></a><img src="./mlss09-tenenbaum-lecture2-44_1.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-44_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-44_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-44_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-44_5.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-44_6.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-44_7.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-44_8.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-44_9.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-44_10.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-44_11.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-44_12.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-44_13.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-44_14.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-44_15.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-44_16.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-44_17.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-44_18.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-44_19.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-44_20.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-44_21.png"/><br/>
Learning words for objects<br/>
Bayesian inference over tree-&#160;<br/>structured hypothesis space:<br/>
(Xu &amp;&#160;Tenenbaum,&#160;<br/>
<i>Psych. Review&#160;</i>2007;<br/>
Schmidt &amp; Tenenbaum,<br/>
in prep)<br/>
“tufa”<br/>
“tufa”<br/>
“tufa”<br/>
<hr/>
<a name=45></a><img src="./mlss09-tenenbaum-lecture2-45_1.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_5.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_6.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_7.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_8.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_9.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_10.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_11.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_12.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_13.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_14.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_15.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_16.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_17.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_18.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_19.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_20.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_21.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_22.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_23.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_24.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_25.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_26.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_27.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_28.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_29.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_30.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_31.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_32.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_33.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_34.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-45_35.png"/><br/>
Learning words for objects<br/>
Bayesian inference over tree-&#160;<br/>structured hypothesis space:<br/>
(Xu &amp;&#160;Tenenbaum,&#160;<br/>
<i>Psych. Review&#160;</i>2007;<br/>
Schmidt &amp; Tenenbaum,<br/>
in prep)<br/>
People<br/>
Model<br/>
“tufa”<br/>
“tufa”<br/>
“tufa”<br/>
<hr/>
<a name=46></a><img src="./mlss09-tenenbaum-lecture2-46_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_2.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_3.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_4.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_5.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_6.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_7.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_8.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_9.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_10.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_11.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_12.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_13.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_14.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_15.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_16.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_17.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_18.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_19.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_20.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_21.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_22.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_23.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_24.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_25.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-46_26.jpg"/><br/>
Hierarchical Bayesian framework<br/>
<i>F</i>: form<br/>
Tree<br/>
<i>S</i>: structure<br/>
“tufa”<br/>
“tufa”&#160;“tufa”<br/>
F1&#160;<br/>
F2<br/>
F3<br/>
F4<br/>
<i>D</i>: data<br/>
<b>…</b><br/>
<b>…</b><br/>
<hr/>
<a name=47></a>Learning to learn: what is the right&#160;<br/>
form of structure for the domain? &#160;<br/>
<i>F</i>: form<br/>
Tree<br/>
Space<br/>
Order<br/>
X1<br/>
X1<br/>
X1<br/>
X2<br/>
X2<br/>
X2<br/>
X3<br/>
X3<br/>
X3<br/>
X4<br/>
<i>S</i>: structure<br/>
X4<br/>
X5<br/>
X4<br/>
X6<br/>
X6<br/>
X5<br/>
X5<br/>
X6<br/>
Features<br/>
X1<br/>X2<br/>
<i>D</i>: data<br/>
X3<br/>
…<br/>
X4<br/>X5<br/>X6<br/>
<hr/>
<a name=48></a><img src="./mlss09-tenenbaum-lecture2-48_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-48_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-48_3.jpg"/><br/>
The value of structural form knowledge:&#160;<br/>
inductive constraints (bias)<br/>
Mystery city …<br/>
- average annual temperature: 66 F / 19 C&#160;<br/>-&#160;voted 60% for George W Bush in 2004<br/>-&#160;popular foods are fried and BBQ<br/>
<hr/>
<a name=49></a><img src="./mlss09-tenenbaum-lecture2-49_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-49_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-49_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-49_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-49_5.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-49_6.png"/><br/>
Property induction<br/>
(Kemp &amp; Tenenbaum,&#160;<i>Psych. Review&#160;</i>2009; Shafto et al.,&#160;<i>Cognition&#160;</i>2008)<br/>
Given that {X&#160;, …, X&#160;} have property P, how likely is it that Y does?<br/>
1&#160;<br/>
n&#160;<br/>
Tree &#160;<br/>
2D<br/>
Horses<br/>
All mammals<br/>
Tree<br/>
2D<br/>
Minneapolis<br/>
Houston<br/>
<hr/>
<a name=50></a><img src="./mlss09-tenenbaum-lecture2-50_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-50_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-50_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-50_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-50_5.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-50_6.png"/><br/>
Property induction<br/>
(Kemp &amp; Tenenbaum,&#160;<i>Psych. Review&#160;</i>2009)<br/>
Given that {X&#160;, …, X&#160;} have property P, how likely is it that Y does?<br/>
1&#160;<br/>
n&#160;<br/>
Tree &#160;<br/>
2D<br/>
1<br/>
1<br/>
−<br/>
Horses<br/>
All mammals<br/>
Property ~&#160;&#160;<i>N&#160;</i>(&#160;,<br/>
0&#160;(Δ&#160;+<br/>
<i>I</i><br/>
<i>N&#160;</i>(&#160;,<br/>
0&#160;exp(&#160;1<br/>
−&#160;||&#160;<i>x&#160;</i>−&#160;<i>x&#160;</i>||)<br/>
σ<br/>
<i>i</i><br/>
<i>j</i><br/>
)<br/>
2<br/>
)&#160;)<br/>
σ<br/>
(Zhu, Lafferty &amp; Ghahramani, 2003)<br/>
(c.f. Lawrence, 2004)<br/>
Tree<br/>
2D<br/>
Minneapolis<br/>
Houston<br/>
<hr/>
<a name=51></a><img src="./mlss09-tenenbaum-lecture2-51_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-51_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-51_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-51_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-51_5.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-51_6.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-51_7.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-51_8.png"/><br/>
Property induction<br/>
(Kemp &amp; Tenenbaum,&#160;<i>Psych. Review&#160;</i>2009; Shafto et al.,&#160;<i>Cognition&#160;</i>2008)<br/>
Given that {X&#160;, …, X&#160;} have property P, how likely is it that Y does?<br/>
1&#160;<br/>
n&#160;<br/>
Tree &#160;<br/>
Smooth:&#160;<i>P</i>(<i>D</i>|<i>S</i>) high&#160;<br/>
Not smooth:&#160;<i>P</i>(<i>D</i>|<i>S</i>) low&#160;<br/>
2D<br/>
1<br/>
1<br/>
−<br/>
Horses<br/>
All mammals<br/>
Property ~&#160;&#160;<i>N&#160;</i>(&#160;,<br/>
0&#160;(Δ&#160;+<br/>
<i>I</i><br/>
2<br/>
)&#160;)<br/>
σ<br/>
(Zhu, Lafferty &amp; Ghahramani, 2003)<br/>
Tree<br/>
2D<br/>
Minneapolis<br/>
Houston<br/>
<hr/>
<a name=52></a><img src="./mlss09-tenenbaum-lecture2-52_1.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-52_2.png"/><br/>
Learning structural forms<br/>
People can discover structural forms…<br/>
–&#160;&#160;Scientists<br/>
Linnaeus<br/>
Darwin<br/>
Mendeleev<br/>
Kingdom Animalia<br/>
Phylum Chordata&#160;<br/>
Class Mammalia&#160;<br/>
Order Primates&#160;<br/>
Family&#160;Hominidae&#160;<br/>
Genus&#160;Homo&#160;<br/>
Species&#160;<i>Homo sapiens</i><br/>
–&#160;&#160;Children<br/>
e.g., hierarchical structure of category labels, cyclical structure of seasons or&#160;<br/>
days of the week, clique structure of social networks.&#160;<br/>
…&#160;but standard learning algorithms assume fixed forms.<br/>
–&#160;&#160;Principal components analysis: low-dimensional spatial structure<br/>
–&#160;&#160;Hierarchical clustering: tree structure<br/>
–&#160;&#160;<i>k</i>-means clustering, mixture&#160;models: flat partition.<br/>
<hr/>
<a name=53></a><img src="./mlss09-tenenbaum-lecture2-53_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-53_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-53_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-53_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-53_5.png"/><br/>
Hypothesis space of structural forms&#160;<br/>
(Kemp &amp; Tenenbaum, PNAS 2008)<br/>
Form<br/>
Process<br/>
Form<br/>
Process<br/>
<hr/>
<a name=54></a><img src="./mlss09-tenenbaum-lecture2-54_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-54_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-54_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-54_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-54_5.png"/><br/>
A hierarchical Bayesian approach&#160;<br/>
(Kemp &amp; Tenenbaum, PNAS 2008)<br/>
<i>P</i>(<i>F</i>)<br/>
<i>F</i>: form<br/>
x<br/>
<i>P</i>(<i>S&#160;</i>|&#160;<i>F</i>)<br/>Simplicity<br/>
X1<br/>
X1<br/>
X2<br/>
X2<br/>
X1<br/>
X2<br/>
X6<br/>
<i>S</i>: structure<br/>
X3<br/>
X3<br/>
X3<br/>
X4<br/>
X4<br/>
X5<br/>
X5<br/>
X4<br/>
X5<br/>
X6<br/>
<i>P</i>(<i>D&#160;</i>|&#160;<i>S</i>)<br/>
X6<br/>
Smoothness&#160;<br/>
Features<br/>
(Fit to data)<br/>
X1<br/>X2<br/>
<i>D</i>: data<br/>
X3<br/>
…<br/>
X4<br/>X5<br/>X6<br/>
<hr/>
<a name=55></a><img src="./mlss09-tenenbaum-lecture2-55_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-55_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-55_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-55_4.png"/><br/>
features<br/>
animals<br/>
cases<br/>
judges<br/>
<hr/>
<a name=56></a><img src="./mlss09-tenenbaum-lecture2-56_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-56_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-56_3.jpg"/><br/>
objects<br/>
similarities<br/>
objects<br/>
<hr/>
<a name=57></a><img src="./mlss09-tenenbaum-lecture2-57_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-57_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-57_3.png"/><br/>
Development of structural forms&#160;<br/>
as more data are observed<br/>
<i>“blessing of abstraction”</i><br/>
<hr/>
<a name=58></a><img src="./mlss09-tenenbaum-lecture2-58_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-58_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-58_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-58_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-58_5.png"/><br/>
Causal learning and reasoning<br/>
Causal<br/>model<br/>
Event<br/>data<br/>
(Griffiths &amp; Tenenbaum;&#160;<br/>Kemp, Goodman, Tenenbaum)<br/>
<hr/>
<a name=59></a><img src="./mlss09-tenenbaum-lecture2-59_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-59_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-59_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-59_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-59_5.png"/><br/>
Causal learning and reasoning<br/>
Behaviors<br/>
Diseases<br/>
Symptoms<br/>
Causal&#160;<br/>
high-fat diet<br/>
heart disease<br/>
coughing<br/>
schema<br/>
working in factory<br/>
lung cancer<br/>
chest pain<br/>
…<br/>
…<br/>
…<br/>
Cut down hypothesis&#160;<br/>
Causal<br/>
space from size&#160;<br/>
model<br/>
521,939,651,343,829,&#160;<br/>405,020,504,063 &#160;<br/>
to<br/>
131,072<br/>
Event<br/>data<br/>
(Griffiths &amp; Tenenbaum;&#160;<br/>Kemp, Goodman, Tenenbaum)<br/>
<hr/>
<a name=60></a><img src="./mlss09-tenenbaum-lecture2-60_1.png"/><br/>
Laws<br/>
Classes<br/>
η<br/>
‘B’<br/>
‘B’&#160;‘D’&#160;‘S’<br/>
<i>z</i><br/>
‘D’<br/>
1 &#160;2&#160;<br/>
‘B’&#160;0.0&#160;0.3&#160;0.01<br/>
3 &#160;4<br/>
5 &#160;6&#160;<br/>
7 &#160;8<br/>
Causal&#160;<br/>
‘D’&#160;0.0&#160;0.0&#160;0.25<br/>
schema<br/>
Infinite&#160;<br/>
0.0&#160;0.0&#160;0.0<br/>
9 &#160;10&#160;<br/>
‘S’<br/>
‘S’<br/>
relational&#160;<br/>
11 &#160;12<br/>
model<br/>
9<br/>
5<br/>
1<br/>
10<br/>
Causal<br/>
6<br/>
model<br/>
2<br/>
11<br/>
7<br/>
Belief net with&#160;<br/>
3<br/>
12<br/>
8<br/>
Dirichlet-&#160;<br/>
4<br/>
multinomial &#160;<br/>
Event<br/>
1 &#160; &#160; 2 &#160; &#160; 3 &#160; &#160;4<br/>
5 &#160; &#160;6 &#160; &#160;7 &#160; &#160;8<br/>
9&#160;&#160;10 &#160; 11 &#160;12<br/>
parameterization<br/>
data<br/>
Patient 1<br/>
Patient 2<br/>
Patient 3<br/>
Patient 4<br/>
Patient 5<br/>
…<br/>
(Mansinghka, Kemp, Tenenbaum, Griffiths,&#160;UAI 2006)<br/>
<hr/>
<a name=61></a><img src="./mlss09-tenenbaum-lecture2-61_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-61_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-61_3.png"/><br/>
1<br/>
2<br/>
3<br/>
4<br/>
5<br/>
6<br/>
Ground-truth<br/>causal network<br/>
7<br/>
8<br/>
9<br/>
10<br/>
11<br/>
12<br/>
13<br/>
14<br/>
15<br/>
16<br/>
20 &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;&#160;80 &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;1000<br/>
# samples<br/>
Causal model<br/>
recovered<br/>
model<br/>
Event data<br/>
<i>“blessing of abstraction”</i><br/>
1 &#160;2 &#160;3&#160;<br/>
7 &#160;8 &#160;9 10&#160;<br/>
4 &#160;5 &#160;6<br/>
0.4<br/>
11 12 13<br/>
recovered<br/>
Causal schema<br/>
<b>…&#160;</b>c<br/>
14 15 16<br/>
1<br/>
<b>…</b><br/>
schema<br/>
<b>…&#160;</b>c2<br/>
Causal model<br/>
recovered<br/>
model<br/>
Event data<br/>
(Mansinghka, Kemp, Tenenbaum, Griffiths,&#160;UAI 2006)<br/>
<hr/>
<a name=62></a>Conclusions<br/>
How does the mind get so&#160;much from so little, in learning about objects,&#160;<br/>
classes,&#160;causes, scenes, sentences, thoughts,&#160;social systems?&#160;<br/>
A toolkit for studying the nature, use&#160;and acquisition of abstract knowledge:<br/>
–&#160;&#160;<i>Bayesian inference&#160;</i>in probabilistic generative models.&#160;<br/>–&#160;&#160;Probabilistic models defined over a range of&#160;<i>structured representations</i>: spaces,&#160;<br/>
graphs, grammars, predicate logic, schemas, programs.&#160;<br/>
–&#160;&#160;<i>Hierarchical models</i>, with inference at multiple levels of abstraction.&#160;<br/>–&#160;&#160;<i>Nonparametric models</i>, adapting their complexity to the data and balancing&#160;<br/>
constraint with flexibility.&#160;<br/>
An alternative to classic “either-or”&#160;dichotomies: Nature versus Nurture, Logic&#160;<br/>
(Structure, Rules, Symbols)&#160;versus Probability (Statistics).<br/>
–&#160;&#160;How can domain-general mechanisms of learning and representation build&#160;<br/>
domain-specific abstract knowledge?<br/>
–&#160;&#160;How can structured symbolic knowledge be acquired by statistical&#160;learning?&#160;<br/>
A different way to think about the development of a cognitive system.<br/>
–&#160;&#160;Powerful abstractions can be learned surprisingly quickly, together with or prior&#160;<br/>
to learning the more concrete knowledge they constrain.<br/>
–&#160;&#160;Structured representations need not be rigid, static, hand-wired, brittle.&#160;<br/>
Embedded in a probabilistic framework, they can grow dynamically&#160;and&#160;<br/>robustly in response to the sparse, noisy data of experience.<br/>
<hr/>
<a name=63></a>Open directions and challenges<br/>
•&#160;&#160;More precise relation to psychology<br/>
–&#160;&#160;How does human cognitive processing perform&#160;<br/>
approximate probabilistic inference (i.e., approximately&#160;<br/>implement rational methods of approximate inference)?&#160;<br/>
•&#160;&#160;Relation to the brain<br/>
–&#160;&#160;How to implement structured probabilistic models in&#160;<br/>
neural architectures?<br/>
•&#160;&#160;Probabilistic models for richly structured knowledge<br/>
–&#160;&#160;How to formalize an intuitive theory of physics or&#160;<br/>
psychology?<br/>
•&#160;&#160;Effective learning of structured probabilistic models<br/>
–&#160;&#160;How to balance expressiveness/learnability&#160;tradeoff?&#160;<br/>
<hr/>
<a name=64></a><img src="./mlss09-tenenbaum-lecture2-64_1.png"/><br/>
Goal-directed action &#160; &#160; &#160; &#160; &#160; &#160;&#160;<br/>
(production and comprehension)<br/>
(Wolpert&#160;et al., 2003)<br/>
<hr/>
<a name=65></a><img src="./mlss09-tenenbaum-lecture2-65_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-65_2.png"/><br/>
Goal inference as inverse&#160;<br/>
probabilistic planning<br/>
constraints<br/>
goals<br/>
(Baker, Tenenbaum &amp; Saxe,&#160;<i>Cognition</i>, in press)<br/>
rational planning<br/>
(MDP)<br/>
Gergely,&#160;<br/>
actions<br/>
Agent<br/>
Csibra&#160;et al.:<br/>
<hr/>
<a name=66></a><img src="./mlss09-tenenbaum-lecture2-66_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-66_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-66_3.jpg"/><br/>
<img src="./mlss09-tenenbaum-lecture2-66_4.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-66_5.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-66_6.png"/><br/>
Inferring social goals<br/>
constraints<br/>
goals<br/>
(Baker, Goodman &amp; Tenenbaum,&#160;<i>Cog&#160;</i><br/>
constraints<br/>
goals<br/>
rational planning<br/>
<i>Sci&#160;</i>2008; Ullman, Baker, Evans,&#160;<br/>
(MDP)<br/>
Macindoe&#160;&amp; Tenenbaum, submitted)<br/>
rational planning<br/>
(MDP)<br/>
actions<br/>
Agent<br/>
Hamlin, Kuhlmeier, Wynn &amp; Bloom:<br/>
actions<br/>
Agent<br/>
Subject<br/>
ratings<br/>
n<br/>o<br/>cti<br/>
Model<br/>
predi<br/>
Subject<br/>
ratings<br/>
n<br/>o<br/>cti<br/>
Model<br/>
predi<br/>
<hr/>
<a name=67></a>The&#160;<i>really&#160;</i>big questions<br/>
Where does it all end?<br/>
Across different domains and tasks,&#160;and&#160;different levels of abstraction, our&#160;<br/>probabilistic models are starting to look increasingly complex and to differ&#160;<br/>in almost arbitrarily many ways. &#160; This seems unsatisfying…<br/>
The brain appears to have a uniform&#160;circuitry. &#160;Other cognitive modeling&#160;<br/>
paradigms adopt a single unifying representational primitive (production&#160;<br/>rules, predicate logic, synaptic strengths, tensors). &#160;Is there a single&#160;<br/>universal Bayesian primitive?<br/>
How does it all begin?<br/>
Can all these different kinds of representations be learned? &#160;What&#160;is the&#160;<br/>ultimate hypothesis space of innate primitives –&#160;or is it simply “turtles all&#160;<br/>the way up”? &#160;Could a universal hypothesis space for all probabilistic&#160;<br/>models&#160;possibly be searched effectively?<br/>
(C.f. Kolmogorov&#160;complexity theory, Chater &amp; Vitanyi)<br/>
<hr/>
<a name=68></a>Church: a universal probabilistic language<br/>
(Goodman et al., UAI 2008)<br/>
<hr/>
<a name=69></a><img src="./mlss09-tenenbaum-lecture2-69_1.png"/><br/>
Church: a universal probabilistic language<br/>
(Goodman et al., UAI 2008)<br/>
Causal networks<br/>
<hr/>
<a name=70></a><img src="./mlss09-tenenbaum-lecture2-70_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-70_2.png"/><br/>
Church: a universal probabilistic language<br/>
(Goodman et al., UAI 2008)<br/>
Causal networks<br/>
Relational schema<br/>
<hr/>
<a name=71></a><img src="./mlss09-tenenbaum-lecture2-71_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-71_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-71_3.png"/><br/>
Church: a universal probabilistic language<br/>
(Goodman et al., UAI 2008)<br/>
Causal networks<br/>
Relational schema<br/>
Physical objects<br/>
<hr/>
<a name=72></a><img src="./mlss09-tenenbaum-lecture2-72_1.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-72_2.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-72_3.png"/><br/>
<img src="./mlss09-tenenbaum-lecture2-72_4.png"/><br/>
Church: a universal probabilistic language<br/>
(Goodman et al., UAI 2008)<br/>
Causal networks<br/>
Relational schema<br/>
Physical objects<br/>
Rational agents<br/>
<hr/>
<a name=73></a>Open directions and challenges<br/>
•&#160;&#160;More precise relation to psychology<br/>
–&#160;&#160;How does human cognitive processing perform&#160;<br/>
approximate probabilistic inference (i.e., approximately&#160;<br/>implement rational methods of approximate inference)?&#160;<br/>
•&#160;&#160;Relation to the brain<br/>
–&#160;&#160;How to implement structured probabilistic models in&#160;<br/>
neural architectures?<br/>
•&#160;&#160;Probabilistic models for richly structured knowledge<br/>
–&#160;&#160;How to formalize an intuitive theory of physics or&#160;<br/>
psychology?<br/>
•&#160;&#160;Effective learning of structured probabilistic models<br/>
–&#160;&#160;How to balance expressiveness/learnability&#160;tradeoff?&#160;<br/>
<hr/>
<a name="outline"></a><h1>Document Outline</h1>
<ul>
<li><a href="mlss09-tenenbaum-lecture2s.html#1">Outline for lectures</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#2">Slide Number 2</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#3">Simple model of concept learning</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#4">Learning to learn: what object features count for word learning? &#160;</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#5">Transfer to real-world vocabulary</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#6">Slide Number 6</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#7">Slide Number 7</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#8">A hierarchical Bayesian model</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#9">A hierarchical Bayesian model</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#10">A hierarchical Bayesian model</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#11">Learning the shape bias (Kemp, Perfors &amp; Tenenbaum, Dev. Science 2007)</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#12">Second-order generalization test (Kemp, Perfors &amp; Tenenbaum, Dev. Science 2007)</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#13">A more realistic model</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#14">A more realistic model</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#15">A more realistic model</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#16">A more realistic model</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#17">Model &#160; &#160;vs. &#160; &#160;People</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#18">Towards more natural concepts</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#19">Towards more natural concepts</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#20">How many different ways to structure a domain?</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#21">How many different ways to structure a domain?</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#22">Learning relational concepts</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#23">Learning relational concepts</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#24">Learning relational concepts</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#25">Learning relational concepts</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#26">Infinite Relational Model (IRM) (Kemp, Griffiths, Tenenbaum, Yamada, &amp; Ueda, 2006)</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#27">Learning algorithm</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#28">Slide Number 32</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#29">Slide Number 33</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#30">Slide Number 36</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#31">Constructing semantic networks</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#32">Upper level medical ontology</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#33">Upper level medical ontology</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#34">Learning semantic networks with IRM</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#35">Learning semantic networks with IRM</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#36">Learning semantic networks with IRM</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#37">Extracting semantic networks from text via relational clustering (Kok &amp; Domingos 2008)</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#38">Annotated hierarchies model (Roy, Kemp, Mansinghka &amp; Tenenbaum, 2007)</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#39">Annotated hierarchies model (Roy, Kemp, Mansinghka &amp; Tenenbaum, 2007)</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#40">The Mondrian Process (Roy &amp; Teh, 2008; in prep)</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#41">The Mondrian Process (Roy &amp; Teh, 2008; in prep)</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#42">Slide Number 48</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#43">Learning words for objects</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#44">Slide Number 50</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#45">Slide Number 51</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#46">Hierarchical Bayesian framework</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#47">Learning to learn: what is the right form of structure for the domain? &#160;</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#48">The value of structural form knowledge: inductive constraints (bias)</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#49">Property induction</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#50">Property induction</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#51">Property induction</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#52">Learning structural forms</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#53">Hypothesis space of structural forms(Kemp &amp; Tenenbaum, PNAS 2008)</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#54">A hierarchical Bayesian approach(Kemp &amp; Tenenbaum, PNAS 2008)</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#55">Slide Number 61</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#56">Slide Number 62</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#57">Development of structural forms as more data are observed</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#58">Causal learning and reasoning</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#59">Causal learning and reasoning</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#60">Slide Number 69</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#61">Slide Number 70</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#62">Conclusions</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#63">Open directions and challenges</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#64">Goal-directed action &#160; &#160; &#160; &#160; &#160; &#160; (production and comprehension)</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#65">Goal inference as inverseprobabilistic planning</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#66">Inferring social goals</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#67">The really big questions</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#68">Church: a universal probabilistic language</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#69">Church: a universal probabilistic language</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#70">Church: a universal probabilistic language</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#71">Church: a universal probabilistic language</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#72">Church: a universal probabilistic language</a></li>
<li><a href="mlss09-tenenbaum-lecture2s.html#73">Open directions and challenges</a></li>
</ul>
<hr/>
</body>
</html>
